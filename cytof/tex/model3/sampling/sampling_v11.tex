\documentclass[12pt,]{article}

%{{{1
%\usepackage{lmodern}
\usepackage{amssymb,amsmath}
%\usepackage{ifxetex,ifluatex}
%\usepackage{fixltx2e} % provides \textsubscript
%\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
%  \usepackage[T1]{fontenc}
%  \usepackage[utf8]{inputenc}
%\else % if luatex or xelatex
%  \ifxetex
%    \usepackage{mathspec}
%  \else
%    \usepackage{fontspec}
%  \fi
%  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
%\fi
% use upquote if available, for straight quotes in verbatim environments
%\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
%% use microtype if available
%\IfFileExists{microtype.sty}{%
%\usepackage{microtype}
%\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
%}{}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Sampling Scheme for CYTOF Model3},
            pdfauthor={Arthur Lui},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
%\bibliographystyle{plainnat}
%\IfFileExists{parskip.sty}{%
%\usepackage{parskip}
%}{% else
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{6pt plus 2pt minus 1pt}
%}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
%\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\bc}[1]{ \left\{#1\right\} }
\newcommand{\abs}[1]{ \left|#1\right| }
\newcommand{\mat}{ \begin{pmatrix} }
\newcommand{\tam}{ \end{pmatrix} }
\newcommand{\suml}{ \sum_{i=1}^n }
\newcommand{\prodl}{ \prod_{i=1}^n }
\newcommand{\ds}{ \displaystyle }
\newcommand{\df}[2]{ \frac{d#1}{d#2} }
\newcommand{\ddf}[2]{ \frac{d^2#1}{d{#2}^2} }
\newcommand{\pd}[2]{ \frac{\partial#1}{\partial#2} }
\newcommand{\pdd}[2]{\frac{\partial^2#1}{\partial{#2}^2} }
\newcommand{\N}{ \mathcal{N} }
\newcommand{\E}{ \text{E} }
\def\given{~\bigg|~}
\usepackage{float}
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\ind}{\overset{ind}{\sim}}
\newcommand{\I}{\mathrm{\mathbf{I}}}

\def\bet{\bm{\eta}}


\allowdisplaybreaks
\def\M{\mathcal{M}}
\def\logit{\text{logit}}
\def\Bern{\text{Bernoulli}}
\def\N{\text{N}}
\def\G{\text{Ga}}
\def\IG{\text{Inverse-Gamma}}
\def\Dir{\text{Dirichlet}}
\def\Ber{\text{Ber}}
\def\Be{\text{Be}}
\def\lin{\lambda_{in}}
\def\btheta{\bm{\theta}}
\def\y{\bm{y}}
\newcommand\m{\bm{m}}
\def\mus{\mu^\star}
\def\sss{{\sigma^2}^\star}
\input{includes/mhSpiel.tex}
\newcommand{\Ind}[1]{\mathbbm{1}\bc{#1}}
\def\rest{\text{rest}}
\def\bang{\boldsymbol{\cdot}}
\def\h{\bm{h}}
\def\Z{\bm{Z}}
\def\Unif{\text{Unif}}
\def\Prob{\text{Pr}}


%sim-tex-commands
\newcommand{\true}{{\mbox{\tiny TR}}}
\newcommand{\bZ}{\mbox{\boldmath $Z$}}
\newcommand{\bp}{\mbox{\boldmath $p$}}
\newcommand{\bq}{\mbox{\boldmath $q$}}
\newcommand{\bz}{\mbox{\boldmath $z$}}
\newcommand{\bw}{\mbox{\boldmath $w$}}
\newcommand{\bW}{\mbox{\boldmath $W$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}

\newcommand{\by}{\mbox{\boldmath $y$}}
%\newcommand{\bm}{\mbox{\boldmath $m$}}

\def\bmu{\bm{\mu}}
\def\bnu{\bm{\nu}}
\def\bomega{\bm{\omega}}
\def\bgam{\bm{\gamma}}
\def\bsig{\bm{\sigma}}
\def\blambda{\bm{\lambda}}


%%% Graphing
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
\usepackage{pgffor}
%}}}1

\title{Bayesian Feature Allocation Models for Natural Killer Cell Repertoire Studies Using Mass Cytometry Data}
\author{Arthur Lui}
\date{\today}

\begin{document}
\maketitle
\onehalfspacing



\begin{abstract}
\noindent
Bayesian feature allocation models (FAMs) embedded with
clustering are developed to analyze mass cytometry data, with primary aim to
characterize underlying cell repertoire structures.   Cell repertoires in
samples are heterogeneous. Each repertoire consists of a collection of cells
possessing different phenotypes that can be characterized by differences in
expression levels of cell surface markers.  In particular, mass cytometry data
collected to study the clinical efficacy of natural killer (NK) cells as
immunotherapeutic agents against leukemia is considered. NK cells play a
critical role in cancer immune surveillance and are the first line of defense
against viruses and transformed tumor cells.  The data includes expression
levels of 32 surface markers on each of thousands of cells from multiple
samples. NK cell repertoires may affect both NK cell function and immune
surveillance.  A key conceptual shift compared with existing approaches is to
explicitly characterize latent cell phenotypes through a FAM.  The models
simultaneously (1) characterize NK cell phenotypes based on expression /
non-expression of surface markers, (2) estimate compositions of the samples
based on the identified phenotypes, and (3) infer associations of subjects'
covariates, with the composition of the identified phenotypes in the samples.
The conventional Indian buffet process (IBP), one of the most popular FAMs, is first utilized to model cell phenotypes. Non-ignorable
missing data that is present due to technical artifacts in mass cytometry is
accounted for by using an informed prior missing mechanism. The repulsive FAM
(rep-FAM) is next proposed.  In contrast with the IBP, the rep-FAM produces a
parsimonious representation of phenotypes by discouraging the creation of
redundant phenotypes, and can thus improve inference on phenotypes.  Further
extensions to incorporate subject-based covariates are discussed to provide
inferences on phenotypes potentially associated with positive clinical
outcomes.  

%\noindent
%{\em Keywords:} ~  Count data, Laplace prior, Metagenomics, Microbiome, Regularizing prior, Process convolution,  Negative binomial model, Next-generation sequencing
\end{abstract}




\section{Introduction}
Clinical application of natural killer (NK) cells
%as immunotherapeutic agents against leukemia
recently has emerged as a powerful treatment modality for advanced cancers
refractory to conventional therapies \citep{rezvani2015application}. 
%and intensely investigated.
%NK cells, the third lymphocyte lineage, % preceded by T cells and B cells,
%play critical roles in the immune response to certain %virus infected cells and
%transformed tumor cells.
NK cells play a critical role in cancer immune surveillance and are the first
line of defense against viruses and transformed tumor cells.  They have the
intrinsic ability to infiltrate cancer tissue and their presence in the tumor
is reported to be associated with better clinical outcomes
\citep{suck2016natural}.  
%NK cells develop in the bone marrow and are ``educated'' during their development to ensure not to attack normal self cells whiling rapidly killing tumor or virally infected cells.
% (called self-tolerance).
%During the development,
Drs. Thall and Rezvani, collaborators at UT MD Anderson Cancer Center, have
conducted clinical trials to study potential clinical efficacy of umbilical
cord blood (UCB) transplantation as a therapy for leukemia.  UCB has become an
established source of hematopoietic stem cells for transplantation. UCB NK cell
therapy has the advantages of low risk of viral transmission from donor to
recipient \citep{sarvaria2017umbilical}. In the trials, leukemia patients
received UCB cell transplants.
% after irradiation therapy.  %Some characteristics of the patients such as their survival after the treatment are recorded.
During follow up, samples were taken at multiple time points from each patient.
Samples from
%In addition, peripheral blood samples from
healthy subjects and cord blood samples also were collected for comparison to
leukemia patient samples.  The samples were processed and expressions of 32 NK
cell-associated cell surface protein markers,
%in individual cells of the samples
measured for individual cells in the samples using mass cytometry.  Their
primary research goal is to understand phenotypes and functions structured
across heterogeneous NK cells.
% based on the markers' expression levels.
Better understanding of the characteristics of NK cells is crucial to estimate
the true potential of NK cell therapies against cancer.
%for cancer immunotherapy. %against leukemia and possibly also against solid tumors.



%NK cells play critical roles in defending against tumors. Furthermore, their diversity and function are
%known to be linked. Researchers have thus studied NK cell diversity from
%various perspectives. For instance, it is known that NK cell diversity
%is lower at birth \citep{strauss2015human} than in adults. Some
%researchers have studied the effect of introducing diverse NK cells into
%tumor patients. Yet again, some researchers have found that patients
%with higher NK diversity are associated with higher exposure risk of
%HIV-1, suggesting that existing diversity may decrease flexibility of
%the antiviral response. Many questions about NK cells remain to be
%answered. Understanding NK diversity through spectrometry has therefore
%been an important research area in the bio-sciences.   

%The main
%inferential goal of this project is to identify the NK cell phenotypes
%(or cell-types) in various samples as a set of subpopulations of the set
%of some provided surface markers. The NK cell-types are latent, and for
%\(J\) markers \(2^J\) different cell-types can be considered. This
%provides a computational challenge when the number of markers is even
%moderately large. Thirty-two markers are included in this analysis, and
%naively enumerating all possible markers is not feasible. We therefore,
%use a latent feature allocation model to learn the latent structure of
%predominant cell-types. Latent feature models have been successfully
%applied to various problems and will be reviewed in the following
%section.
%Data for this project is rendered through CyTOF analyses of
%NK-cell-targeting markers. Having some understanding of CyTOF and NK
%cells their importance is therefore necessary.



Advances in cytometry have led to more research and greater understanding  of
natural killer (NK) cells and how their diversity impacts immunity against the
development of tumors and other viral diseases.  Flow cytometry has been routinely used for single-cell analysis in cellular and clinical immunology. One of primary interest in cytometry data analysis is to identify different cell types in a heterogeneous population and measure their abundances.  Cell types are characterized by their distinct expression patterns of multivariate protein markers and expression patterns can be related to differentiation of cells and their biological function.  
%Flow cytometry (developed by
%Wallace Coulter in the 1950's) is a laser-based biophysical technology which is
%sometimes used for biomarker detection. It is regularly used to diagnose health
%disorders like cancer. Cytometry has advanced over the years.
%Fluorescence-based flow cytometry, which makes use of fluorescent dyes and
%lasers that emit light at specific wavelengths, is one such advancement that
%has been mainstream for several decades \citep{herzenberg2002history}.  
In recent years, a new cytometry technique, Cytometry at Time-Of-Flight (CyTOF, also known as mass cytometry) has surfaced.
It makes use of time-of-flight mass spectrometry, where sophisticated devices
are used to accelerate, separate, and identify ions by mass. This new method
enables to detect a greater number of parameters (biological phenotypic or functional markers), up to 40 parameters for a cell, in less time and in a higher resolution
\citep{cheung2011screening}. 
%Through CyTOF, scientists have been able to better understand natural killer
%(NK) cells \citep{horowitz2013genetic}. 
A major challenge in understanding mass cytometry data is to develop efficient
inferential frameworks that can handle their complexity. One of traditional methods is manual ``gating'', which is the sequential identification and refinement of clusters of homogeneous cells using a set of markers. However, it has several serious shortcomings such as inherent subjectivity of manual analysis and being unscalable for multiparameter data. While the manual gating is still popular in practice, many computational methods that automatically identify cell clusters have been proposed to analyze high-dimensional cytometry data.  
%These data are often
%high-dimensional and noisy. Hence, many 
Many of existing automated analysis methods use dimension reduction techniques and/or
clustering methods such as density-based clustering methods, model-based clustering methods, and self-organizing maps.
% FLOWSOM
For example, FlowSOM in \cite{van2015flowsom} uses a self-organizing map (SOM), an unsupervised neural network technique, for clustering and dimension reduction. A low-dimensional
representation of the input space is obtained using unsupervised neural networks for easy visualization in a graph called a map.  FlowSOM is fast and can be used as a starting point for a manual gating or as a visualization tool after a gating.
Other common approaches are density-based clustering methods including DBSCAN \citep{ester1996density} and ClusterX \citep{chen2016cytofkit} and  model-based clustering methods including flowClust \citep{lo2009flowclust} and BayesFlow \citep{johnsson2016bayesflow}.
Density-based clustering methods, DBSCAN and ClusterX form clusters when regions are densely occupied by
observations. Observations from dense clusters that are near enough according to
some predetermined proximity metric and threshold are grouped, while observations that are considered far from dense regions are classified as outliers. Density-based methods can produce clusters that take on flexible shapes. FlowClust clusters Box-Cox transformed data using a mixture of t-distributions and provides parameter estimates through expectation-maximization (EM) algorithm. BayesFlow developed a Bayesian hierarchical model to cluster cells through a Gaussian mixture model.  \cite{weber2016comparison} performed a study to compare freely available clustering methods for high-dimensional cytometry data.  They analyzed six publicly available cytometry datasets and compared identified cell subpopulations to cell population identities known from expert manual gating. 


Existing methods, while promising, have fallen short at providing comprehensive inference on the underlying cell phenotypes in cell populations.  Many of them do not properly handle large sample-to-sample variations and abnormalities due to technical artifacts in cytometry data and do not provide uncertainty measurements for resulting inferences.  Expression levels in different samples can significantly vary due to technical variation and most of existing methods often analyze samples separately.  Observations can also be missing due to some technical limitations when markers are not expressed.  Existing methods often ignore missing data or use pre-imputed data.  More importantly, exiting methods do not directly model the underlying cell types.  They rather identify cell clusters based on similarities in expression patterns and determine cell types based on identified clusters. When cells have the same expression pattern but with different expression levels due to some experimental noise, they can be grouped in different clusters although they are likely to have a cell type.  We will utilize Bayesian feature allocation models (FAMs) embedded with clustering
capabilities as our main tools to directly model cell types. FAM will provide solid foundations to an effective way of revealing the underlying cell phenotypes. We also propose mechanisms for imputing missing values within the
models. The proposed models will efficiently provide a full model-based and probabilistic inference with honest uncertainty quantification.


 

%{\tt introduce some existing approaches.  Explain a bit the methods
%that we will use for comparison.  State limitations of existing methods.  Are
%they Bayesian?  Are they modeling phenotypes like our $\Z$.  Emphasize that our
%methods produce direct inferences on phenotypes or quantification of
%uncertainty associated with their inference.}
%
%{\tt include several examples of existing methods. explain what they do and
%produce for inference. explain what they miss.}
%
%To better understand the properties of these different methods, 
%\cite{weber2016comparison} performed a study which compares freely available
%clustering methods for high-dimensional cytometry data. These methods are based
%on a variety of techniques including hierarchical clustering methods , k-means,
%density-based clustering methods, model-based clustering methods,
%nearest-neighbor methods, and self-organizing maps. Each method exhibits their
%own strengths.
% kmeans
%In K-mean, one of the most well known clustering methods, observations are
%iteratively grouped into clusters based on their proximity to the $K$
%centroids, and then the centroids are updated based on the newly formed
%clusters. This method, while fast, memory-efficient, and intuitive, produces
%only point estimates of clusters. In addition, the number of clusters needs to
%be predetermined.
%This is a common trait among the clustering methods used in analyzing cytometry
%data.
% hclust
%In hierarchical clustering, no assumptions about the number of clusters are
%made, and the final output is a dendrogram, which can then be cut at different
%levels, forming clusters. However, these dendrograms are typically cut based on
%some visual criteria, which can be cumbersome.  Hierarchical clustering
%implementations also typically suffer from memory-related bottlenecks for large
%datasets.  Memory-efficient implementations suited for such datasets have been
%implemented in the statistical programming language R by
%\citep{linderman2013package}, making it a viable candidate method for cytometry
%data.
% nearest neighbors. phenograph @levine2015data
%Nearest-neighbor clustering methods like PhenoGraph \citep{levine2015data}
%similarly attempt to reduce the computational costs of hierarchical clustering
%methods by merging clusters by following paths in the nearest neighborhood
%graph of clusters.
%
%These mentioned hierarchical clustering methods do not provide uncertainty
%estimates.
%
% FLOWSOM
%Some methods, like FlowSOM \citep{van2015flowsom}, are able to learn the number
%of clusters in the data, while also allowing the number of clusters to be
%explicitly specified.  FlowSOM uses a self-organizing map (SOM) to reduce the
%dimensionality of cytometry data, and cluster them accordingly. Self-organizing
%maps are trained using unsupervised neural networks to obtain a low-dimensional
%representation of the input space. Since neural networks are capable of
%learning non-linear functions, SOMs may be considered a nonlinear
%generalization of principal components analysis \citep{yin2008learning}. In the
%FlowSOM implementation, the input space is projected onto a two-dimensional
%space and can be conveniently visualized in a graph called a map.
%Among existing methods FlowSOM is considered the fastest and most flexible at
%finding subpopulations in cytometry data. However, it does not quantify
%uncertainty about the learned clusters.
%% Write about density-based, model-based, and nearest neighbors.
% density-based. DBSCAN \citep{ester1996density}
%Another class of methods where the number of clusters does not need to be
%specified beforehand is density-based clustering methods. The earliest of these
%methods include DBSCAN \citep{ester1996density}. In density-based
%clustering, clusters are formed when regions are densely occupied by
%observations. Points from dense clusters that are near enough (according to
%some predetermined proximity metric and threshold) are grouped into the
%cluster. Points that are considered far from dense regions are classified as
%outliers. In general, density-based methods can produce clusters that take on
%very flexible shapes. But again, no uncertainty estimates are produced for
%these clusters. In addition, older implementations like DBSCAN can be
%computationally inefficient, having a worst-case complexity of
%$\mathcal{O}(n^2)$ (due to the computation and storage of the distance matrix). 
% density-based. ClusterX @chen2016cytofkit 
%ClusterX \citep{chen2016cytofkit} is a density-based clustering method used for
%cytometry data. It alleviates some computational and memory burdens by using a
%split-apply-combine strategy where the data are first split into smaller 
%manageable chunks so as to compute a smaller distance matrix. Parameters
%are computed for each small chunk and the results are combined. 
%
% model-based. flowClust @lo2009flowclust. 
%%%% - Uses mixture of t-dist on box-cox-transformed data
%%%% - model is fit using EM.
%%%% - Model is fit K times, each time with different number of clusters
%%%%      - using BIC, the number of clusters is chose
%%%% - slower. Where FlowSOM takes seconds, this could take hours to get about the same performance
%Another clustering approach is model-based clustering, the most popular of
%which is the finite gaussian mixture model. To account for irregular shapes and
%the presence of outliers in cytometry data, \cite{lo2009flowclust} developed
%flowClust, which first applies a Box-Cox transformation to the data, and then
%uses a mixture of t-distributions to cluster the transformed data.  The
%expectation-maximization (EM) algorithm \citep{dempster1977maximum} can be used
%to learn the parameters in this model.
%Model-based clustering algorithms are
%among the best performing algorithms used in cytometry data analysis, but they
%can be much slower to train. For instance, flowClust may take hours to train in
%order to get the same performance as FlowSOM when it is only trained for
%seconds.
% 


\subsection{Review: Feature Allocation Models}\label{literature-review}
In the motivating dataset, one of the main inferential goals is  to learn a latent
structure of predominant NK cell phenotypes, where cell phenotype are defined based on
distinct expression combinations of the markers. Specifically, phenotype $k$ is
represented by a $J$-dim binary vector, $\bm z_k=(z_{1k}, \ldots, z_{Jk})$,
with $J$ denoting the number of markers, where $z_{jk}$ equals 1 if marker $j$
is expressed in phenotype $k$, and 0 otherwise.  Let $\bZ$ denote a $J \times
K$ binary matrix by letting columns represent $K$ different phenotypes.  \(2^J\) different
phenotypes can be constructed for \(J\) markers.  One may consider a \(J \times
2^J\) binary matrix, with $K=2^J$, that includes all possible phenotypes generated by the
\(J\) markers. It is computationally infeasible even when \(J\) is moderately
large. Taking a Bayesian approach, we consider a prior probability model over
binary matrices
%with infinite number of columns 
and learn the predominant ones in posterior that  generate observed data.
%For computational efficiency, we require a flexible prior which will also
%learn the number of cell-types \((K)\). 
These models are called latent feature allocation models (FAMs). In FAMs, rows and columns correspond to objects and features, respectively (in the culinary metaphor of FAMs, customers and dishes, respectively, and in
our applications, markers and phenotypes, respectively). Similar to our
construction of phenotypes, if object $j$ takes feature $k$, the corresponding
$z_{jk}$ takes the value 1. Otherwise, it takes 0.  One of popular models for
binary feature matrices is the Indian buffet process (IBP), a Bayesian
nonparametric distribution over $\bZ$ with an unbounded number of latent
features, proposed by \citet{griffiths2011indian}. 
%The IBP have been used in a variety of applications where modelling latent binary features is of interest. 
They construct the IBP by considering the finite feature allocation
model and taking the limit with respect to the number of features; for a given
$K$,
\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1),~ k=1, \ldots, K \\
z_{jk} \mid \pi_k &\sim \text{Bernoulli}(v_k),~ k=1, \ldots, K~\mbox{ and } j=1, \ldots, J. \\
\end{split}
\label{eq:ibp}
\end{align}
The marginal limiting distribution of $\Z$ defines an IBP as $K \rightarrow
\infty$ and dropping all columns with all 0's, that is,  \(Z \sim
\text{IBP}(\alpha)\).
%Under the IBP, an object can take a set of latent features and is expected to
%have  \(\alpha\) active features.
Under the IBP, each row has an expected row sum of $\alpha$.
% 
%{\tt active means?}
%
It can be shown that the number of non-zero columns in $Z$ has a Poisson
distribution with mean $\alpha \sum_{j=1}^J j^{-1}$. A prior
distribution can be placed on \(\alpha\) to reflect uncertainty of the number
of latent features. A gamma prior is popular for $\alpha$ due to its conjugacy.
\cite{teh2007stick} represented the IBP using the stick-breaking construction
similar to the stick-breaking representation of the Dirichlet process (DP).
\cite{williamson2010dependent} developed a dependent IBP (dIBP) to induce
correlations between objects and to model multiple $\Z$'s that can be dependent.
\cite{broderick2015combinatorial} first coined the term ``feature allocation
model". Just as the class of probability distributions over partitions of a
dataset has been characterized through exchangeable partition probability
functions, they developed theory for an exchangeable feature probability
function for certain feature allocation models. Under their framework,
many other extensions of the IBP can be proposed. \cite{broderick2013feature} developed the beta-negative binomial process for
admixtures, where observations are represented multiple times across several
latent features. This is another extension of the IBP, where
infinite-dimensional priors are proposed for vectors of counts. They develop
some computationally efficient algorithms that rely on Gibbs sampling for 
posterior sampling. They applied their methods in some simulation studies 
involving topic modeling and computer vision.


%% TODO
%{\tt need to include Tamara's papers on feature allocation model, my papers on
%  tumor heterogeneity. 
%
%\begin{itemize}
%\item ``Feature allocations, probability functions, and paintboxes.'',
%  ``Combinatorial clustering and the beta negative binomial process.'' etc by
%  Tamara \url{http://www.tamarabroderick.com/papers.html}
%
%\item ``Bay- clone: Bayesian nonparametric inference of tumor subclones using
%  ngs data'' by Subhajit et al.
% 
%\item YX's works: ``Bayesian Inference for Latent Biologic Structure with
%  Determinantal Point Processes'', "MAD Bayes for Tumor Heterogeneity Feature
%  Allocation with Exponential Family Sampling"
%  \url{http://www.ams.jhu.edu/~yxu70/pub.html}.
%
%\item There could be more.  Please search for those.
%\end{itemize}
%}

% TODO
%{\tt please include more. for example, XY's JASA paper develops a fast
%computational method for feature allocation models.  It is something that we
%need to know.}


The IBP as a prior distribution in FAMs has been
successfully applied in diverse areas. \cite{lee2015bayesian} modeled
tumor-sample heterogeneity through a finite IBP using DNA sequencing data.  They used $\bZ$ to describe latent haplotypes. A prior distribution is placed on the number of
subclones. These parameters are learned jointly through MCMC.  Building on this
work, \cite{xu2015mad} proposed a general class of feature allocation model for
exponential family sampling distributions for modeling tumor heterogeneity.
They note that a computational challenge in sampling from the joint posterior
in such models involves implementing reversible jump MCMC
\citep{green1995reversible} for transdimensional move. They avoid this by
proposing an MAP-based small-variance asymptotic approximation for any
exponential family likelihood with an IBP feature allocation prior to learn the
feature allocation matrix. The learned feature allocation matrix is then used
to fix the dimensions of a subsequent MCMC conditioned on the number of
features estimated. This method is orders magnitudes faster than
reversible-jump implementations. 
\cite{sengupta2014bayclone} and \cite{lee2016bayesian} proposed a categorical variant of the IBP for tumor-heterogeneity modeling so that
the feature allocation matrix may contain integer values beyond binary values.
They demonstrated how to obtain posterior estimates of the feature allocation
matrix. They also developed a computational method to allow a random number of features so
as to avoid reversible jump.






% \cite{lee2016bayesian}  % tumor hetero
% \cite{xu2015mad} % MAD Bayes. tumor hetero. Fast MAP est.

\subsection{Proposed Projects}
% TODO. Done
%{\tt include a short summary of all three projects and explain how your report
%will be organized in the later part.}
We propose the following projects to study NK cell phenotypes using mass cytometry data. 
\begin{itemize}
\item \underline{Project 1:}
We first develop a model to study the composition of cell populations in multiple samples.
%ell-types across several
%blood samples. 
The model directly characterize latent cell pheotypes with an IBP prior and clusters individual cells based on identified cell types. 
The model also includes a mechanism for imputing missing observations to account for missing marker-expression data missing not at
random. We demonstrate the developed model with simulation studies and an analysis of a real cord-blood dataset.

\item \underline{Project 2:}
Building on Project 1, we extend our model so as to compare the composition of the NK cell
populations present in cord blood samples and samples taken from healthy subjects.
We develop a repulsive FAM (rep-FAM) that discourages cell phenotypes that are similar, and replace the IBP prior distribution with the rep-FAM to obtain parsimonious representation of the underlying cell population structures in different samples.  We examine the properties of the proposed rep-FAM and compare to those of the IBP.  Results from a simulation study
highlighting the key differences between an IBP and a rep-FAM are presented.  An analysis based on a real dataset will be performed.

\item \underline{Project 3:}
We desire to study the change in the abundance of cell phenotypes in patient's
blood samples collected over time. To this end, we will propose a
FAMs with a regression to model associations between time and cell type abundances.  Simulation studies and an analysis based on longitudinal data from patients will be
conducted.
\end{itemize} 
The remainder of the document is organized as follows.
\S~\ref{sec:proj1} and \S!\ref{sec:proj3} describe the proposed projects.
\S~\ref{sec:time} discusses a plan to progress the projects.  
Appendix A contains details for posterior computations in Project 1.



\section{Project 1: Bayesian Feature Allocation Model for Heterogeneous Cell Populations}\label{sec:proj1}
\subsection{Introduction}
% {\tt short intro for project 1}

% TODO
%{\tt expand} 

In this project, we develop a Bayesian FAM, combined with clustering to characterize underlying cell repertoire structures using mass cytometry data. Samples in cytometry data consist of tens of thousands of cells and expression levels of a set of markers are recorded for individual cells. Large and small observed expression levels may imply expression and no expression of the markers, respectively.  A phenotype is defined by its unique subset of expressed markers and a repertoire in a sample can be described as a collection of cells possessing different phenotypes.  One of primary research goals is to characterize underlying cell phenotypes in samples based on observed expression levels and differentiate the samples based on the identified phenotypes.  Many of existing methods proposed to analyze cytometry data use clustering approaches to identify cell subpopulations based on their expression levels of the makers.  They often focus on
producing point estimates of inferred subpopulations, convenient two-dimensional visualization of multivariate expression data, and
computational efficiency. 
While encouraging, many of existing methods fail to provide direct inference on latent cell types.  Under the clustering methods, cells having the same set of expressed markers but with observed expression levels different due to technical variability in an experiment, can be grouped in different clusters. That is, cells in different clusters can be characterized as a cell phenotype. Most of them are also algorithmic methods and do not produce uncertainty
quantification for the learned clusters and their abundances in samples. 
To address the problem of characterizing underlying cell repertoire structures, 
we use a IBP, one of the most popular FAMs and directly model expression patterns of latent cell types.   Individual cells in a sample possess cell types and the proportions of cell types differ in samples. In other words, individual cells will be connected to the identified phenotypes via clustering, with cluster probabilities varying between samples.  We further model observed marker expression levels with
flexible mixture models to effectively accommodate variability in expression levels within cells having a cell type.  Another challenge in analyzing 
cytometry data is data missing not at random. When a marker in a cell is not expressed, cytometry devices are not
able to register a signal and fail to record expression levels, yielding missing values. \cite{franks2016non}
provide a brief overview of typical approaches to imputing data missing not at
random, including a computationally efficient method and intuitive model
representation (Tukey's representation) based only on the observed data. 
In our application, the parameters are entangled with the missingness of the data and we model missing data through a selection factorization representation \citep{rubin1974characterizing}, rather than through Tukey's representation. 


In remainder of the section, we will present the proposed statistical model in \S~\ref{prob-model}, simulation studies in \S~\ref{sec:CB-sim}, analysis of real mass cytometry data in \S~\ref{sec:CB-real} and some concluding remarks in \S~\ref{sec:CB-conc}.



%Each sample consists of tens of thousands of cells and
%records expression levels for 32 NK-cell markers. High marker expression levels
%correspond to the expression of that marker, while low marker expression levels  correspond to non-expression. 

%For a given cell, the expression of certain markers corresponds to a phenotype.
%We are interested in identifying phenotypes that occur frequently within each
%sample. This is important to practitioners because NK-cell diversity is known
%to be affect immunity against infectious diseases. 

% We therefore
%propose a fully Bayesian model to model marker expression levels with a
%flexible mixture model, and the latent cell-type structure with a latent
%feature allocation model.

%we analyze marker expression data from a CyTOF analysis of
%patient blood samples. 
% We will provide results to a simulation study and an analysis of real cytometry
%data.

% TODO. Done
%{\tt include some references on missing value modeling. Alex's paper and
%Rubin's old papers.}  



\subsection{Probability Model}\label{prob-model}
\subsubsection{Sampling Model} 
$I$ samples are taken from subjects, \(i = 1,2,...,I\). Sample \(i\)
consists of \(N_i\) cells, \(n=1, \ldots, N_i\) and for each cell,
expression levels of \(J\) markers are measured. Let
\(\tilde{y}_{inj} \in \mathbb{R}^+\) represent the raw measurement of an
expression level of marker \(j\) of cell \(n\) in sample \(i\). Let
\(c_{ij}\) denote the ``cutoff'' for
 marker \(j\) in sample \(i\). A marker of a cell is likely to be expressed if its observed expression level is greater than the cutoff. A value of $\tilde{y}_{inj}$ below the cutoff may imply that marker $j$ is not expressed in cell $n$ of sample $i$.    We consider the logarithm transformation
after scaling \(\tilde{y}_{inj}\) by \(c_{ij}\), \[
y_{inj}=\log\p{\frac{\tilde{y}_{inj}}{c_{ij}}} \in \mathbb{R}.
\]
Due to the transformation, a value above (below) 0 is likely to represent (no) expression. For some \((i, n, j)\), \(\tilde{y}_{inj}\) is missing due to experimental artifacts and we
introduce a binary indicator, \[
m_{inj} = \begin{cases}
  0, & \text{if $\tilde{y}_{inj}$ is observed,} \\
  1, & \text{if $\tilde{y}_{inj}$ is missing.}
\end{cases}
\] %That is, \(m_{inj}=1\) indicates that the expression level of marker \(j\) of cell \(n\) in sample \(i\) is missing.

%\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
%\tightlist
%\item
%  The data have infinite support.
%\item
%  \(y_{inj} = 0\) has a special meaning, which is that the data take on
%  the same value as the cutoff. Consequently, \(y_{inj} > 0\) means that
%  the data take on values greater than the cutoff, etc.
%\item
%  \(y_{inj}\) for which \(\tilde y_{inj} = 0\) are regarded as missing,
%  and is to be imputed.
%\end{enumerate}

%\newpage


We assume that a sample has heterogeneous cell populations having $K$ different phenotypes.  The phenotypes are not directly observable and we introduce latent phenotype indicators \(\lambda_{in} \in \{1, \ldots, K\}\), for cell $n$ in sample $i$, \(i=1, \ldots, I\) and \(n=1, \ldots, N_i\).
%\(\lambda_{in} \in \{1, \ldots, K\}\) denotes the cell phenotype of cell \(n\) in sample \(i\) defined by 
The event $\lambda_{in}=k$, $k=1, \ldots, K$ represents that cell $n$ in sample $i$ possesses phenotype $k$.  The cell phenotypes are defined by columns of \(J \times K\) binary matrix \(\Z\). The element \(z_{j, k} \in \{0, 1\}\) indicates if marker \(j\) is expressed in cell phenotype \(k\). The event \(z_{jk}=0\) represents that marker \(j\) is not expressed for phenotype $k$, and \(z_{jk}=1\) for
expression. We let $\Z$ and $\lambda_{in}$ random. Details will be discussed later.  Given \(z_{j, \lambda_{in}} \in \{0, 1\}\), we assume a mixture of
normals for \(y_{inj}\),
\begin{align}
y_{inj} \mid \eta_{ij}, \bmu^\star, \bsig^{2 \star}_{i} \ind
\begin{cases}
\sum_{\ell=1}^{L^0} \eta^0_{ij\ell}~ \N(\mu^\star_{0\ell}, \sigma^{2 \star}_{0i\ell}), &\mbox{if $z_{j,\lambda_{in}}=0$},\\
\sum_{\ell=1}^{L^1} \eta^1_{ij\ell}~ \N(\mu^\star_{1\ell}, \sigma^{2 \star}_{1i\ell}), &\mbox{if $z_{j,\lambda_{in}}=1$},\\
\end{cases} \label{eq:y-mix}
\end{align}
where the number of mixture components \(L^0\) and \(L^1\) are fixed.  The vectors $\bet^0_{ij}$ and $\bet^1_{ij}$ are mixture weights with \(\sum_{\ell=1}^{L^0} \eta^0_{ij\ell}=\sum_{\ell=1}^{L^1}\eta^1_{ij\ell}=1\), where \(0 < \eta^1_{ij\ell} < 1\) and \(0 < \eta^0_{ij\ell} < 1\).  In \eqref{eq:y-mix}, $\bmu^\star_1$ and $\bmu^\star_1$ are common for all samples and markers but $\bsig^{2\star}$ are indexed by sample $i$ to account for sample specific variability.  Sample and marker specific mixture weight vectors $\bet^0_{ij}$ and $\bet^1_{ij}$ allow markers in samples to have different distributions.   The mixture model can thus flexibly capture various features in data.  For easy computation, we introduce mixture component indicators $\gamma_{inj}$ for $y_{inj}$.  Given \(\lambda_{in}=k\) we define
\(\gamma_{inj}\); for \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and
\(j=1, \ldots, J\),
\begin{eqnarray}
p(\gamma_{inj} = \ell)=\eta^{z_{jk}}_{ij\ell}, \mbox{ where }~ \ell \in \{1,\ldots, L^{z_{jk}}\}. \label{eq:gam}
\end{eqnarray}
Given \(\lambda_{in}=k\) and \(\gamma_{inj}=\ell\), we assume a normal
distribution for \(y_{inj}\); for \(i=1, \ldots, I\),
\(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  y_{inj} \mid \mu_{inj}, \sigma^2_{inj}  &\ind \N(\mu_{inj}, \sigma^2_{inj}), \label{eq:y-gam}
\end{align}
where \(\mu_{inj} = \mu^\star_{z_{j,k},\ell}\) and \(\sigma^2_{inj} =
{\sigma^{2}}^\star_{iz_{j,k}\ell}\). After marginalizing over $\gamma_{inj}$,
the model in \eqref{eq:y-gam} and \eqref{eq:gam} is equivalent to the model in
\eqref{eq:y-mix}.  

We next build a model for the missingness mechanism.
To build the mechanism, we incorporate information provided by a subject
scientist that a marker expression level is recorded as ``missing'' when a
maker in a cell has a very weak signal, strongly implying that the marker is
not expressed.
% from cells for a marker are extremely weak.  
We take an empirical approach by assuming that the distribution of the values
with $m_{inj}=1$ is similar to the distribution of observed $y$ below 0. We
then let the probability of $y$ being missing depend on its unobserved value of
$y$.
% and assume   % use observed values of $y$ to elicit the probability of $y$
% being missing, $\Prob(m_{inj}=1 \mid y_{inj})$ as a function of $y$.  %In
% addition, we will impose the restriction that extremely low
% expression values have low prior probability. This ensures that parameters in
% the sampling density will not be unnecessarily inflated.
%
Given
\(y_{inj}\), we consider a selection function for \(m_{inj}\); for
\(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  m_{inj} \mid p_{inj} &\ind \Bern(p_{inj}) \label{eq:missing} \\
  \logit(p_{inj}) &= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber, \\
  \beta_{0i} - \beta_{1i}c_1\p{y_{inj}-c_0}^{1/2}, & \text{otherwise}, \nonumber \\
  \end{cases} 
\end{align}
where \(c_0\) and \(c_1\) are real constants, $\beta_{0i} \in \mathbb{R}$ , and
$\beta_{1i} > 0$.
%{\tt include a figure showing what the function looks like. }
Figure \ref{fig:prob-miss-eg} shows an example missing mechanism. We desire the
missing mechanism have a peak in the middle so as discourage extremely large
and extremely small values from being classified as observed. This helps to
control the size of the scale parameter in the mixture of Normals used in the
data density. Note that the assumptions for the distribution of the unobserved data are
untestable. However, we can incorporate input from biologists through informed
prior specifications. Prior uncertainty can then be directly propagated to the
posterior distribution for the missing mechanism.

\begin{figure}[th!]
\begin{center}
\includegraphics[scale=.5]{img/prob_miss_example.pdf}
\caption{Example missing mechanism. The blue points serve as guides in
determining a missing mechanism. Values for $\beta$ and $c$ can be solved for
through a system of equations.}
\label{fig:prob-miss-eg}
\end{center}
\end{figure}



\subsubsection{Priors}\label{priors}
\paragraph*{Latent cell phenotypes}  Recall that we characterize cell phenotypes with a $J\times K$ binary matrix \(\Z =\{z_{jk}\}\).  Following \citet{williamson2010dependent}, we assume
\begin{eqnarray*}
v_k \mid \alpha &\iid& \Be(\alpha/K, 1),~ k=1, \ldots, K, \\
\h_k &\iid& \N_J(\bm{0}, \Gamma), \\ 
z_{jk} \mid h_{jk}, v_k &=& \mathbb{I}\left\{ \Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k \right\},
\end{eqnarray*}
where $\Phi(h \mid m, s)$ is the cumulative distribution function of the normal
distribution with mean $m$ and variance $s$ and $\mathbb{I}(\cdot)$ is an
indicator function having 1 if $\Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k$ or 0
otherwise.  As $K \rightarrow \infty$, the limiting distribution of $Z$ is the
IBP \citep{griffiths2011indian}.  Interactions between $J$ markers in
phenotypes can be modeled through $\bm G$.  Due to the multivariate probit
construction for $\Z$, $\Gamma$ is not identifiable and it is common to
restrict $\Gamma$ to be a correlation matrix. Prior distributions for
correlation matrices have been proposed to handle such cases. Jointly uniform
and marginally uniform prior distributions have been identified by
\cite{barnard2000modeling} for correlation matrices. \cite{box2011bayesian}
have noted that the Jeffreys' prior for correlation matrices is $p(\Gamma)
\propto \abs{\Gamma}^{-(J+1)/2}$.  \cite{zhang2006sampling} presents more
generalized and flexible priors which can be reduced to the two previous priors
as special cases.
%
We let $\alpha \sim \G(a_\alpha, b_\alpha)$ with mean $a_\alpha/b_\alpha$.  

The $K$ cell phenotypes are common in all samples but the relative weights vary
across samples. Let $w_{ik}$ denote an abundance level of phenotype $k$ in
sample $i$.  We assume independent Dirichlet priors for $\bw_i=(w_{i1},
\ldots, w_{iK})$ given $K$, $\bw_{i} \mid K \iid \Dir_K(d/K)$. For latent
cell phenotype indicators, we let $p(\lin=k \mid \bm \bw_i) = w_{ik}$.

\paragraph*{Parameters in the Mixture for $y$}
In \eqref{eq:y-mix}, normal mixture models are assumed for $y_{inj}$. The mean
expression level of maker $j$ in cell $n$ is determined by its phenotype
$\lambda_{in}$.  In particular, if the marker is not expressed in the cell
type, $z_{j \lambda_{in}}=0$, its mean expression level is below the cutoff,
that is, a negative value.  If the marker is expressed $z_{j \lambda_{in}}=1$,
the expression of marker $j$ take a positive value.   Recall that
$\mus_{0\ell}$, $\ell=1, \ldots, L^0$ are mixture locations for $z_{j
\lambda_{in}}=0$ and $\mus_{1\ell}$, $\ell=1, \ldots, L^1$ for $z_{j
\lambda_{in}}=1$.  We assume 
\begin{align*}
\mus_{0\ell} \mid \psi_0, \tau^2_0 &\iid \N_-(\psi_0, \tau^2_0), ~~~ \ell \in \bc{1,...,L^0}, \\
\mus_{1\ell} \mid \psi_1, \tau^2_1 &\iid \N_+(\psi_1, \tau^2_1), ~~~ \ell \in \bc{1,...,L^1}, 
\end{align*}
where \(\N_-(m,s^2)\) and \(\N_+(m,s^2)\) denote the normal distribution with
mean \(m\) and variance \(s^2\), truncated to take only negative values and
positive values, respectively.  The variances $\bsig^{2 \star}_0$ and $\bsig^{2 \star}_1$ in the mixture components differ
by the value of $z_{j \lambda_{in}}$ and also vary across samples. We let, for
\(i=1, \ldots, I\),
\begin{align*}
\sigma^2_{0i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^0}, \\
\sigma^2_{1i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^1}.  
\end{align*}
We also assume $s_i \iid \G(a_s, b_s)$, $i \in \bc{1,...,I}$, with mean
\(a_s/b_s\). Lastly, we consider a model for mixture weights $\bm\eta^0_{ij}$ and $\bm\eta^1_{ij}$. To flexibly model
the distribution of $y$, we assume for a marker in a sample have two sets of it
own weights, one for each value of $z$, $\bm\eta^0_{ij}$ and $\bm\eta^1_{ij}$,
for each $(i, j)$. So for \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1,
\ldots, J\),
\begin{align*}
\bm\eta^0_{ij} &\iid \Dir_{L^0}(a_{\eta^0}/L^0), \\
\bm\eta^1_{ij} &\iid \Dir_{L^1}(a_{\eta^1}/L^1). 
\end{align*}


\paragraph*{Parameters for Missingness Mechanism}
A prior distribution over the missing mechanism can be specified through
placing priors on the parameters $\beta_{0i}$ and $\beta_{1i}$. 
%
We assume that $\beta_{0i} \iid \N(m_{\beta_0}, s^2_{\beta_0})$ and $\beta_{1i}
\iid \N_+(m_{\beta_1}, s^2_{\beta_1})$, $i=1, \ldots, I$.  We use data to
specify the values of the fixed hyperparameters, $m_{\beta_0}$ and
$m_{\beta_1}$. We let $s^2_{\beta_0}$ and $s^2_{\beta_1}$ be small to induce a
informative prior for $\beta_{0i}$ and $\beta_{1i}$. One way of determining
priors for the parameters in the missing mechanism is described in detail in 
the derivation of the full conditionals for $\beta$ in the appendix.



\subsubsection{Posterior Computation}\label{sampling-via-mcmc}
Let \(\btheta=\{\bZ, \bw, \bm \mu^\star_0, \bm \mu^\star_1, \bm \sigma^2_{0i}, \bm \sigma^2_{1i}, \bm \eta^0, \bm \eta^1, \bm \lambda, \bm \gamma, \bm v, \bm h, \bm \beta_0, \bm \beta_1\}\) represent all random parameters.  Let \(\y\) and \(\m\) denote all \(y_{inj}\) and \(m_{inj}$ for all \((i,n,j)\), respectively. The joint posterior distribution is 
\begin{align*}
p(\btheta \mid \y, \m) &\propto 
p(\btheta) \prod_{i,n,j} p(m_{inj} \mid y_{inj}, \btheta) p(y_{inj} \mid \btheta) \nonumber\\
&=  
p(\btheta)
\prod_{i,n,j} \left[
  p_{inj}^{m_{inj}} (1-p_{inj})^{1-m_{inj}} \times 
   \frac{1}{\sqrt{2\pi\sigma^2_{inj}}} \exp\bc{-\frac{(y_{inj}-\mu_{inj})^2}{2\sigma^2_{inj}}}
\right].
\end{align*}
%The marginal density for \(y_{i,n,j}\) after integrating out
%\(\lambda\) and \(\gamma\) is
%\begin{align}
%p(y_{inj} \mid \btheta) = \sum_{k=1}^K W_{ik} \sum_{\ell=1}^{L^{Z_{jk}}}
%\eta^{Z_{jk}}_{ijl} \cdot \N(y_{inj} \mid \mu^\star_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{jk},\ell}).
%\end{align}
Posterior simulation can be done via Gibbs sampling by repeatedly updating each
parameter one at a time until convergence. Parameter updates are made by
sampling from it full conditional distribution. Where this cannot be done
conveniently, a metropolis step can be used.
%
%{\tt do you use the marginal density of $y$ in posterior simulation?  If so,
%explain why we use it.}
%
Details of the posterior simulation are described in Appendix A. 



Summarizing the joint posterior distribution $p(\btheta \mid \y, \m)$ is
challenging.  Especially, posterior summary of $\bw$ and $\bm \lambda$ are dependent on $\bZ$. We use a method based on sequentially-allocated latent structure optimization
(SALSO) \citep{salso} to find point estimates for $\Z$, $\bm W$, and $\lambda$.  SALSO first constructs $A(\bZ)=\{A_{j,j'}\}$, the $J \times J$ pairwise allocation matrix corresponding to a binary matrix $\bZ$, where  $A_{j,j'} = \sum_{k=1}^K \mathbb{I}(Z_{j,k}=1)\mathbb{I}(Z_{j',k}=1), 1\leq j, j^\prime \leq J$ is the number of features that markers $j$ and $j'$ share. It then uses posterior samples of $\bZ$ and finds a point estimate $\hat{\bZ}$ that minimizes the sum of elementwise squared distances, 
\begin{eqnarray*}
\text{argmin}_Z\sum_{j=1}^J\sum_{j'=1}^J(A(\bZ)_{j,j'} - \bar{A}_{j,j'})^2
\label{eq:salso}
\end{eqnarray*}
where $\hat A$ is the pairwise allocation matrix averaged over all posterior
samples of $Z$.  %Thus in the current application, $A_{j,j'} = \sum_{k=1}^K
%\Ind{Z_{j,k}=1}\Ind{Z_{j',k}=1}$ is the number of times that marker $j$ and
%marker $j'$ share a feature. 
%
We extend SALSO to point estimates for each sample $i$, $\hat{\bZ}_i$ by incorporating $\bw_i$. That is, we consider the sum of elementwise squared distances weighted by $\bw_i$.  We use posterior Monte Carlo samples to posterior point estimates $\hat{\Z}_i$, $\hat{\bm W}_i$ and $\hat{\lambda}_{in}$, $i=1, \ldots, I$ and $n=1, \ldots, N_i$ as follows; suppose we obtain \(B\) posterior samples simulated from the posterior
distribution of \(\theta\). For each posterior sample of \(\Z\) and \(\bw_i\), we compute a $J \times J$ adjacency matrix, \(\bm A_i^{(b)}
=\{A^{(b)}_{i,j,j'}\}\), where 
\[
A^{(b)}_{i,j,j'} = \sum_{k=1}^K w^{(b)}_{ik} 
\mathbb{I}\left( z^{(b)}_{jk} = 1\right)
\mathbb{I}\left(z^{(b)}_{j^\prime k} = 1\right), b \in \bc{1,...,B}.
\]
We then compute the mean adjacency matrix \(\bar A_i = \sum_{b=1}^B A_i^{(b)} /
B\).  We report a posterior point estimate of $\Z_i$ by choosing
\begin{eqnarray}
\hat{\bm Z}_i = \text{argmin}_{\bm Z} \sum_{j,j'} (A_{i,j,j'}^{(b)} - \bar A_{i,j,j'})^2).\label{eq:myZ}
\end{eqnarray}
Conditioning on $\hat{\Z}_i$, we report posterior point estimates $\hat{\bw}_i$ and $\hat{\lambda}_{in}$. %Note that in our adaptation, we weight
%the adjacency counts by the sample-specific weight for each latent feature.
Eq\eqref{eq:myZ} places greater weight on phenotypes that are more prevalent in samples, and downweights pheotnyes having small $w_{ik}$ for $\hat{\bZ}_i$.

%%% sim-study-proj1 %%%
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
  \begin{center}
\begin{tabular}{c}
\includegraphics[scale=.8]{img/sim/Z_true_all.pdf}
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{The transpose of $\bZ^\true$ with markers in columns and latent phenotypes in rows. Black and white represents $z^\true_{jk}=0$ and 1, respectively.    The phenotypes and $\bw^\true_i$ are shown on the left and right sides of each panel.  The phenotypes are arranged in order of $w_{ik}^\true$ within each sample.}
\label{fig:sim-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Simulation Study}\label{sec:CB-sim} %%% Change name to Simulation Study?
We conducted a simulation study to  to evaluate the performance of the proposed model and compare to those of some existing methods.
%
%To understand the strengths and limitations of the proposed model, a
%simulation study was conducted. The study generated data that resemble the
%cord blood (CB) CyTOF data. This section details the data-simulation strategy
%and the results of the analysis on the simulated data using the proposed model.
%
%\subsubsection{Data Simulation}
To simulate data, we assume $J=32$ markers and three samples ($I=3$) and let the numbers of cells in
samples $N_i=30000$, 20000 and 10000, $i=1, 2, 3$.
We let the true number of latent cell types $K^\true=10$.  We specified
$\bZ^\true$ and $\bw^\true_i$, $i=1,2,3$ as follows;
%
% TODO. DONE
%{\tt give specific distributions used for $\bw$ and explain the details.} 
% Simulating Z
We first simulated $\bZ^\true$ by setting $\bZ^\true_{jk}=1$ with probability
0.5 for $j=1,...,J$ and $k=1,...,K$. If any column in $\bZ^\true$ is a column
of only 0's, the entire matrix is re-sampled.
% Simulating W
We then simulated $\bw^\true_i$ from a Dirichlet distribution with parameters
being some permutation of (1,..,K). This encourages the simulated values
in $\bw^\true_i$ to contain large as well as small values.
%
Figure \ref{fig:sim-Z} shows the transpose of $\bZ^\true$ and $\bw^\true_i$ for
the samples. In each panel, the cell types ($\bZ^\true$) are rearranged using
$w_{ik}^\true$.  Three and four mixture components were used for non-expressed
and expressed marker expression levels, respectively, $L^{0, \true}=3$ and
$L^{1, \true}=4$. We set values for $\mu^{\star, \true}_{0\ell}$ and
$\mu^{\star, \true}_{1\ell}$ using empirical values from CB data; $\mu^{\star, \true}_{0} =
(-1, -3, -5)$ and $\mu^{\star, \true}_{1} = (1, 2.33, 3.67, 5)$. 
$\sigma^{\star 2, \true}_{0i\ell}$ and $\sigma^{\star 2,\true}_{1i\ell}$ were fixed at $0.1$ for all $i$ and $\ell$.
%
We simulated $\bet_{0ij}^\true$ and $\bet_{1ij}^\true$, $i=1,2,3$ and $j=1, \ldots, J$ similar to $\bw^\true_i$;
%
% TODO. Done
%{\tt Give specific distributions used for $\bet$ and explain the details.} 
We simulated $\bet_{0ij}^\true$ from a Dirichlet distribution with
parameters being some permutation of $(1,...,L^0)$. Similarly, 
$\bet_{1ij}^\true$ was simulated from a Dirichlet distribution with
parameters being some permutation of $(1,...,L^1)$.
%
We then simulated latent phenotype indicators $\lambda_{in}^\true$ using
$\bw_i^\true$ and conditional on $z^\true_{j,\lambda_{in}^\true}$ generated
$y_{inj}$ from a mixture model, $\sum_{\ell=1}^{L^{0,\true}}
\eta^\true_{0ij}\cdot\N(\mu^{\star, \true}_{0\ell}, \sigma^{\star 2,
\true}_{0i\ell})$ or $\sum_{\ell=1}^{L^{1,\true}}
\eta^\true_{1ij}\cdot\N(\mu^{\star, \true}_{1\ell}, \sigma^{\star 2,
\true}_{1i\ell})$.   Finally, probabilistically let some of the $y_{inj}$
missing as follows; simulate a proportion $p_{ij}$ of missing for marker
$j$ in sample $i$, from a uniform distribution between 0 and  $\sum_k
w^\true_{ik}(1-z^\true_{jk})$, sample $p_{ij}\times N_i$ cells without
replacement with probability proportional to
% TODO. DONE
%{\tt give the function used}.
$$
\begin{cases}
  \text{logit}^{-1}\p{4.6 - 0.42(y_{inj}+3)^2}, & \text{if } y_{inj} < -3 \\
  \text{logit}^{-1}\p{4.6 - 0.42\sqrt{y_{inj}+3}}, & \text{otherwise.} \\
\end{cases}
$$
%
Under the true missingness mechanism, $y$ taking a negative value has a larger
chance to be missing, while $y$ with a positive value has only slight chance of
being missing.  Note that the true mechanism is different from that assumed in
the proposed model.   
Heatmaps of the simulated $\y$ are shown in the heatmaps on the top of each
panel in Figure \ref{fig:sim-post-Z}. $y_{inj}$'s are sorted within a sample
according to their posterior phenotype estimates.  Red, blue and white colors
represent high expression levels, low expression levels, and missing values,
respectively.


To fit the proposed model, we fix the hyperparameters as
$(K=12, L^0=5, L^1=5)$. In addition, we fixed $c_0=-2.5$, which was
the mean of the observed data having negative values.
We fixed $\Gamma=\bI_J$, $J\times J$ identity matrix, and $\beta_{1i}$ for
simplification. To run the MCMC simulation, we initialized the parameters
as follows;
Missing values are initialized at $c_0$. $\lambda_{in}$ is sampled uniformly
from $\bc{1,...,K}$. $\gamma_{inj}$ is sampled uniformly from
$\bc{1,...,\min(L^0,L^1)}$. $v_k$ is initialized at $1/K$. $H_{jk}$ is randomly
sampled from the standard normal distribution. $\bZ$ is computed according
to the initialized $H$ and $v$. All other parameters are initialized at
their prior means.
Initial values of $\lambda_{in}$ are used to initialize $\mu^{\star}_{0\ell}$, $\mu^{\star}_{1\ell}$, $\bZ$ and $\bw_i$.
We then implemented posterior inference using MCMC simulation
over 3,000 iterations, discarding the first 1,000 iterations as burn-in.
We diagnose convergence and mixing of the described posterior MCMC simulation using trace plots. We found no evidence of practical convergence problems. 

Figure \ref{fig:sim-post-Z} summarizes the posterior inference for the simulated data.  The posterior point estimates $\hat{\bZ}_i$ and $\hat{\bw}_i$ are obtained using the method described in \S~\ref{sampling-via-mcmc}.  
%
%provides
%representation of the data $y$ sorted by a point-estimate of their cell-types,
%for each sample.  These figures are supplemented by a point-estimate of $\Z$
%and $W$. 
The bottom of each panel illustrates $\hat{\bZ}_i$ with $\hat{\bw}_i$ in percentages on the right side. Among $K=XX$ (give number) phenotypes, phenotypes having large $\hat{w}_{ik}$ that make up more than 90\% of cells are included in the plots of $\hat{\bZ}_i$.    Comparing their truth in Figure~\ref{fig:sim-Z}, $\hat{\bZ}_i$ and $\hat{\bw}_i$ are very close to $\bZ^\true$ and $\bw_i^\true$ for all sample. Note that the phenotype labels do not match in the figures due to the fact that the model for $\bZ$ is invariant under relabelling of the phenotyps. For example, phenotype 1 in $\bZ^\true$ has $w^\true_{ik}=23.6\%$ for sample 1.  That phenotype is shown as phenotype 8 in the very bottom of $\hat{\bZ}_i$ with $\hat{w}_{ik}=24\%$.   %Phenotypes with small $w^\true_{ik}$ are not well captured in $\hat{\bZ}_i$.  For example, cell type 7 ($k=7$) with $w^\true_{1k}=4.5\%$ is not well identified.  On the other hand, it is well captured for sample 2 since $w^\true_{27}=14.8\%$ in the truth. 
The heatmaps of observed $y$ arranged according to $\hat{\lambda}_{in}$ shows that the expression patterns in $y$ are well explained by $\hat{\bZ}_i$ and $\hat{\bw}_i$.  The top of each panel has a heatmap of $y$ rearranged by their $\hat{\lambda}_{in}$, with red, blue and white colors for large, small and missing values, respectively.  The horizontal dotted lines separate cells based on $\hat{\lambda}_{in}$.  It shows that the estimated phenotypes capture the expression patterns of $y$ well.    

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[th!]
  \begin{center}
\begin{tabular}{cc}
\includegraphics[scale=.48]{img/sim/YZ001.png}&
\includegraphics[scale=.48]{img/sim/YZ002.png}\\
(a) Sample 1 & (b) Sample 2\\
\includegraphics[scale=.48]{img/sim/YZ003.png}&\\
(c) Sample 3 & \\
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{[Simulation]  Heatmaps of $y$ for simulated data. Cells and markers are in rows and columns, respectively. Each column contains the expression levels of a
marker for all cells in the sample. High expression levels are red, low expression levels are blue, missing values are white.   Cells are rearranged by their posterior estimate of phenotype indicator, $\hat{\lambda}_{in}$.   Horizontal lines separate cells in different estimated phenotypes.
% TODO
{\tt make it more visible. TODO}
%
At the bottom of each panel, the transpose of $\hat{\Z}_i$ and $\hat{\bw}_i$ are provided for each sample. We include phenotypes having large $\hat{w}_{ik}$ to explain at least 90\% of the cells in a sample.
% TODO
{\tt make the figures more visible.  the resolution seems not high enough
especially for $\bw$. TODO}
%
}
\label{fig:sim-post-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%



%vary only rarely by
%small permutations in less common cell-types. The cell-types in $\Z$ are sorted
%by a point-estimate for $W_i$.  These also resemble $W_i^\true$ but vary
%occasionally from the truth for smaller values of $W_i$. Only the cell-types
%that make up the top 90\% of cells are included in Figure \ref{fig:sim-post-Z}.


%\textbf{Posterior Estimate for $\bZ$ and $W$}
%\beginmyfig
%\includegraphics[scale=.3]{img/sim/YZ001.png}
%\includegraphics[scale=.3]{img/sim/YZ002.png}
%\includegraphics[scale=.3]{img/sim/YZ003.png}
%\caption{$y$ sorted by a point-estimate of their cell-types ($\lambda$), for
%each sample.  Point-estimates of $\Z$ (sorted by $W_i$) and $W_i$ are provided
%for each sample below each $y_i$.}
%\label{fig:sim-post-Z}
%\endmyfig

% TODO
{\tt TODO.
\begin{itemize}
\item as we discussed, include posterior prob that a maker is not expressed if
  it is missing and posterior distributions of $y_{inj}$ for observed ones for
  each $(i,j)$. (I have this, just waiting for graphs.)

\item comparison of posterior predictive distribution to density estimates of
  observed $y$. (I have this, just waiting for graphs.)

\item comparison

\item sensitivity analysis ($K$ larger than $K^\true$ and smaller than
  $K^\true$. (I do not have this for smaller $K$...)

\end{itemize}
}



%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
  \begin{center}
\begin{tabular}{cc}
\includegraphics[scale=.25]{img/cb/YZ001.png}&
\includegraphics[scale=.25]{img/cb/YZ002.png}\\
(a) Sample 1 & (b) Sample 2 \\
\includegraphics[scale=.25]{img/cb/YZ003.png} &\\
(c) Sample 3 & \\
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{[CD Data]  Heatmaps of $y$ for the samples. Cells and markers are in
rows and columns, respectively. Each column contains the expression levels of
a marker for all cells in the sample. High expression levels are red, low
expression levels are blue, missing values are white.   Cells are rearranged
by their posterior estimate of phenotype indicator, $\hat{\lambda}_{in}$.
Horizontal lines separate cells in different estimated phenotypes.
% TODO
{\tt TODO: make it more visible}
%
At the bottom of each panel, the transpose of $\hat{\Z}_i$
and $\hat{\bw}_i$ are provided for each sample. We include phenotypes having
large $\hat{w}_{ik}$ to explain at least 90\% of the cells in a sample.}
\label{fig:cb-post-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%



%%% cb-proj1 %%%
\subsection{Cord Blood Data}\label{sec:CB-real}
We fit the proposed model to a real data set comprising cord blood samples from
three patients ($I=3$), with the number of cells $N=(41474, 10454, 5177)$ for $J=32$
markers. The heatmaps in Figure \ref{fig:cb-post-Z} illustrates heatmaps of
observed expression levels $y_{inj}$.  Markers and cells are in columns and
rows, respectively. The values are rescaled for better illustration.  Red, blue and
white colors represent high expression levels, low expression levels and missing
values, respectively.  From the figure, expression levels of some markers are
missing in most cells, e.g, marker CD25 (column 9) is missing in more than 80\%
of cells in all samples.
%
%From the provided information on the missingness mechanism, the marker is
%likely not to be expressed in most NK cell phenotypes.   
% TODO
%{\tt give a short summary about \% of missing values for $(i,j)$}
In each sample, the proportion of missing values in each marker can be as low
as 0.1\% and as high as 80\%.  
Median proportions of missing values across markers are 22\%, 17\% and 18\% in the samples, respectively.
%The median proportion of missing values across markers in sample 2 is 17\%.
%The median proportion of missing values across markers in sample 3 is 18\%.
%shows the data. 
%Each panel in the figure represents a different sample.  Each column in the
%panels contains the expression levels of a marker across all cells in the
%sample.  High expression levels are red, low expression levels are blue,
%and missing values are white.


We used hyperparameters similar to those in the simulation studies.
% TODO
%{\tt used similar hyperparameters?  check if this is right}. Yes
%
2000 samples from the posterior distribution were obtained after a burn-in
period of 1000 iterations. The results are summarized in Figure
\ref{fig:cb-post-Z}.  Point estimates $\hat{\bZ}_i$ and $\hat{\bw}_i$ are
obtained using the method in \S~\ref{sampling-via-mcmc}.
% illustrated at the bottom of each panel.   
Plots of $\hat{\bZ}_i$ are given with corresponding $\hat{\bw}_i$ at the bottom
of the panels.  From the estimated weights $\hat{\bw}_i$ the samples have some
common phenotypes such as phenotypes 17, 15 and 18.  In particular, phenotype
17 is the most abundant pheontype in all samples, 28.4\%, 23.8\% and 30.2\% for
samples 1, 2 and 3, respectively.  $\hat{\bw}_i$ also shows heterogeneity
across samples.  For example, phenotype 7 has the second largest $\hat{w}_i$ in
samples 2 and 3. Specifically, 22.4\% and 13.5\% of the cells in samples 2 and
3, respectively, possess phenotype 7, but only
% TODO
%{\tt XXX  Give a number here} \%
1.87\%
%
of the cells in sample 1 has the phenotype.  Similarly, phenotype 18 is more
prevalent in sample 1.  $\hat{w}_{ik}$ for the phenotype is 17.4\%, 4.7\%,
3.0\% in samples 1, 2 and 3, respectively.  Phenotypes 7 and 18 are different
dominantly for expression of markers 17, 18 and 19 corresponding to EOMES, GrA
(granzyme A) and GrB (granzyme B).  Also, note that some phenotypes are very
similar, for example, phenotypes 2 and 18 in panel (a).  It may be because
independence across columns under the prior model for $\bZ$ allows identical
columns with positive probability.  For the heatmaps on the top of the panels,
$y_{inj}$ are sorted by a posterior estimate $\hat{\lambda}_{in}$ of their cell
phenotypes. The horizontal dotted lines separate cells using
$\hat{\lambda}_{in}$.  From the heatmaps, the cells having a phenotype have
similar expression patterns, implying that the model provides reasonable
estimates of underlying cell subpopluations.

%does k
%But it should be noted that
%the estimated $\bZ$ matrix for each sample contains different cell-types.
%Moreover, within each sample, the cell-types may be repeated or vary by only
%one marker. We would like the learned cell-types to be different rather than
%very similar (or repeated). We can impose this constraint using a repulsive
%mixture prior \citep{petralia2012repulsive} for the cell-types. This serves as
%the motivation for Project 2, where we impose more structure in the prior 
%specification of the latent feature matrix $\bZ$ so that similar cell-types
%appearing in the matrix is discouraged, while distinctness is encouraged.

%Finally, it can be seen that the different samples consist of different
%sub-populations of NK-cells though some cell-types are shared across samples.
%Hence, we see that information can be borrowed across multiple samples to
%discover common cell-types across samples.

%\textbf{Assessing Model Fit}

To assess model fit, we compare the posterior predictive distribution of
$y_{inj}$ with $m_{inj}=0$ (observed) to the distribution of observed data.
% TODO
Figure XX 
%%
illustrates comparison of posterior predictive distributions (grey
lines) to kernel density estimates (blue lines) of observed $y_{inj}$ for some
selected ($i,j)$.
% TODO
The fit is  reasonably good for (... ) in panels (a)-(c).
For (...), the fit is poor possibly because the number of observations is small
%
% TODO
%{\tt ??? is this what you meant? or something different???}. YES.
%
and $y$ is missing for most of cells.  The plot also shows a posterior
probability that a marker is expressed for phenotypes possessed by cells in a
sample, $\hat{q}_1=\Prob(z_{j \lambda_{in}}=1 \mid \by, \bf m)$ for each
($i,j)$. When observed $y_{inj}$ is larger for many cells, $\hat{q}_1$ is
expected to be large.  We also compute $\hat{q}_2=\Prob(z_{j \lambda_{in}}=0
\mid \by, \bf m)$ for $(i,n,j)$ with $m_{inj}=1$ (missing), posterior
probability that a cell with missing value for marker $j$ has a phenotype for
which the marker is not expressed. 
% TODO
Figure~XXX 
%
illustrates a histogram of $\hat{q}_2$ for all $(i, n, j)$.  Recall that the
assumption made for the missingness mechanism is non-testable.   The figure
shows that when a cell has a missing value for a marker, the marker is not
expressed in the phenotype that the cell possesses, which complies with the
subject knowledge provided by biologists.  



% TODO
{\tt  TODO
\begin{itemize}
\item make plots as described above.
\item comparison??
\end{itemize} }

%We check the posterior probabilities that a 

% if the distribution is tipped more to the positive side, and close to zero otherwise.


%At the top of each sub-figure, the sample ($i$) and marker ($j$) are listed.
%The statistic $Z_ij$ mean refers to the posterior mean proportion of cells
%that express marker $j$ in sample $i$. We expect this statistic to be close 
%to one if the distribution is tipped more to the positive side, and close to
%zero otherwise. 


%The fit is mostly good but can be poor when the number of
%observations is small. The figures are annotated with a statistic $\hat
%P(Z=0\mid m=1, \text{data})$, which is the posterior mean probability that an
%observation recorded as missing will not express a marker. We expect this
%quantity to be close to 1. This is generally the case, except for samples and
%markers where there are only a few missing values and most of the expression
%levels are high.


%The blue and grey lines represent the posterior predictive distribution and kernel density estiamtes  the observed data density
%for $y>0$.



% Since the data contains missing values, we compare
%the posterior predictive density to the observed data augmented with the
%posterior mean of the imputed data for each sample and marker.  We have
%included these figures in Appendix \ref{sec:cb-pp}. 
%At the top of each sub-figure, the sample ($i$) and marker ($j$) are listed.
%The statistic $Z_ij$ mean refers to the posterior mean proportion of cells
%that express marker $j$ in sample $i$. We expect this statistic to be close 
%to one if the distribution is tipped more to the positive side, and close to
%zero otherwise.  Within the figures, the thick grey line is the posterior
%predictive density, and the red line is the data augmented with the posterior
%mean of the imputed values.  The thin grey line is the observed data augmented
%with one sample from the posterior distribution of the imputed values.

%Another way to assess model fit is through comparing the observed data
%distribution to the posterior predictive distribution of non-missing data.
%This requires storing $\gamma_{inj}$ at each iteration of the MCMC and is quite
%expensive storage-wise. A compromise is to compare the observed data having
%positive values to the positive values in the posterior predictive
%distribution.  This comparison is provided in Appendix
%\ref{sec:cb-pp-positive}. The blue line represents the posterior predictive 
%distribution for $y>0$, and the grey line represents the observed data density
%for $y>0$. The fit is mostly good but can be poor when the number of
%observations is small. The figures are annotated with a statistic $\hat
%P(Z=0\mid m=1, \text{data})$, which is the posterior mean probability that an
%observation recorded as missing will not express a marker. We expect this
%quantity to be close to 1. This is generally the case, except for samples and
%markers where there are only a few missing values and most of the expression
%levels are high.


\subsection{Conclusions}\label{sec:CB-conc}

% TODO
%{\tt expand}

We proposed a Bayesian FAM to study NK-cell
diversity from mass cytometry data in the presence of missing data. We used a
simulation study to show that our model is able to recover the true latent
feature allocation matrix generating the observed data. We are also able to
learn the true abundance of cell-types when many observations are available. 
%
Assumptions about the missing mechanism are untestable. But through providing
an informed prior distribution on the missing mechanism, we are able to impute
data missing not at random such that most of the imputed values are correspond
to the non-expression of markers. This is consistent with what scientists
believe about precision of the cytometry devices.
%
While not explicitly shown, the posterior variance for $\bZ$ can be computed
easily from the posterior samples of $\bZ$ when the number of cell types $K$ is fixed. This will help in
quantifying uncertainty about the learned NK-cell subpopulations. 
%
A comparison of our model to FlowSOM remains to be conducted. 
%

When applied to real cord blood data, we find that the cell-types learned by
the model tend to be similar, and sometimes repeated. This can be remedied
using a repulsive FAM and will be included in Project 2. %Moreover, the ideas in this project may be extended to account for sample-specific
%covariates and time-dependency across samples. These will be explored in
%Projects 2 and 3.





\section{Project 2: Repulsive Feature Allocation Model}\label{sec:proj2}
\subsection{Introduction}
In project 2, we propose a repulsive FAM (rep-FAM), where repulsion discourages similar features. The traditional IBP assumes a priori independence between features and may yield redundant features. Thus, the independence assumption may not be desirable in many applications.  The concept of repulsion is introduced to penalize creating similar features, resulting in a more parsimonious representation of the underlying structures.  The property of inducing parsimony can be more critical for analyzing heterogeneous samples collected from different backgrounds, such as joint analysis of cord blood samples and samples collected from healthy subjects.  Different approaches for repulsive models have been developed mostly in the context of mixture models \citep{petralia2012repulsive, quinlan2017parsimonious, xie2017bayesian, quinlan2017density}.  Independent priors for component specific parameters in a mixture are commonly assumed, for example, Dirichlet process mixture model, where the atoms are iid from the baseline distribution and mixture weights are constructed through stick-breaking. The independence assumption produces redundant mixture components located close together, resulting in overfitting. To separate mixture components, the repulsive models include a repulsion function and assume a joint model for all component specific parameters.  The models smoothly push the components apart based on pairwise distances through some repulsion parameters, resulting in well separated clusters.  Different from the others, \cite{xu2016bayesian} used the determinantal point process (DPP) for repulsive mixture models and feature allocation model.  We exploit the repulsive mixture model developed in \cite{quinlan2017density} to create repulsive features.  The rep-FAM explicitly incorporates a model for the repulsion that penalizes inclusion of similar features. Properties of the proposed rep-FAM is explored through simulation studies and compared to the IBP.   In addition, the model is further extended to let samples possess a subset of cell phenotypes. In our applications, some cell phenotypes can only be present in cord blood samples or healthy subject samples, while some cell types are shared by both.  By letting abundances exactly zero for some phenotypes, different sets of phenotypes can be used to describe samples.  The remainder of this section will outline the proposed rep-FAM in \S~\ref{sec:rep-model} and present simulation studies to examine the rep-FAM and comparison to the IBP in \S~\ref{sec:rep-sim}.


{\tt Read all references on repulsive models that I sent and find more and read them as well.  }




\subsection{Probability Model}\label{sec:rep-model}
Similar to the notation in \S~\ref{prob-model}, suppose that $I$ samples are taken from subjects, \(i = 1,2,...,I\). Sample \(i\)
consists of \(N_i\) cells, \(n=1, \ldots, N_i\) and for each cell,
expression levels of \(J\) markers are measured.  We introduce $x_i$ to denote covariates for sample $i$.  In our data of cord blood samples and healthy subject samples, let $x_i=0$ or 1 if sample $i$ is a cord blood sample or a healthy subject sample, respectively.  We also let transformed observed expression levels $y_{inj}$ of marker $j$ in cell $n$ for sample $i$ and its binary missingness indicator $m_{inj}$, where $m_{inj} = 0$ if $y_{inj}$ is observed and $m_{inj} = 1$ otherwise.  We assume the sampling distributions in \eqref{eq:y-mix} and \eqref{eq:missing} for $y_{inj}$ and $m_{inj}$.

\paragraph*{Repulsive Feature Allocation Model:} Recall that $J\times K$ binary matrix $\bZ$ characterizes $K$ different cell phenotypes.  Let $v_k \mid \alpha \iid \Be(\alpha, 1)$, $k=1, \ldots, K.$ We define a joint distribution of $\bZ=[\bz_1, \ldots, z_K]$;
\begin{eqnarray}
P(\bZ \mid v, C_\phi) \propto \prod_{k=1}^K  \bc{\prod_{j=1}^J v_k^{z_{jk}}(1-v_k)^{1-z_{jk}}} \times
\prod_{k_1=1}^{K-1}\prod_{k_2=k_1+1}^{K} \left\{1 - C_\phi\p{\rho(\bz_{k_1}, \bz_{k_2})}\right\},  \label{eq:rep-FAM}
\end{eqnarray}
where $\rho(\bz_{k_1}, \bz_{k_2})$ measure distance between columns $k_1$ and $k_2$, $k_1 \neq k_2$ and $C_\phi(\cdot)$ is a continuous nondecreasing function in distance with $C_\phi(0)=1$ and $\lim_{d\rightarrow\infty}C_\phi(d)= 0$. For a distance metric, we use $\rho(\bz_{k_1}, \bz_{k_2})=\sum_{j=1}^J \abs{z_{jk_1} - z_{jk_2}}$, the number of discordance between columns $k_1$ and $k_2$.  The function $C_\phi(\cdot)$ can be interpreted as a proximity function. A
suitable form is $C_\phi(d) = \exp\p{-d/\phi}$. \cite{quinlan2017parsimonious} showed that the model in \eqref{eq:rep-FAM} has a finite normalizing constant and the distribution is proper.  Under the model in \eqref{eq:rep-FAM}, probability 0 is assigned to $\bZ$ having identical columns.  For matrices $\bZ$ that have the same number of 0's and 1's, $\bZ$ with similar columns has a smaller probability since $C_\phi(\cdot)$ is decreasing in distance. $C_\phi(\cdot)$ smoothly penalizes any $\bZ$ having similar columns in prior and can remove redundant columns in posterior inference. Note that different from the IBP, under the model in \eqref{eq:rep-FAM}, $\Prob(z_{jk}=1) \neq v_k$ due to the repulsive function.  We place a prior on $\alpha$, $\alpha
\sim \Gamma(a_\alpha, b_\alpha)$


\paragraph*{Feature Selection} We assume that cord blood samples ($x_i=0$) and healthy subject samples ($x_i=1$) can have a distinct set of cell phenotypes. We introduce binary indicators, $\delta_{xk}\in \{0, 1\}$, $x=0,1$ and $k=1,\ldots, K$ to indicate whether samples with $x$ possess  phenotype $k$ ($\delta_{xk}=1$) or do not possess ($\delta_{xk}=0$). Assume $\delta_{xk} \ind \Ber(p_{x})$ and $p_x \iid \Be(a_p, b_p)$.  Let unnormalized cell phenotype abundances $\tilde w_{ik} \ind \G(a_W/K, 1)$, $i=1, \ldots, I$ and $k=1, \ldots, K$ and we define relative abundances in sample $i$ having $x_i$ as 
$
w_{ik} =\tilde w_{ik} \delta_{x_i k}/\sum_{\ell=1}^K\tilde w_{i\ell} \delta_{x_i \ell}.
$
Relative abundance $w_{ik}$ is exactly zero for $\delta_{x_ik}=0$. Samples having $x$ have the same subset of phenotypes but can have different relative abundances over the selected phenotypes. Phenotypes with $\delta_{0k}=\delta_{1k}=1$ appear in all samples, while some present in only one type of samples.   Feature selection by $\delta_{xk}$ efficiently facilitates joint analysis of samples obtained from different sources. The model also allows phenotypes absent in all samples, implying that some phenotypes are not used to describe any cells, providing more parsimonious representation of cell populations in samples. The probability models for the other parameters remain unchanged as in \S~\ref{prob-model}.  



%TODO: HERE
\subsection{Simulation Study}~\label{sec:rep-sim}
TODO. Show difference between IBP and rep-FAM.


\section{Project 3: Feature Allocation Model with Regression for Abundances of Features}\label{sec:proj3}
% TODO
In Project 3, we further extend the proposed FAM to analyze samples taken at multiple time points from a patient after NK cell infusion. Suppose $I$ samples are taken at time points $t_1, \ldots, t_I$.   As a NK cell population evolves over time, samples may have different sets of cell phenotypes with different abundances.    We achieve this goal by letting phenotype abundances $\bw_{t}=(w_{t1}, \ldots, w_{tK})$ be a function of time $(t)$ after
treatment. Inferred $\bw(t)$ reflects the evolutionary process of cell subpopulation expansion over time.   $\bZ$ includes all phenotypes that can be possessed in a sample and different composition of NK cell populations in different samples is modeled through  phenotype abundances $w_{t_i, k}$, $i=1, \ldots, I$ that change over time in a time-dependent manner.


Similar to the model in \S~\ref{sec:proj2}, let $\xi_{t_1,k}$ represent unnormalized abundance of phenotype $k$ in sample 1 collected at time $t_1$ and we obtain relative abundances by rescaling $w_{t_1,k}= \xi_{t_1,k}/\sum_{\ell=1}^K \xi_{t_1, \ell}$.  We fix $\xi_{t_1,1}=a$ a positive arbitrary number, to avoid potential identifiability issue and let $\xi_{t_1,k} = \max(\xi^\prime_{t_1,k}, 0)$, for $k\ge 2$, where $\xi^\prime_{t_1,k} \iid
\N(0, s^2_1)$.  Since $\xi_{t_1, 1}$ is fixed at $a>0$, phenotype 1 is present in sample 1 and its relative abundance is determined by $\xi_{t_1, 2}, \ldots, \xi_{t_1, K}$.  For any phenotype with $\xi^\prime_{t_1, k} < 0$, $k=2, \ldots, K$, its relative abundance is zero and the phenotype is absent.  That is, sample 1 can have a subset of $K$ phenotypes and if the cells in sample 1 has the same phenotype, the phenotype is set to be phenotype 1. Values of $a$ and $s^2_1$ need to be jointly calibrated. For the remaining samples, we assume $\xi_{t_i,k} =
\max(\xi^\prime_{t_i,k}, 0)$, $i=2, \ldots, I$ and $k=1, \ldots, K$ with $\xi^\prime_{t_i,k} = \xi^\prime_{t_1, k} + f_k(t_i)$. $f_k(t)$ is a function of time and phenotype specific.   $\xi^\prime_{t_1, k}$ is served as a baseline abundance of phenotype $k$ and $f_k(t)$ explains how abundance of phenotype $k$ changes over time.    Depending on $f_k(t)$, samples may have different subsets of phenotypes. Various functions can be used for $f_k(t)$.  One choice for $f_k(t_i)$ is a quadratic function in time $t$, $f_k(t) = \xi^\prime_{t_1,k} + \beta_{k1}t + \beta_{k2}t^2$, possibly with some constraints on $\beta_{k1}$ and $\beta_{k2}.$  For example, if $\beta_{k2} \leq 0$, the mean abundance of a phenotype cannot bounce up.  We will further investigate to accommodate biological knowledge on dynamics of cell populations in the model for $\bw$ and potentially in the model for $\bZ$.   



\section{Timeline}\label{sec:time}
%T

\bibliographystyle{natbib}
\bibliography{litreview.bib}

\end{document}


\appendix
\section{Posterior Computation for Project 1}
This section presents detailed derivations of the full conditional
distributions for each model parameter. Using traditional Markov
chain Monte Carlo (MCMC), we can sample from the joint posterior 
distribution of the parameters.

To sample from a distribution which is otherwise difficult to sample
from, the Metropolis-Hastings algorithm can be used. This is
particularly useful when sampling from a full conditional distribution
of one of many parameters in an MCMC based sampling scheme (such as a
Gibbs sampler). Say \(B\) samples from a distribution with density
\(p(\theta)\) is desired, one can do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide an initial value for the sampler, e.g. \(\theta^{(0)}\).
\item
  Repeat the following steps for \(i = 1,...,B\).
\item
  Sample a new value \(\tilde\theta\) for \(\theta^{(i)}\) from a
  proposal distribution \(Q(\cdot \mid \theta^{(i-1)})\).

  \begin{itemize}
  \tightlist
  \item
    Let \(q(\tilde\theta \mid \theta)\) be the density of the proposal
    distribution.
  \end{itemize}
\item
  Compute the ``acceptance ratio'' to be \[
     \rho=
     \min\bc{1, \frac{p(\tilde\theta)}{p(\theta^{(i-1)})} \Big/ 
            \frac{q(\tilde\theta\mid\theta^{(i-1)})}
                 {q(\theta^{(i-1)}\mid\tilde\theta)}
        }
     \]
\item
  Set \[
     \theta^{(i)} := 
     \begin{cases}
     \tilde\theta &\text{with probability } \rho \\
     \theta^{(i-1)} &\text{with probability } 1-\rho.
     \end{cases}
     \]
\end{enumerate}

Note that in the case of a symmetric proposal distribution, the
acceptance ratio simplifies further to be
\(\frac{p(\tilde\theta)}{p(\theta^{(i-1)})}\).

The proposal distribution should be chosen to have the same support as
the parameter. Transforming parameters to have infinite support can,
therefore, be convenient as a Normal proposal distribution can be used.
Moreover, as previously mentioned, the use of symmetric proposal
distributions (such as the Normal distribution) can simplify the
computation of the acceptance ratio.

Some common parameter transformations are therefore presented here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For parameters bounded between \((0,1)\), a logit-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has support in the unit interval, then
  \(Y=\logit(X)=\log\p{\frac{p}{1-p}}\) will have density
  \(f_Y(y) = f_X\p{\frac{1}{1+\exp(-y)}}\frac{e^{-y}}{(1+e^{-y})^{2}}\).
\item
  For parameters with support \((0,\infty)\), a log-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has positive support, then \(Y = \log(X)\) has pdf
  \(f_Y(y) = f_X(e^y) e^y\).
\end{enumerate}



\vspace{5em}
\hrule
\vspace{5em}

% fc-v
\textbf{Full Conditional for $\bm v$}

The prior distribution for \(v_k\) are
\(v_k \mid \alpha \ind \Be(\alpha/K, 1)\), for \(k = 1,...,K\). So,
\(p(v_k \mid \alpha) = \alpha v_k^{\alpha/K-1}\).

Let \(S = \bc{(i,n)\colon \lin = k}\).

\begin{align*}
p(v_k \mid \y, \rest) &\propto p(v_k) \prod_{j=1}^J\prod_{(i,n)\in S} p(\y \mid v_k, \rest) \\
&\propto (v_k)^{\alpha/K-1} \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhLogitSpiel{v_k}{\xi}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(v_k\), and should be computed accordingly. Moreover, we
will only recompute the likelihood (in the metropolis acceptance ratio)
when \(Z_{jk}\) becomes different.
\vspace{2em}


% fc-alpha
\textbf{Full Conditional for $\alpha$}

\begin{align*}
p(\alpha \mid \y, \rest) &\propto p(\alpha) \times \prod_{k=1}^K p(v_k \mid \alpha) \\
&\propto \alpha^{a_\alpha - 1} \exp\bc{-b_\alpha \alpha} \times \prod_{k=1}^K 
\alpha~v_k^{\alpha/K} \\
&\propto \alpha^{a_\alpha + K -1} \exp\bc{-\alpha\p{b_\alpha - 
\frac{\sum_{k=1}^K \log v_k}{K}}}
\end{align*}

\[
\therefore \alpha \mid \y, \rest \sim 
\G\p{a_\alpha + K,~ b_\alpha - \frac{\sum_{k=1}^K \log v_k}{K}}
\]
\vspace{2em}


% fc-H
\textbf{Full Conditional for $\bm H$}

The prior for \(\h_k\) is \(\h_k \sim \N_J(0, \Gamma)\). We can
analytically compute the conditional distribution
\(h_{j,k} \mid \h_{-j,k}\), which is

\[
h_{jk}  \mid \h_{-j,k} \sim \N(m_j, S^2_j),
\]

where

\[
\begin{cases}
m_j &= \bm G_{j,-j} \bm G_{-j,-j}^{-1}(\h_{-j,k})\\
S_j^2 &= \bm G_{j,j} - \bm G_{j,-j}\bm G_{-j,-j}^{-1}\bm G_{-j,j}\\
\end{cases}
\]

and the notation \(\h_{-j,k}\) refers to the vector \(h_k\) excluding
the \(j^{th}\) element. Likewise, \(\bm G_{-j,k}\) refers to the
\(k^{th}\) column of the matrix \(\bm G\) excluding the \(j^{th}\) row.

Note that if \(\bm G = \I_J\), then \(m_j=0\) and \(S_j^2 = 1\). Let
\(S = \bc{(i,n)\colon \lin=k}\).

\begin{align*}
p(h_{jk} \mid \y, \rest)  &\propto p(h_{jk}) \prod_{(i,n) \in S} p(y_{inj} \mid h_{jk}, \rest) \\
%
&\propto
\exp\bc{\frac{-(h_{jk} - m_j)^2}{2S_j^2}}
 \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhSpiel{h_{jk}}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(h_{jk}\), and should be computed accordingly. Moreover,
we will only recompute the likelihood (in the metropolis acceptance
ratio) when \(Z_{jk}\) becomes different.  
\vspace{2em}


% fc-lam
\textbf{Full Conditional for $\bm \lambda$}

The prior for \(\lin\) is \(p(\lin = k \mid \bm W_i) = W_{ik}\).

\begin{align*}
p(\lin=k\mid \y,\rest) &\propto p(\lin=k) ~ p(\y \mid \lin=k, \rest) \\
&\propto W_{ik}
\prod_{j=1}^J 
\p{
  \sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
  \N(y_{inj} \mid 
  \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
}\\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(k = 1,...,K\). Moreover, since \(k\) is discrete, a Gibbs update can
be done on \(\lin\).

\textbf{Full Conditional for $\bm W$}

The prior for \(\bm{W}_i\) is \(\bm W_i \sim \Dir(d, \cdots, d)\). So
the full conditional for \(\bm{W}_i\) is:

\begin{align*}
p(\bm W_i \mid \rest) \propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i} p(\lin \mid \bm{W}_i)\\
\propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\mathbb{I}{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{d/K-1} \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\Ind{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{\p{d/K + \sum_{n=1}^{N_i}\Ind{\lin=k}}-1}\\
%
\end{align*}

Therefore, \[
\bm{W}_i \mid \y,\rest ~\sim~ \Dir\p{d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=1},...,d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=K}} 
\]

Consequently, the full conditional for \(\bm{W}_i\) can be sampled from
directly from a Dirichlet distribution of the form above.
\vspace{2em}


% fc-mu
\textbf{Full Conditional for $\bm\mu^\star$}

For \(\mus_{0\ell}\)g, let
\(S_{0i\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 0 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{0i\ell}|\) the cardinality of \(S_{0i\ell}\).

\newcommand\musZeroPostvarDenom{
  \frac{1}{\tau^2_0} + \sum_{i=1}^I\frac{|S_{0i\ell}|}{{\sigma^2}^\star_{0i\ell}}
}
\newcommand\musZeroPostMeanNum{
  \frac{\psi_0}{\tau^2_0} + 
  \sum_{i=1}^I \sum_{S_{0i\ell}}  
  \frac{y_{inj}}{{\sigma^2}^\star_{0i\ell}}
}

\begin{align*}
p(\mus_{0\ell} \mid \y, \rest) &\propto 
p(\mus_{0\ell} \mid \psi_0, \tau^2_0) \times p(\y \mid \mus_{0\ell},\rest) \\
%
&\propto
\Ind{\mus_{0\ell}<0} \exp\bc{\frac{-(\mus_{0\ell} - \psi_0)^2}{2\tau^2_{0}}}
\prod_{i=1}^I\prod_{(i,n,j)\in S_{0i\ell}} \exp\bc{\frac{-(y_{inj} - \mus_{0\ell})^2}{2{\sigma^2}^\star_{i0\ell}}} \\
%
&\propto
\exp\bc{
  -\frac{(\mus_{0\ell})^2}{2}\p{\musZeroPostvarDenom} + 
  \mus_{0\ell}\p{\musZeroPostMeanNum}
} \\ 
& ~~~ \times \Ind{\mus_{0i\ell}<0} \\
\end{align*}

\[
\renewcommand\musZeroPostvarDenom{
  1 + \tau^2_0\sum_{i=1}^I(|S_{0i\ell}|/{\sigma^2}^\star_{0i\ell})
}
\renewcommand\musZeroPostMeanNum{
  \psi_0 + \tau^2_0 \sum_{i=1}^I\sum_{S_{0i\ell}} (y_{inj} / {\sigma^2}^\star_{0i\ell})
}
\therefore \mus_{0l} \mid \y, \rest \ind \N_-\p{
  \frac{\musZeroPostMeanNum}{\musZeroPostvarDenom},
  \frac{\tau^2_0}{\musZeroPostvarDenom}
}
\]

Similarly for \(\mus_{1\ell}\)g, let
\(S_{1\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 1 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{1i\ell}|\) the cardinality of \(S_{1i\ell}\).

\[
\newcommand\musOnePostvarDenom{
  1 + \tau^2_1 \sum_{i=1}^I (|S_{1i\ell}|/{\sigma^2}^\star_{1i\ell})
}
\newcommand\musOnePostMeanNum{
  \psi_1 + \tau^2_1 \sum_{i=1}^I \sum_{S_{1i\ell}} (y_{inj} / {\sigma^2}^\star_{1i\ell})
}
\therefore \mus_{1l} \mid \y, \rest \ind \N_+\p{
  \frac{\musOnePostMeanNum}{\musOnePostvarDenom},
  \frac{\tau^2_1}{\musOnePostvarDenom}
}
\]
\vspace{2em}


% fc-sig2
\textbf{Full Conditional for $\bm{{\sigma^2}}^*$}

Let
\(S_{0i\ell} = \bc{(i, n,j): Z_{j,\lin} = 0 ~\cap~ \gamma_{inj}=\ell}\),
\(i=1, \ldots, I\).

\begin{align*}
p(\sss_{0i\ell} \mid \y, \rest) &\propto p(\sss_{0i\ell} \mid s_i) \times p(\y \mid \sss_{0i\ell}, \rest) \\
&\propto (\sss_{0i\ell})^{-a_\sigma-1} \exp\bc{-\frac{s_i}{\sss_{0i\ell}}} 
\prod_{(i,n,j)\in S_{0i\ell}} \bc{
  \frac{1}{\sqrt{2\sss_{0i\ell}}}
  \exp\bc{\frac{-(y_{inj}-\mus_{0\ell})^2}{2\sss_{0i\ell}}}
} \\
&\propto (\sss_{0i\ell})^{-(a_\sigma + \frac{\abs{S_{0i\ell}}}{2})-1}
\exp\bc{\p{\frac{1}{\sss_{0i\ell}}}\p{s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}}.
\end{align*}

\[
\therefore \sss_{0i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{0i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}.
\]

Similarly, let
\(S_{1i\ell} = \bc{(i, n,j): Z_{j,\lin} = 1 ~\cap~ \gamma_{inj}=\ell}\).
Then, the full conditional for \(\sss_{1i\ell}\) is \[
\therefore \sss_{1i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{1i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{1i\ell}}
\frac{(y_{inj}-\mus_{1\ell})^2}{2}
}.
\]
\vspace{2em}


% fc-s
\textbf{Full Conditional for $s_i$}

\begin{align*}
p(s_i \mid \y, \rest) &\propto p(s_i) \times \prod_{z=0}^1 \prod_{\ell=1}^{L^z} p(\sss_{zi\ell} \mid s_i)\\
&\propto s_i^{a_s-1} \exp\bc{-b_s s_i} \times \prod_{z=0}^1  \prod_{\ell=1}^{L^z} s_i^{a_\sigma} \exp\bc{-s_i / \sss_{zi\ell}} \\
&\propto s_i^{a_s + (L^0 + L^1)a_\sigma - 1} \exp\bc{-s_i \p{b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} 1 / \sss_{zi\ell}}}.
\end{align*}

\[
\therefore s_i \mid \y, \rest \sim 
\G\p{a_s + (L^0 + L^1)a_\sigma, ~~ b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} \frac{1}{\sss_{zi\ell}} }.
\]
\vspace{2em}


% fc-gam
\textbf{Full Conditional for $\bm\gamma$}

The prior for \(\gamma_{inj}\) is
\(p(\gamma_{inj} = \ell \mid Z_{j\lin}=z, \eta^z_{ij\ell}) = \eta^z_{ij\ell}\),
where \(\ell \in \bc{1,...,L^z}\).

\begin{align*}
p(\gamma_{inj}=\ell \mid \y, Z_{j\lin}=z, \rest) &\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \gamma_{inj}=\ell, \rest) \\
&\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}, \rest) \\
%
&\propto \eta^z_{ij\ell} \times \N(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}) \\
&\propto \eta^z_{ij\ell} \times (\sss_{zi\ell})^{-1/2}
\exp\bc{-\frac{(y_{inj} - \mus_{z\ell})^2}{2\sss_{zi\ell}}} \\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(\ell = 1,...,L^z\). Moreover, since \(\ell\) is discrete, a Gibbs
update can be done on \(\gamma_{inj}\).
\vspace{2em}


% fc-eta
\textbf{Full Conditional for $\bm\eta$}

The prior for \(\bm\eta^z_{ij}\) is
\(\bm \eta^z_{ij} \sim \Dir_{L^z}(a_{\eta^z})\), for \(z\in\bc{0,1}\).
So the full conditional for \(\bm\eta^z_{ij}\) is:

\begin{align*}
p(\bm \eta^z_{ij} \mid \rest) \propto&~~ p(\bm{\eta}^z_{ij}) \times \prod_{n=1}^{N_i} p(\gamma_{inj} \mid \bm \eta^z_{ij})\\
\propto&~~ p(\bm \eta^z_{ij}) \times \prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\mathbb{I}{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
%
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{a_{\eta^z}/L^z-1} \times 
\prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\p{a_{\eta^z} / L^z + \sum_{n=1}^{N_i} \Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}} - 1}\\
%
\end{align*}

Therefore, \[
\bm{\eta}^z_{ij} \mid \y,\rest ~\sim~ \Dir_{L^z}\p{a^*_1,...,a^*_{L^z}}
\] where
\(a^*_\ell = a_{\eta^z}/L^z+\sum_{n=1}^{N_i}\Ind{\gamma_{inj}=\ell ~\cap~ Z_{j\lin}=z}\).
Consequently, the full conditional for \(\bm{\eta}^z_{ij}\) can be
sampled from directly from a Dirichlet distribution of the form above.
\vspace{2em}


% fc-beta
\textbf{Full Conditional for $\bm\beta$}

Define \(f_{inj}\) to be

\begin{align*}
f_{inj} &:= P(m_{inj} \mid p_{inj}, y_{inj}) \\
&= p_{inj}^{m_{inj}} (1-p_{inj})^{1 - m_{inj}} \\
&= \left(\frac{1}{1+e^{-x_{inj}}} \right)^{m_{inj}}\left(\frac{1}{1+e^{x_{inj}}} \right)^{1-m_{inj}},
\end{align*}

where

\begin{align*}
  x_{inj} &:= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber \\
  \beta_{0i} - \beta_{1i}c_1\sqrt{y_{inj}-c_0}, & \text{otherwise}, \nonumber \\
  \end{cases}\\
\end{align*}

where \(c_0\) and \(c_1\) are real constant, $\beta_{0i} \in \mathbb{R}$, and
$\beta_{1i} > 0$.
One way to determine a prior for the missing mechanism is to first select three
points to constrain the missing mechanism $(c_\text{low}, p_\text{low})$,
$(c_0, p_0)$, and $(c_\text{high}, p_\text{high})$, and then 
solve for $\beta_0$, $\beta_1$, and $c_1$. Figure \ref{fig:prob-miss-eg} shows
an example missing mechanism where $(c_\text{low}, p_\text{low}) = (-6,0.1)$,
$(c_0, p_0)=(-2,.99)$, and $(c_\text{high}, p_\text{high}) = (-1,0.01)$.
%\beginmyfig
%\includegraphics[scale=.5]{img/prob_miss_example.pdf}
%\caption{Example missing mechanism. The blue points serve as guides in
%determining a missing mechanism. Values for $\beta$ and $c$ can be solved for
%through a system of equations.}
%\label{fig:prob-miss-eg}
%\endmyfig

Through simple algebra, we can solve for $\beta$ and $c_0$ as follows:
\begin{align*}
  \beta_0 &:= \logit(p_0) \\
  \beta_1 &:= \frac{\beta_0 - \logit(p_\text{low})}{(y_\text{low} - c_0)^2} \\
  c_1 &:= \frac{\beta_0 - \logit(p_\text{high})}{\beta_1 ~ \sqrt{y_\text{high} - c_0} }. \\
\end{align*}
Hence, specifying three critical points in the missing mechanism can guide
the construction of its prior distribution.
\vspace{2em}


% fc-beta_0
\textbf{Full Conditional for $\beta_{0i}$}

Recall that \(\beta_{0i} \iid \N(m_{\beta_0},s^2_{\beta_0})\).

\begin{align*}
p(\beta_{0i} \mid \y, \rest) &\propto
p(\beta_{0i}) \times \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{\frac{-(\beta_{0i}-m_{\beta_0})^2}{2s^2_{\beta_0}}} \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhSpiel{\beta_{0i}}
\vspace{2em}


%fc-beta_1
\textbf{Full Conditional for $\beta_{1i}$}

Recall that $\beta_{1i}\ind \N^+(m_{\beta_1}, s^2_{\beta_1})$.
%
\begin{align*}
p(\beta_{1i} \mid \y, \rest) &\propto
p(\beta_{1i}) \times 
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{-\frac{(\beta_{1i} - m_{\beta_1})^2}{2s^2_{\beta_1}}}
\Ind{\beta_{1i} > 0}
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhLogSpiel{\beta_{1i}}{\xi}
\vspace{2em}


% fc-y
\textbf{Full Conditional for Missing $\bm \y$}

\begin{align*}
p(y_{inj} \mid m_{inj}=1, \rest) &\propto
p(m_{inj} =1\mid y_{inj}, \rest) ~
p(y_{inj} \mid \rest) \\
%
%&\propto
%\exp\bc{\frac{-(y_{inj} - \mu_{inj})^2}{2\sigma^2_{inj}}}
%f_{inj} \\
&\propto
p_{inj} 
\sum_{\ell=1}^{L^{Z_{j\lin}}} \eta^{Z_{j\lin}}_{ij\ell} \cdot \N(y_{inj} \mid \mu^*_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{j\lin},\ell}).
\end{align*}

\mhSpiel{y_{inj}}

Note that \(f_{inj}\) is a function of \(y_{inj}\) and should be
computed accordingly.
\end{document}



%%% TODO %%%
% Remove these graphs.
% Put only a couple of these subgraphs in the main report.

\newpage

%%% Many Graphs 
\section{Posterior Predictive for CB data \label{sec:cb-pp}}
\foreach \ppp in {1,...,12}{
  \beginmyfig \includegraphics[page=\ppp]{img/cb/y_hist.pdf} \endmyfig
}

\section{Posterior Predictive for Positive values of CB data \label{sec:cb-pp-positive}}
\foreach \ppp in {1,...,12}{
  \beginmyfig \includegraphics[page=\ppp]{img/cb/pp_obs.pdf} \endmyfig
}

\end{document}
