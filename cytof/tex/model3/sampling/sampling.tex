\documentclass[12pt,]{article}

%{{{1
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
%\usepackage{ifxetex,ifluatex}
%\usepackage{fixltx2e} % provides \textsubscript
%\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
%  \usepackage[T1]{fontenc}
%  \usepackage[utf8]{inputenc}
%\else % if luatex or xelatex
%  \ifxetex
%    \usepackage{mathspec}
%  \else
%    \usepackage{fontspec}
%  \fi
%  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
%\fi
% use upquote if available, for straight quotes in verbatim environments
%\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
%% use microtype if available
%\IfFileExists{microtype.sty}{%
%\usepackage{microtype}
%\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
%}{}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Sampling Scheme for CYTOF Model3},
            pdfauthor={Arthur Lui},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
%\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\bc}[1]{ \left\{#1\right\} }
\newcommand{\abs}[1]{ \left|#1\right| }
\newcommand{\mat}{ \begin{pmatrix} }
\newcommand{\tam}{ \end{pmatrix} }
\newcommand{\suml}{ \sum_{i=1}^n }
\newcommand{\prodl}{ \prod_{i=1}^n }
\newcommand{\ds}{ \displaystyle }
\newcommand{\df}[2]{ \frac{d#1}{d#2} }
\newcommand{\ddf}[2]{ \frac{d^2#1}{d{#2}^2} }
\newcommand{\pd}[2]{ \frac{\partial#1}{\partial#2} }
\newcommand{\pdd}[2]{\frac{\partial^2#1}{\partial{#2}^2} }
\newcommand{\N}{ \mathcal{N} }
\newcommand{\E}{ \text{E} }
\def\given{~\bigg|~}
\usepackage{float}
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\ind}{\overset{ind}{\sim}}
\newcommand{\I}{\mathrm{\mathbf{I}}}

\def\bet{\bm{\eta}}


\allowdisplaybreaks
\def\M{\mathcal{M}}
\def\logit{\text{logit}}
\def\Bern{\text{Bernoulli}}
\def\N{\text{Normal}}
\def\G{\text{Gamma}}
\def\IG{\text{Inverse-Gamma}}
\def\Dir{\text{Dirichlet}}
\def\Be{\text{Beta}}
\def\lin{\lambda_{in}}
\def\btheta{\bm{\theta}}
\def\y{\bm{y}}
\newcommand\m{\bm{m}}
\def\mus{\mu^*}
\def\sss{{\sigma^2}^*}
\input{includes/mhSpiel.tex}
\newcommand{\Ind}[1]{\mathbbm{1}\bc{#1}}
\def\rest{\text{rest}}
\def\bang{\boldsymbol{\cdot}}
\def\h{\bm{h}}
\def\Z{\bm{Z}}
\def\Unif{\text{Unif}}

%sim-tex-commands
\newcommand{\true}{{\mbox{\tiny TR}}}
\newcommand{\bZ}{\mbox{\boldmath $Z$}}
\newcommand{\bp}{\mbox{\boldmath $p$}}
\newcommand{\bq}{\mbox{\boldmath $q$}}
\newcommand{\bz}{\mbox{\boldmath $z$}}
\newcommand{\bw}{\mbox{\boldmath $w$}}

%%% Graphing
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
%}}}1

\title{Bayesian Feature Allocation Models for Natural Killer Cell Repertoire Studies Using Mass Cytometry Data}
\author{Arthur Lui}
\date{\today}

\begin{document}
\maketitle
\onehalfspacing



\begin{abstract}
\noindent
Bayesian feature allocation models (FAMs) embedded with
clustering are developed to analyze mass cytometry data, with primary aim to
characterize underlying cell repertoire structures.   Cell repertoires in
samples are heterogeneous. Each repertoire consists of a collection of cells
possessing different phenotypes that can be characterized by differences in
expression levels of cell surface markers.  In particular, mass cytometry data
collected to study the clinical efficacy of natural killer (NK) cells as
immunotherapeutic agents against leukemia is considered. NK cells play a
critical role in cancer immune surveillance and are the first line of defense
against viruses and transformed tumor cells.  The data includes expression
levels of 32 surface markers on each of thousands of cells from multiple
samples. NK cell repertoires may affect both NK cell function and immune
surveillance.  A key conceptual shift compared with existing approaches is to
explicitly characterize latent cell phenotypes through a FAM.  The models
simultaneously (1) characterize NK cell phenotypes based on expression /
non-expression of surface markers, (2) estimate compositions of the samples
based on the identified phenotypes, and (3) infer associations of subjects'
covariates, with the composition of the identified phenotypes in the samples.
The conventional Indian buffet process (IBP), one of the most popular feature
allocation models, is first utilized to model cell phenotypes. Non-ignorable
missing data that is present due to technical artifacts in mass cytometry are
accounted for using an informed prior missing mechanism. The repulsive FAM
(rep-FAM) is next proposed.  In contrast with the IBP, the rep-FAM produces a
parsimonious representation of phenotypes by discouraging the creation of
redundant phenotypes, and thus can improve inference on phenotypes.  Further
extensions to incorporate subject-based covariates are discussed to provide
inferences on phenotypes potentially associated with positive clinical
outcomes.  

%\noindent
%{\em Keywords:} ~  Count data, Laplace prior, Metagenomics, Microbiome, Regularizing prior, Process convolution,  Negative binomial model, Next-generation sequencing
\end{abstract}




\section{Introduction}
Clinical application of natural killer (NK) cells %as immunotherapeutic agents against leukemia
recently has emerged as a powerful treatment modality for advanced cancers refractory to conventional therapies \citep{rezvani2015application}. % and intensely investigated.
%NK cells, the third lymphocyte lineage, % preceded by T cells and B cells,
%play critical roles in the immune response to certain %virus infected cells and
%transformed tumor cells.
NK cells play a critical role in cancer immune surveillance and are the first line of defense against viruses and transformed tumor cells.
They have the intrinsic ability to infiltrate cancer tissue and their presence in the tumor is reported to be associated with better clinical outcomes \citep{suck2016natural}.  %NK cells develop in the bone marrow and are ``educated'' during their development to ensure not to attack normal self cells whiling rapidly killing tumor or virally infected cells.
% (called self-tolerance).
%During the development,
Drs. Thall and Rezvani, collaborators at UT MD Anderson Cancer Center, have conducted clinical trials to study potential clinical efficacy of umbilical cord blood (UCB) transplantation as a therapy for leukemia.  UCB has become an established source of hematopoietic stem cells for transplantation. UCB NK cell therapy has the advantages of low risk of viral transmission from donor to recipient \citep{sarvaria2017umbilical}. In the trials, leukemia patients received UCB cell transplants. % after irradiation therapy.  %Some characteristics of the patients such as their survival after the treatment are recorded.
During follow up, samples were taken at multiple time points from each patient.  Samples from %In addition, peripheral blood samples from
healthy subjects and cord blood samples also were collected for comparison to leukemia patient samples.
The samples were processed and expressions of 32 NK cell-associated cell surface protein markers, %in individual cells of the samples
measured for individual cells in the samples using mass cytometry.
Their primary research goal is to understand phenotypes and functions structured across heterogeneous NK cells. % based on the markers' expression levels.
Better understanding of the characteristics of NK cells is crucial to estimate the true potential of NK cell therapies against cancer.  %for cancer immunotherapy. %against leukemia and possibly also against solid tumors.



%NK cells play critical roles in defending against tumors. Furthermore, their diversity and function are
%known to be linked. Researchers have thus studied NK cell diversity from
%various perspectives. For instance, it is known that NK cell diversity
%is lower at birth \citep{strauss2015human} than in adults. Some
%researchers have studied the effect of introducing diverse NK cells into
%tumor patients. Yet again, some researchers have found that patients
%with higher NK diversity are associated with higher exposure risk of
%HIV-1, suggesting that existing diversity may decrease flexibility of
%the antiviral response. Many questions about NK cells remain to be
%answered. Understanding NK diversity through spectrometry has therefore
%been an important research area in the bio-sciences.   

%The main
%inferential goal of this project is to identify the NK cell phenotypes
%(or cell-types) in various samples as a set of subpopulations of the set
%of some provided surface markers. The NK cell-types are latent, and for
%\(J\) markers \(2^J\) different cell-types can be considered. This
%provides a computational challenge when the number of markers is even
%moderately large. Thirty-two markers are included in this analysis, and
%naively enumerating all possible markers is not feasible. We therefore,
%use a latent feature allocation model to learn the latent structure of
%predominant cell-types. Latent feature models have been successfully
%applied to various problems and will be reviewed in the following
%section.
%Data for this project is rendered through CyTOF analyses of
%NK-cell-targeting markers. Having some understanding of CyTOF and NK
%cells their importance is therefore necessary.


Advances in cytometry has led to more research and greater understanding  of natural killer (NK) cells and how their diversity impacts immunity against the development of tumors and other viral diseases. Flow cytometry (developed by Wallace Coulter in the 1950's) is a laser-based biophysical technology which is sometimes used for biomarker
detection. It is regularly used to diagnose health disorders like cancer. Cytometry has advanced over the years. Fluorescence-based flow cytometry, which makes use of fluorescent dyes and lasers that emit
light at specific wavelengths, is one such advancement that has been
mainstream for several decades. {\tt include reference}  In recent years, a new technique called
Cytometry at time-of-flight (CyTOF) has surfaced. It makes use of
time-of-flight mass spectrometry, where sophisticated devices are used
to accelerate, separate, and identify ions by mass. This new method
warrants the analysis of multiple parameters in shorter time.  {\tt include reference} %Through CyTOF, scientists have been able to better understand natural killer (NK) cells \citep{horowitz2013genetic}. 
A major challenge in deciphering mass cytometry data is to develop efficient inferential frameworks that can handle the complexity of the data, including high dimensionality and noise. Many existing computational methods use dimension reduction techniques and/or clustering-based approaches.  For example,  {\tt introduce some existing approaches.  Explain a bit the methods that we will use for comparison.  State limitations of existing methods.  Are they Bayesian?  Are they modeling phenotypes like our $\Z$.  Emphasize that our methods produce direct inferences on phenotypes or quantification of uncertainty associated with their inference.}  To overcome the limitations of existing methods, we propose to develop novel Bayesian feature allocation models (FAMs) embedded with clustering. 



\subsection{Literature Review: Feature Allocation Models}\label{literature-review}
One of the main inferential goals is  to learn a latent structure of predominant cell phenotypes, where cell phenotype are composed of distinct expression combinations of the markers. Specifically, phenotype $k$ is represented by a $J$-dim binary vector, $\bm z_k=(z_{1k}, \ldots, z_{Jk})$ where $z_{jk}$ take either of 1 or 0 if marker $j$ is or is not expressed in phenotype $k$.  For \(J\) markers, \(2^J\) phenotypes can be constructed.
One could create a \(J \times 2^J\) matrix, which contains all possible
phenotypes generated by the \(J\) markers, with each column representing 
each phenotype. It is computationally infeasible when \(J\) is large. Taking a Bayesian approach, we consider a prior probability model for binary matrices with infinite number of columns and learn the predominant ones in posterior that  generate observed data . %For computational efficiency, we require a flexible prior which will also learn the number of cell-types \((K)\). 
These models are also called latent feature allocation models.  One of popular models for binary feature matrices is the Indian buffet process (IBP) proposed by \citet{griffiths2011indian}. 
%The IBP have been used in a variety of applications where modelling latent binary features is of interest. 

\citet{griffiths2011indian} construct the IBP by considering the finite feature allocation
model and taking the limit with respect to the number of features; for a given $K$,
\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1),~ k=1, \ldots, K \\
z_{jk} \mid \pi_k &\sim \text{Bernoulli}(v_k),~ k=1, \ldots, K~\mbox{ and } j=1, \ldots, J. \\
\end{split}
\label{eq:ibp}
\end{align}
The marginal limiting distribution of $\Z$ defines an IBP as $K \rightarrow \infty$ and dropping all columns with all 0s, that is,  \(Z \sim \text{IBP}(\alpha)\). Under the IBP, each row is expected to have  \(\alpha\) active features.  The number of columns with $\sum_{j=1}^J z_{jk} >0$ is random and it can be shown that the number of such columns follows \(\text{Poisson}(\alpha \sum_{j=1}^J j^{-1})\).   A prior distribution can be placed on \(\alpha\) to reflect uncertainty. A gamma prior is popular for $\alpha$ due to conjugacy.  \citet{teh2007stick} represented the IBP using the stick-breaking construction similar to the stick-breaking representation of the Dirichlet process (DP).  \citet{williamson2010dependent} developed a dependent IBP (dIBP) to induce correlations between objects
(rows) and to model multiple $\Z$s that can be dependent.



{\tt need to include Tamara's papers on feature allocation model, my papers on tumor heterogeneity. 
\begin{itemize}
\item ``Feature allocations, probability functions, and paintboxes.'', ``Combinatorial clustering and the beta negative binomial process.'' etc by Tamara \url{http://www.tamarabroderick.com/papers.html}

\item ``Bayesian Inference for Intra-Tumor Heterogeneity in Mutations and Copy Number Variation'', ``A Bayesian Feature Allocation Model for Tumor Heterogeneity.'' by me

\item ``Bay- clone: Bayesian nonparametric inference of tumor subclones using ngs data'' by Subhajit et al.
 
\item YX's works: ``Bayesian Inference for Latent Biologic Structure with Determinantal Point Processes'', "MAD Bayes for Tumor Heterogeneity Feature Allocation with Exponential Family Sampling"  \url{http://www.ams.jhu.edu/~yxu70/pub.html}.

\item There could be more.  Please search for those.
\end{itemize}

}

\section{Project 1: }
\subsection{Introduction}
{\tt short intro for project 1}

\subsection{Model}\label{prob-model}
\subsubsection{Sampling Model} 
Samples are taken from \(I\) subjects, \(i = 1,2,...,I\). Sample \(i\)
consists of \(N_i\) cells, \(n=1, \ldots, N_i\) and for each cell,
expression levels of \(J\) markers are measured. Let
\(\tilde{y}_{inj} \in \mathbb{R}^+\) represent the raw measurement of an
expression level of marker \(j\) of cell \(n\) in sample \(i\). Let
\(c_{ij}\) denote the ``cutoff'' for
 marker \(j\) in sample \(i\). A marker of a cell is likely to be expressed if its observed expression level is greater than the cutoff. A value of $\tilde{y}_{inj}$ below the cutoff may imply that marker $j$ is not expressed in cell $n$ of sample $i$.    We consider the logarithm transformation
after scaling \(\tilde{y}_{inj}\) by \(c_{ij}\), \[
y_{inj}=\log\p{\frac{\tilde{y}_{inj}}{c_{ij}}} \in \mathbb{R}.
\] For some \((i, n, j)\), \(\tilde{y}_{inj}\) is missing and we
introduce a binary indicator, \[
m_{inj} = \begin{cases}
  0, & \text{if $\tilde{y}_{inj}$ is observed,} \\
  1, & \text{if $\tilde{y}_{inj}$ is missing.}
\end{cases}
\] That is, \(m_{inj}=1\) indicates that the expression level of marker
\(j\) of cell \(n\) in sample \(i\) is missing.

%\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
%\tightlist
%\item
%  The data have infinite support.
%\item
%  \(y_{inj} = 0\) has a special meaning, which is that the data take on
%  the same value as the cutoff. Consequently, \(y_{inj} > 0\) means that
%  the data take on values greater than the cutoff, etc.
%\item
%  \(y_{inj}\) for which \(\tilde y_{inj} = 0\) are regarded as missing,
%  and is to be imputed.
%\end{enumerate}

%\newpage


We assume that a sample has heterogeneous cells having $K$ different phenotypes.  The phenotypes are not directly observable and we introduce latent phenotype indicators \(\lambda_{in} \in \{1, \ldots, K\}\), for cell $n$ in sample $i$, \(i=1, \ldots, I\) and \(n=1, \ldots, N_i\).
%\(\lambda_{in} \in \{1, \ldots, K\}\) denotes the cell phenotype of cell \(n\) in sample \(i\) defined by 
The cell phenotypes are defined by columns of \(J \times K\) binary matrix \(\Z\). Element \(z_{j, k} \in \{0, 1\}\) indicates if marker \(j\) is expressed in cell phenotype \(k\). Event \(z_{jk}=0\) represents that marker \(j\) is not expressed for phenotype $k$, and \(z_{jk}=1\) for
expression. We let $\Z$ and $\lambda_{in}$ random. Details will be discussed later.  Given \(z_{j, \lambda_{in}} \in \{0, 1\}\), we assume a mixture of
normals for \(y_{inj}\),
\begin{align}
y_{inj} \mid \eta_{ij}, \mu^\star, \sigma^{2 \star}_{i} \ind
\begin{cases}
\sum_{\ell=1}^{L^0} \eta^0_{ij\ell}~ \N(\mu^\star_{0\ell}, \sigma^{2 \star}_{0i\ell}), &\mbox{if $z_{j,\lambda_{in}}=0$},\\
\sum_{\ell=1}^{L^1} \eta^1_{ij\ell}~ \N(\mu^\star_{1\ell}, \sigma^{2 \star}_{1i\ell}), &\mbox{if $z_{j,\lambda_{in}}=1$},\\
\end{cases} \label{eq:y-mix}
\end{align}
where the number of mixture components \(L^0\) and \(L^1\) are fixed.  The vectors $\bet^0_{ij}$ and $\bet^1_{ij}$ are mixture weights with \(\sum_{\ell} \eta^0_{ij\ell}\sum_{\ell}= \eta^1_{ij\ell}=1\) where \(0 < \eta^1_{ij\ell} < 1\) and \(0 < \eta^0_{ij\ell} < 1\).  In \eqref{eq:y-mix}, $\mu^\star$ are common for all samples and markers but $\sigma^{2, \star}$ are indexed by sample $i$ to account for sample specific variability.  The mixture model can thus flexibly capture various features in data.  For easy computation, we introduce mixture component indicators $\gamma_{inj}$ for $y_{inj}$.  Given \(\lambda_{in}=k\) we define
\(\gamma_{inj}\); for \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and
\(j=1, \ldots, J\),
\begin{eqnarray}
p(\gamma_{inj} = \ell)=\eta^{z_{jk}}_{ij\ell}, \mbox{ where }~ \ell \in \{1,\ldots, L^{z_{jk}}\}. \label{eq:gam}
\end{eqnarray}
Given \(\lambda_{in}=k\) and \(\gamma_{inj}=\ell\), we assume a normal
distribution for \(y_{inj}\); for \(i=1, \ldots, I\),
\(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  y_{inj} \mid \mu_{inj}, \sigma^2_{inj}  &\ind \N(\mu_{inj}, \sigma^2_{inj}), \label{eq:y-gam}
\end{align}
where \(\mu_{inj} = \mu^\star_{z_{j,k},\ell}\) and
\(\sigma^2_{inj} = {\sigma^{2}}^\star_{iz_{j,k}\ell}\). After marginalizing $\gamma_{inj}$ out, the model in \eqref{eq:y-gam} and \eqref{eq:gam} is equivalent to the model in \eqref{eq:y-mix}.  

We next build a model for the missingness mechanism.  {\tt explain our approach for missing in few sentences.}  Given
\(y_{inj}\), we consider a selection function for \(m_{inj}\); for
\(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  m_{inj} \mid p_{inj} &\ind \Bern(p_{inj}) \nonumber \\
  \logit(p_{inj}) &= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber, \\
  \beta_{0i} - \beta_{1i}c_1\p{y_{inj}-c_0}^{1/2}, & \text{otherwise}, \nonumber \\
  \end{cases} \label{eq:missing}
\end{align}
where \(c_0\) and \(c_1\) are fixed.  {\tt say something like it is ``untestable'' but take this approach by incorporating our input from biologists}


\subsubsection{Priors}\label{priors}
\paragraph*{Latent cell phenotypes}  Recall that we characterize cell phenotypes with a $J\times K$ binary matrix \(\Z =\{z_{jk}\}\).  Following \citet{williamson2010dependent}, we assume
\begin{eqnarray*}
v_k \mid \alpha &\iid& \Be(\alpha/K, 1),~ k=1, \ldots, K, \\
\h_k &\iid& \N_J(\bm{0}, \Gamma), \\ 
z_{jk} \mid h_{jk}, v_k &=& \mathbb{I}\left\{ \Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k \right\},
\end{eqnarray*}
where $\Phi(h \mid m, s)$ is the cumulative distribution function of the normal distribution with mean $m$ and variance $s$ and $\mathbb{I}(\cdot)$ is an indicator function having 1 if $\Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k$ or 0 otherwise.  As $K \rightarrow \infty$, the limiting distribution of $Z$ is the IBP {\tt cite the paper}.  Interactions between $J$ markers in phenotypes can be modeled through $\bm G$.  Due to the multivariate probit construction for $\Z$, $\Gamma$ is not identifiable and it is common to restrict $\Gamma$ to be a correlation matrix.  {\tt discuss a bit models for correlation matrix.  I will send you some references on models for correlation matrix.}  We let $\alpha \sim \G(a_\alpha, b_\alpha)$ with mean $a_\alpha/b_\alpha$.  

The $K$ cell phenotypes are common in all samples but the relative weights vary across samples. Let $W_{ik}$ denote an abundance level of phenotype $k$ in sample $i$.  We assume independent Dirichlet priors for $\bm W_i=(W_{i1}, \ldots, W_{iK})$ given $K$, $\bm W_{i} \mid K \iid \Dir_K(d/K)$. For latent cell phenotype indicators, we let $p(\lin=k \mid \bm W_i) = W_{ik}$.

\paragraph*{Parameters in the Mixture for $y$} In \eqref{eq:y-mix}, normal mixture models are assumed for $y_{inj}$. The mean expression level of maker $j$ in cell $n$ is determined by its phenotype $\lambda_{in}$.  In particular, if the marker is not expressed in the cell type, $z_{j \lambda_{in}}=0$, its mean expression level is below the cutoff, that is, a negative value.  If the marker is expressed $z_{j \lambda_{in}}=0$, the expression of marker $j$ take a positive value.   Recall that $\mus_{0\ell}$, $\ell=1, \ldots, L^0$ are mixture locations for $z_{j \lambda_{in}}=0$ and $\mus_{1\ell}$, $\ell=1, \ldots, L^1$ for $z_{j \lambda_{in}}=1$.  We assume 
\begin{align*}
\mus_{0\ell} \mid \psi_0, \tau^2_0 &\iid \N_-(\psi_0, \tau^2_0), ~~~ \ell \in \bc{1,...,L^0}, \\
\mus_{1\ell} \mid \psi_1, \tau^2_1 &\iid \N_+(\psi_1, \tau^2_1), ~~~ \ell \in \bc{1,...,L^1}, 
\end{align*}
where \(\N_-(m,s^2)\) and \(\N_+(m,s^2)\) denote the normal distribution with mean \(m\) and variance \(s^2\), truncated to take on only negative values and positive values, respectively.  The variances in the mixture components differ by the value of $z_{j \lambda_{in}}$ and also vary across samples. We let; for \(i=1, \ldots, I\),
\begin{align*}
\sigma^2_{0i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^0}, \\
\sigma^2_{1i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^1}.  
\end{align*}
We also assume $s_i \iid \G(a_s, b_s)$, $i \in \bc{1,...,I}$, with mean \(a_s/b_s\). Lastly, we consider a model for mixture weights. To flexibly model the distribution of $y$, we assume for a marker in a sample have two sets of it own weights, one for each value of $z$, $\bm\eta^0_{ij}$ and $\bm\eta^1_{ij}$; for each $(i, j)$ For \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align*}
\bm\eta^0_{ij} &\iid \Dir_{L^0}(a_{\eta^0}/L^0), \\
\bm\eta^1_{ij} &\iid \Dir_{L^1}(a_{\eta^1}/L^1). 
\end{align*}


\paragraph*{Parameters for Missingness Mechanism}  {\tt explain what you do for missingness mechanism}.  We assume that $\beta_{0i} \iid \N(m_{\beta_0}, s^2_{\beta_0})$ and $\beta_{1i} \iid \N_+(m_{\beta_1}, s^2_{\beta_1})$, $i=1, \ldots, I$.  We use data to specify the values of the fixed hyperparameters, $m_{\beta_0}$ and $m_{\beta_1}$.  We let $s^2_{\beta_0}$ and $s^2_{\beta_1}$ small to induce a informative prior for $\beta_{0i}$ and $\beta_{1i}$.



\subsubsection{Posterior Computation}\label{sampling-via-mcmc}
Let \(\btheta=\{\bm \mu^\star_0, \bm \mu^\star_1, \bm \sigma^2_{0i}, \bm \sigma^2_{1i}, \bm \eta^0, \bm \eta^1, \bm \lambda, \bm \gamma, \bm v, \bm h, \bm \beta_0, \bm \beta_1\}\) represent all random parameters.  Let \(\y\) and \(\m\) denote all \(y_{inj}\) and \(m_{inj}$ for all \((i,n,j)\), respectively. The joint posterior distribution is 
\begin{align*}
p(\btheta \mid \y, \m) &\propto 
p(\btheta) \prod_{i,n,j} p(m_{inj} \mid y_{inj}, \btheta) p(y_{inj} \mid \btheta) \nonumber\\
&=  
p(\btheta)
\prod_{i,n,j} \left[
  p_{inj}^{m_{inj}} (1-p_{inj})^{1-m_{inj}} \times 
   \frac{1}{\sqrt{2\pi\sigma^2_{inj}}} \exp\bc{-\frac{(y_{inj}-\mu_{inj})^2}{2\sigma^2_{inj}}}
\right].
\end{align*}
The marginal density for \(y_{i,n,j}\) after integrating out
\(\lambda\) and \(\gamma\) is
\begin{align}
p(y_{inj} \mid \btheta) = \sum_{k=1}^K W_{ik} \sum_{\ell=1}^{L^{Z_{jk}}}
\eta^{Z_{jk}}_{ijl} \cdot \N(y_{inj} \mid \mu^\star_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{jk},\ell}).
\end{align}
Posterior simulation can be done via Gibbs sampling by repeatedly updating each parameter one at a time until convergence. Parameter updates are made by
sampling from it full conditional distribution. Where this cannot be done conveniently, a metropolis step can be used.  {\tt do you use the marginal density of $y$ in posterior simulation?  If so, explain why we use it.}  Details of the posterior simulation is described in Appendix A. 


Summarizing the joint posterior distribution $p(\btheta \mid \y, \m)$ is challenging, especially for $\Z$, $\bm W$ and $\lambda$, possibly due to a label switching problem. {\tt Include David's approach here and explain his approach.  Also explain why we include $w$ in the adjacency matrix, which is different from David's approach. }  Using the posterior Monte Carlo sample we find posterior point estimates for each sample $\hat{\Z}_i$, $\hat{\bm W}_i$ and $\hat{\lambda}_{in}$ as follows; Suppose we obtain \(B\) posterior samples simulated from the posterior distribution of \(\theta\). For each posterior sample of \(\Z\) and \(\bm W_i\), we compute a $J \times J$ adjacency matrix, \(\bm A_i^{(b)} =\{A^{(b)}_{i,j,j'}\}\), where 
\[
A^{(b)}_{i,j,j'} = \sum_{k=1}^K W^{(b)}_{i,k} 
\mathbb{I}\left( z^{(b)}_{j,k} = 1\right)
\mathbb{I}\left(z^{(b)}_{j',k} = 1\right), b \in \bc{1,...,B}.
\]
We then compute the mean adjacency matrix \(\bar A_i = \sum_{b=1}^B A_i^{(b)} / B\).  We report a posterior point estimate of $\Z_i$ by choosing
\[
\hat{\bm Z}_i = \text{argmin}_{\bm Z} \sum_{j,j'} (A_{i,j,j'}^{(b)} - \bar A_{i,j,j'})^2).
\]
Conditioning on $\hat{\Z}_i$, we report posterior point estimates $\hat{\bm W}_i$ and $\hat{\lambda}_{in}$.


%%% sim-study-proj1 %%%
\subsection{Simulation} %%% Change name to Simulation Study?
In order to understand the strengths and limitations of the proposed model, a
simulation study was conducted. The study generated data that resemble the
cord-blood (CB) CyTOF data. This section details the data-simulation strategy
and the results of the analysis on the simulated data using the proposed model.

\subsubsection{Data Simulation}
The steps for simulating data is as follows.
First specify dimensions of the data through $I$ (number of samples), $J$
(number of markers), and $N_i$ (number of cells in sample $i$). Specify the
dimensions of the parameters through $K$ (true number of latent cell-types),
$L^0$ (number of mixture components in density for cells not expressing a
marker), and $L^1$ (number of mixture components in density for cells
expressing a marker). Then fix the latent cell-type matrix $\bZ^\true$
according to $J$ and $K$. Set values for $\sigma^{2, \true}_{0i\ell}$,
$\sigma^{2,\true}_{1i\ell}$, $\mu^{\star, \true}_{0\ell}$ and $\mu^{\star,
\true}_{1\ell}$.  This can be done using empirical values from CB data.
Simulate $\bw^\true_i$ from a Dirichlet distribution with parameters $a_1,
\ldots, a_K$. This ensures that $\sum_{k=1}^K \bw^\true_i = 1$.
Similarly, $\eta^\true_{0ij}$ and $\eta^\true_{1ij}$ can be simulated
from Dirichlet distributions. Note that $\eta^\true_{0ij}$ is an
$L^0$-dimensional vector, while $\eta^\true_{0ij}$ is an $L^1$-dimensional
vector. Using these parameters, simulate $y_{inj}$ according to the sampling
density. Finally, probabilistically set some of the $y_{inj}$ to be missing.
This can be done by first predetermining a certain percentage ($p\%$) of the
data to be missing. This percentage should should be less than $\sum_k
w^\true_{ik}z^\true_{jk}$ so that truly expressed cells are unlikely to be
missing. Sample the ($N_i\times p\%$) of $y_{inj}$ without replacement.  The
probability of missing for $y$ is proportional to some predetermined missing
mechanism (possibly different from the one used in this model). Care should be
taken to ensure $y$ with positive values have almost zero probability of being
missing.

\subsubsection{Results}

\textbf{True $\bZ$ and $W$}
\beginmyfig
\includegraphics[height=0.3\textwidth,width=1.0\textwidth]{img/Z_true_byW_p1.pdf}
\endmyfig

\textbf{Posterior Estimate for $\bZ$ and $W$}

\textbf{Posterior Predictive for $\y$}




%%% cb-proj1 %%%
\subsection{Cord Blood Data}

\subsection{Conclusions}

\section{Project 2: Repulsive Feature Allocation Model}




\bibliography{litreview.bib}


\appendix
\section{Posterior Computation for Project 1}
{\tt Please write a short intro about the posterior sampling and reorganize the full conditionals }


To sample from a distribution which is otherwise difficult to sample
from, the Metropolis-Hastings algorithm can be used. This is
particularly useful when sampling from a full conditional distribution
of one of many parameters in an MCMC based sampling scheme (such as a
Gibbs sampler). Say \(B\) samples from a distribution with density
\(p(\theta)\) is desired, one can do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide an initial value for the sampler, e.g. \(\theta^{(0)}\).
\item
  Repeat the following steps for \(i = 1,...,B\).
\item
  Sample a new value \(\tilde\theta\) for \(\theta^{(i)}\) from a
  proposal distribution \(Q(\cdot \mid \theta^{(i-1)})\).

  \begin{itemize}
  \tightlist
  \item
    Let \(q(\tilde\theta \mid \theta)\) be the density of the proposal
    distribution.
  \end{itemize}
\item
  Compute the ``acceptance ratio'' to be \[
     \rho=
     \min\bc{1, \frac{p(\tilde\theta)}{p(\theta^{(i-1)})} \Big/ 
            \frac{q(\tilde\theta\mid\theta^{(i-1)})}
                 {q(\theta^{(i-1)}\mid\tilde\theta)}
        }
     \]
\item
  Set \[
     \theta^{(i)} := 
     \begin{cases}
     \tilde\theta &\text{with probability } \rho \\
     \theta^{(i-1)} &\text{with probability } 1-\rho.
     \end{cases}
     \]
\end{enumerate}

Note that in the case of a symmetric proposal distribution, the
acceptance ratio simplifies further to be
\(\frac{p(\tilde\theta)}{p(\theta^{(i-1)})}\).

The proposal distribution should be chosen to have the same support as
the parameter. Transforming parameters to have infinite support can,
therefore, be convenient as a Normal proposal distribution can be used.
Moreover, as previously mentioned, the use of symmetric proposal
distributions (such as the Normal distribution) can simplify the
computation of the acceptance ratio.

Some common parameter transformations are therefore presented here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For parameters bounded between \((0,1)\), a logit-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has support in the unit interval, then
  \(Y=\logit(X)=\log\p{\frac{p}{1-p}}\) will have density
  \(f_Y(y) = f_X\p{\frac{1}{1+\exp(-y)}}\frac{e^{-y}}{(1+e^{-y})^{2}}\).
\item
  For parameters with support \((0,\infty)\), a log-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has positive support, then \(Y = \log(X)\) has pdf
  \(f_Y(y) = f_X(e^y) e^y\).
\end{enumerate}


{\tt instead of subsections, we may do ``itemize'' for full conditionals.  I commented out the full conditionals temporarily since it gives some errors in compiling.  Please comment in as you revise. }



%\end{document}

\subsection{\texorpdfstring{Full Conditional for
\(\bm \beta\)}{Full Conditional for \textbackslash{}bm \textbackslash{}beta}}\label{full-conditional-for-bm-beta}

Define \(f_{inj}\) to be

\begin{align*}
f_{inj} &:= P(m_{inj} \mid p_{inj}, y_{inj}) \\
&= p_{inj}^{m_{inj}} (1-p_{inj})^{1 - m_{inj}} \\
&= \left(\frac{1}{1+e^{-x_{inj}}} \right)^{m_{inj}}\left(\frac{1}{1+e^{x_{inj}}} \right)^{1-m_{inj}},
\end{align*}

where

\begin{align*}
  x_{inj} &:= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber \\
  \beta_{0i} - \beta_{1i}c_1\sqrt{y_{inj}-c_0}, & \text{otherwise}, \nonumber \\
  \end{cases}\\
\end{align*}

where \(c_0\) and \(c_1\) are fixed.

\textbf{Write a little about the following...} \\
Predetermine $c_\text{low}$, $c_\text{high}$, $p_\text{low}$, $p_\text{high}$, and
$p_0$, which will determine the shape of the missing mechanism. Then

\begin{align*}
  \beta_0 &:= \logit(p_0) \\
  \beta_1 &:= \frac{\beta_0 - \logit(p_\text{low})}{(y_\text{low} - c_0)^2} \\
  c_1 &:= \frac{\beta_0 - \logit(p_\text{high})}{\beta_1 ~ \sqrt{y_\text{high} - c_0} }. \\
\end{align*}


\subsubsection{\texorpdfstring{Full Conditional for
\(\beta_{0i}\)}{Full Conditional for \textbackslash{}beta\_\{0i\}}}\label{full-conditional-for-beta_0i}

Recall that \(\beta_{0i} \iid \N(m_{\beta_0},s^2_{\beta_0})\).

\begin{align*}
p(\beta_{0i} \mid \y, \rest) &\propto
p(\beta_{0i}) \times \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{\frac{-(\beta_{0i}-m_{\beta_0})^2}{2s^2_{\beta_0}}} \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhSpiel{\beta_{0i}}

\subsubsection{\texorpdfstring{Full Conditional for
\(\beta_{1i}\)}{Full Conditional for \textbackslash{}beta\_\{1i\}}}\label{full-conditional-for-beta_1i}

Recall that $\beta_{1i}\ind \N^+(m_{\beta_1}, s^2_{\beta_1})$.
%
\begin{align*}
p(\beta_{1i} \mid \y, \rest) &\propto
p(\beta_{1i}) \times 
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{-\frac{(\beta_{1i} - m_{\beta_1})^2}{2s^2_{\beta_1}}}
\Ind{\beta_{1i} > 0}
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhLogSpiel{\beta_{1i}}{\xi}

\subsection{\texorpdfstring{Full Conditional for Missing
\(\bm \y\)}{Full Conditional for Missing \textbackslash{}bm \textbackslash{}y}}\label{full-conditional-for-missing-bm-y}

\begin{align*}
p(y_{inj} \mid m_{inj}=1, \rest) &\propto
p(m_{inj} =1\mid y_{inj}, \rest) ~
p(y_{inj} \mid \rest) \\
%
%&\propto
%\exp\bc{\frac{-(y_{inj} - \mu_{inj})^2}{2\sigma^2_{inj}}}
%f_{inj} \\
&\propto
p_{inj} 
\sum_{\ell=1}^{L^{Z_{j\lin}}} \eta^{Z_{j\lin}}_{ij\ell} \cdot \N(y_{inj} \mid \mu^*_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{j\lin},\ell}).
\end{align*}

\mhSpiel{y_{inj}}

Note that \(f_{inj}\) is a function of \(y_{inj}\) and should be
computed accordingly.

\subsection{\texorpdfstring{Full Conditional for
\(\bm\mu^\star\)}{Full Conditional for \textbackslash{}bm\textbackslash{}mu\^{}\star}}\label{full-conditional-for-bmmu}

For \(\mus_{0\ell}\)g, let
\(S_{0i\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 0 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{0i\ell}|\) the cardinality of \(S_{0i\ell}\).

\newcommand\musZeroPostvarDenom{
  \frac{1}{\tau^2_0} + \sum_{i=1}^I\frac{|S_{0i\ell}|}{{\sigma^2}^\star_{0i\ell}}
}
\newcommand\musZeroPostMeanNum{
  \frac{\psi_0}{\tau^2_0} + 
  \sum_{i=1}^I \sum_{S_{0i\ell}}  
  \frac{y_{inj}}{{\sigma^2}^\star_{0i\ell}}
}

\begin{align*}
p(\mus_{0\ell} \mid \y, \rest) &\propto 
p(\mus_{0\ell} \mid \psi_0, \tau^2_0) \times p(\y \mid \mus_{0\ell},\rest) \\
%
&\propto
\Ind{\mus_{0\ell}<0} \exp\bc{\frac{-(\mus_{0\ell} - \psi_0)^2}{2\tau^2_{0}}}
\prod_{i=1}^I\prod_{(i,n,j)\in S_{0i\ell}} \exp\bc{\frac{-(y_{inj} - \mus_{0\ell})^2}{2{\sigma^2}^\star_{i0\ell}}} \\
%
&\propto
\exp\bc{
  -\frac{(\mus_{0\ell})^2}{2}\p{\musZeroPostvarDenom} + 
  \mus_{0\ell}\p{\musZeroPostMeanNum}
} \\ 
& ~~~ \times \Ind{\mus_{0i\ell}<0} \\
\end{align*}

\[
\renewcommand\musZeroPostvarDenom{
  1 + \tau^2_0\sum_{i=1}^I(|S_{0i\ell}|/{\sigma^2}^\star_{0i\ell})
}
\renewcommand\musZeroPostMeanNum{
  \psi_0 + \tau^2_0 \sum_{i=1}^I\sum_{S_{0i\ell}} (y_{inj} / {\sigma^2}^\star_{0i\ell})
}
\therefore \mus_{0l} \mid \y, \rest \ind \N_-\p{
  \frac{\musZeroPostMeanNum}{\musZeroPostvarDenom},
  \frac{\tau^2_0}{\musZeroPostvarDenom}
}
\]

Similarly for \(\mus_{1\ell}\)g, let
\(S_{1\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 1 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{1i\ell}|\) the cardinality of \(S_{1i\ell}\).

\[
\newcommand\musOnePostvarDenom{
  1 + \tau^2_1 \sum_{i=1}^I (|S_{1i\ell}|/{\sigma^2}^\star_{1i\ell})
}
\newcommand\musOnePostMeanNum{
  \psi_1 + \tau^2_1 \sum_{i=1}^I \sum_{S_{1i\ell}} (y_{inj} / {\sigma^2}^\star_{1i\ell})
}
\therefore \mus_{1l} \mid \y, \rest \ind \N_+\p{
  \frac{\musOnePostMeanNum}{\musOnePostvarDenom},
  \frac{\tau^2_1}{\musOnePostvarDenom}
}
\]

\subsection{\texorpdfstring{Full Conditional for
\(\bm{{\sigma^2}}^*\)}{Full Conditional for \textbackslash{}bm\{\{\textbackslash{}sigma\^{}2\}\}\^{}*}}\label{full-conditional-for-bmsigma2}

Let
\(S_{0i\ell} = \bc{(i, n,j): Z_{j,\lin} = 0 ~\cap~ \gamma_{inj}=\ell}\),
\(i=1, \ldots, I\).

\begin{align*}
p(\sss_{0i\ell} \mid \y, \rest) &\propto p(\sss_{0i\ell} \mid s_i) \times p(\y \mid \sss_{0i\ell}, \rest) \\
&\propto (\sss_{0i\ell})^{-a_\sigma-1} \exp\bc{-\frac{s_i}{\sss_{0i\ell}}} 
\prod_{(i,n,j)\in S_{0i\ell}} \bc{
  \frac{1}{\sqrt{2\sss_{0i\ell}}}
  \exp\bc{\frac{-(y_{inj}-\mus_{0\ell})^2}{2\sss_{0i\ell}}}
} \\
&\propto (\sss_{0i\ell})^{-(a_\sigma + \frac{\abs{S_{0i\ell}}}{2})-1}
\exp\bc{\p{\frac{1}{\sss_{0i\ell}}}\p{s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}}.
\end{align*}

\[
\therefore \sss_{0i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{0i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}.
\]

Similarly, let
\(S_{1i\ell} = \bc{(i, n,j): Z_{j,\lin} = 1 ~\cap~ \gamma_{inj}=\ell}\).
Then, the full conditional for \(\sss_{1i\ell}\) is \[
\therefore \sss_{1i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{1i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{1i\ell}}
\frac{(y_{inj}-\mus_{1\ell})^2}{2}
}.
\]

\subsection{\texorpdfstring{Full Conditional for
\(s_i\)}{Full Conditional for s\_i}}\label{full-conditional-for-s_i}

\begin{align*}
p(s_i \mid \y, \rest) &\propto p(s_i) \times \prod_{z=0}^1 \prod_{\ell=1}^{L^z} p(\sss_{zi\ell} \mid s_i)\\
&\propto s_i^{a_s-1} \exp\bc{-b_s s_i} \times \prod_{z=0}^1  \prod_{\ell=1}^{L^z} s_i^{a_\sigma} \exp\bc{-s_i / \sss_{zi\ell}} \\
&\propto s_i^{a_s + (L^0 + L^1)a_\sigma - 1} \exp\bc{-s_i \p{b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} 1 / \sss_{zi\ell}}}.
\end{align*}

\[
\therefore s_i \mid \y, \rest \sim 
\G\p{a_s + (L^0 + L^1)a_\sigma, ~~ b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} \frac{1}{\sss_{zi\ell}} }.
\]

\subsection{\texorpdfstring{Full Conditional for
\(\bm\gamma\)}{Full Conditional for \textbackslash{}bm\textbackslash{}gamma}}\label{full-conditional-for-bmgamma}

The prior for \(\gamma_{inj}\) is
\(p(\gamma_{inj} = \ell \mid Z_{j\lin}=z, \eta^z_{ij\ell}) = \eta^z_{ij\ell}\),
where \(\ell \in \bc{1,...,L^z}\).

\begin{align*}
p(\gamma_{inj}=\ell \mid \y, Z_{j\lin}=z, \rest) &\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \gamma_{inj}=\ell, \rest) \\
&\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}, \rest) \\
%
&\propto \eta^z_{ij\ell} \times \N(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}) \\
&\propto \eta^z_{ij\ell} \times (\sss_{zi\ell})^{-1/2}
\exp\bc{-\frac{(y_{inj} - \mus_{z\ell})^2}{2\sss_{zi\ell}}} \\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(\ell = 1,...,L^z\). Moreover, since \(\ell\) is discrete, a Gibbs
update can be done on \(\gamma_{inj}\).

\subsection{\texorpdfstring{Full Conditional for
\(\bm\eta\)}{Full Conditional for \textbackslash{}bm\textbackslash{}eta}}\label{full-conditional-for-bmeta}

The prior for \(\bm\eta^z_{ij}\) is
\(\bm \eta^z_{ij} \sim \Dir_{L^z}(a_{\eta^z})\), for \(z\in\bc{0,1}\).
So the full conditional for \(\bm\eta^z_{ij}\) is:

\begin{align*}
p(\bm \eta^z_{ij} \mid \rest) \propto&~~ p(\bm{\eta}^z_{ij}) \times \prod_{n=1}^{N_i} p(\gamma_{inj} \mid \bm \eta^z_{ij})\\
\propto&~~ p(\bm \eta^z_{ij}) \times \prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
%
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{a_{\eta^z}/L^z-1} \times 
\prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\p{a_{\eta^z} / L^z + \sum_{n=1}^{N_i} \Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}} - 1}\\
%
\end{align*}

Therefore, \[
\bm{\eta}^z_{ij} \mid \y,\rest ~\sim~ \Dir_{L^z}\p{a^*_1,...,a^*_{L^z}}
\] where
\(a^*_\ell = a_{\eta^z}/L^z+\sum_{n=1}^{N_i}\Ind{\gamma_{inj}=\ell ~\cap~ Z_{j\lin}=z}\).
Consequently, the full conditional for \(\bm{\eta}^z_{ij}\) can be
sampled from directly from a Dirichlet distribution of the form above.

\subsection{\texorpdfstring{Full Conditional for
\(\bm v\)}{Full Conditional for \textbackslash{}bm v}}\label{full-conditional-for-bm-v}

The prior distribution for \(v_k\) are
\(v_k \mid \alpha \ind \Be(\alpha/K, 1)\), for \(k = 1,...,K\). So,
\(p(v_k \mid \alpha) = \alpha v_k^{\alpha/K-1}\).

Let \(S = \bc{(i,n)\colon \lin = k}\).

\begin{align*}
p(v_k \mid \y, \rest) &\propto p(v_k) \prod_{j=1}^J\prod_{(i,n)\in S} p(\y \mid v_k, \rest) \\
&\propto (v_k)^{\alpha/K-1} \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhLogitSpiel{v_k}{\xi}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(v_k\), and should be computed accordingly. Moreover, we
will only recompute the likelihood (in the metropolis acceptance ratio)
when \(Z_{jk}\) becomes different.

\subsection{\texorpdfstring{Full Conditional for
\(\alpha\)}{Full Conditional for \textbackslash{}alpha}}\label{full-conditional-for-alpha}

\begin{align*}
p(\alpha \mid \y, \rest) &\propto p(\alpha) \times \prod_{k=1}^K p(v_k \mid \alpha) \\
&\propto \alpha^{a_\alpha - 1} \exp\bc{-b_\alpha \alpha} \times \prod_{k=1}^K 
\alpha~v_k^{\alpha/K} \\
&\propto \alpha^{a_\alpha + K -1} \exp\bc{-\alpha\p{b_\alpha - 
\frac{\sum_{k=1}^K \log v_k}{K}}}
\end{align*}

\[
\therefore \alpha \mid \y, \rest \sim 
\G\p{a_\alpha + K,~ b_\alpha - \frac{\sum_{k=1}^K \log v_k}{K}}
\]

\subsection{\texorpdfstring{Full Conditional for
\(\bm H\)}{Full Conditional for \textbackslash{}bm H}}\label{full-conditional-for-bm-h}

The prior for \(\h_k\) is \(\h_k \sim \N_J(0, \Gamma)\). We can
analytically compute the conditional distribution
\(h_{j,k} \mid \h_{-j,k}\), which is

\[
h_{jk}  \mid \h_{-j,k} \sim \N(m_j, S^2_j),
\]

where

\[
\begin{cases}
m_j &= \bm G_{j,-j} \bm G_{-j,-j}^{-1}(\h_{-j,k})\\
S_j^2 &= \bm G_{j,j} - \bm G_{j,-j}\bm G_{-j,-j}^{-1}\bm G_{-j,j}\\
\end{cases}
\]

and the notation \(\h_{-j,k}\) refers to the vector \(h_k\) excluding
the \(j^{th}\) element. Likewise, \(\bm G_{-j,k}\) refers to the
\(k^{th}\) column of the matrix \(\bm G\) excluding the \(j^{th}\) row.

Note that if \(\bm G = \I_J\), then \(m_j=0\) and \(S_j^2 = 1\). Let
\(S = \bc{(i,n)\colon \lin=k}\).

\begin{align*}
p(h_{jk} \mid \y, \rest)  &\propto p(h_{jk}) \prod_{(i,n) \in S} p(y_{inj} \mid h_{jk}, \rest) \\
%
&\propto
\exp\bc{\frac{-(h_{jk} - m_j)^2}{2S_j^2}}
 \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhSpiel{h_{jk}}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(h_{jk}\), and should be computed accordingly. Moreover,
we will only recompute the likelihood (in the metropolis acceptance
ratio) when \(Z_{jk}\) becomes different.

\subsection{\texorpdfstring{Full Conditional for
\(\bm \lambda\)}{Full Conditional for \textbackslash{}bm \textbackslash{}lambda}}\label{full-conditional-for-bm-lambda}

The prior for \(\lin\) is \(p(\lin = k \mid \bm W_i) = W_{ik}\).

\begin{align*}
p(\lin=k\mid \y,\rest) &\propto p(\lin=k) ~ p(\y \mid \lin=k, \rest) \\
&\propto W_{ik}
\prod_{j=1}^J 
\p{
  \sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
  \N(y_{inj} \mid 
  \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
}\\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(k = 1,...,K\). Moreover, since \(k\) is discrete, a Gibbs update can
be done on \(\lin\).

\subsection{\texorpdfstring{Full Conditional for
\(\bm W\)}{Full Conditional for \textbackslash{}bm W}}\label{full-conditional-for-bm-w}

The prior for \(\bm{W}_i\) is \(\bm W_i \sim \Dir(d, \cdots, d)\). So
the full conditional for \(\bm{W}_i\) is:

\begin{align*}
p(\bm W_i \mid \rest) \propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i} p(\lin \mid \bm{W}_i)\\
\propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\Ind{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{d/K-1} \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\Ind{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{\p{d/K + \sum_{n=1}^{N_i}\Ind{\lin=k}}-1}\\
%
\end{align*}

Therefore, \[
\bm{W}_i \mid \y,\rest ~\sim~ \Dir\p{d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=1},...,d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=K}} 
\]

Consequently, the full conditional for \(\bm{W}_i\) can be sampled from
directly from a Dirichlet distribution of the form above.



\end{document}
