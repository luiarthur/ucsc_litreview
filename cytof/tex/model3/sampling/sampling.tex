\documentclass[12pt,]{article}

%{{{1
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
%\usepackage{ifxetex,ifluatex}
%\usepackage{fixltx2e} % provides \textsubscript
%\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
%  \usepackage[T1]{fontenc}
%  \usepackage[utf8]{inputenc}
%\else % if luatex or xelatex
%  \ifxetex
%    \usepackage{mathspec}
%  \else
%    \usepackage{fontspec}
%  \fi
%  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
%\fi
% use upquote if available, for straight quotes in verbatim environments
%\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
%% use microtype if available
%\IfFileExists{microtype.sty}{%
%\usepackage{microtype}
%\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
%}{}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Sampling Scheme for CYTOF Model3},
            pdfauthor={Arthur Lui},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
%\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\bc}[1]{ \left\{#1\right\} }
\newcommand{\abs}[1]{ \left|#1\right| }
\newcommand{\mat}{ \begin{pmatrix} }
\newcommand{\tam}{ \end{pmatrix} }
\newcommand{\suml}{ \sum_{i=1}^n }
\newcommand{\prodl}{ \prod_{i=1}^n }
\newcommand{\ds}{ \displaystyle }
\newcommand{\df}[2]{ \frac{d#1}{d#2} }
\newcommand{\ddf}[2]{ \frac{d^2#1}{d{#2}^2} }
\newcommand{\pd}[2]{ \frac{\partial#1}{\partial#2} }
\newcommand{\pdd}[2]{\frac{\partial^2#1}{\partial{#2}^2} }
\newcommand{\N}{ \mathcal{N} }
\newcommand{\E}{ \text{E} }
\def\given{~\bigg|~}
\usepackage{float}
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\ind}{\overset{ind}{\sim}}
\newcommand{\I}{\mathrm{\mathbf{I}}}

\def\bet{\bm{\eta}}


\allowdisplaybreaks
\def\M{\mathcal{M}}
\def\logit{\text{logit}}
\def\Bern{\text{Bernoulli}}
\def\N{\text{N}}
\def\G{\text{Ga}}
\def\IG{\text{Inverse-Gamma}}
\def\Dir{\text{Dirichlet}}
\def\Be{\text{Be}}
\def\lin{\lambda_{in}}
\def\btheta{\bm{\theta}}
\def\y{\bm{y}}
\newcommand\m{\bm{m}}
\def\mus{\mu^\star}
\def\sss{{\sigma^2}^\star}
\input{includes/mhSpiel.tex}
\newcommand{\Ind}[1]{\mathbbm{1}\bc{#1}}
\def\rest{\text{rest}}
\def\bang{\boldsymbol{\cdot}}
\def\h{\bm{h}}
\def\Z{\bm{Z}}
\def\Unif{\text{Unif}}
\def\Prob{\text{Pr}}


%sim-tex-commands
\newcommand{\true}{{\mbox{\tiny TR}}}
\newcommand{\bZ}{\mbox{\boldmath $Z$}}
\newcommand{\bp}{\mbox{\boldmath $p$}}
\newcommand{\bq}{\mbox{\boldmath $q$}}
\newcommand{\bz}{\mbox{\boldmath $z$}}
\newcommand{\bw}{\mbox{\boldmath $w$}}
\newcommand{\bW}{\mbox{\boldmath $W$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}

\newcommand{\by}{\mbox{\boldmath $y$}}
%\newcommand{\bm}{\mbox{\boldmath $m$}}

\def\bmu{\bm{\mu}}
\def\bnu{\bm{\nu}}
\def\bomega{\bm{\omega}}
\def\bgam{\bm{\gamma}}
\def\bsig{\bm{\sigma}}
\def\blambda{\bm{\lambda}}


%%% Graphing
\def\beginmyfig{\begin{figure}[H]\center}
\def\endmyfig{\end{figure}}
\usepackage{pgffor}
%}}}1

\title{Bayesian Feature Allocation Models for Natural Killer Cell Repertoire Studies Using Mass Cytometry Data}
\author{Arthur Lui}
\date{\today}

\begin{document}
\maketitle
\onehalfspacing



\begin{abstract}
\noindent
Bayesian feature allocation models (FAMs) embedded with
clustering are developed to analyze mass cytometry data, with primary aim to
characterize underlying cell repertoire structures.   Cell repertoires in
samples are heterogeneous. Each repertoire consists of a collection of cells
possessing different phenotypes that can be characterized by differences in
expression levels of cell surface markers.  In particular, mass cytometry data
collected to study the clinical efficacy of natural killer (NK) cells as
immunotherapeutic agents against leukemia is considered. NK cells play a
critical role in cancer immune surveillance and are the first line of defense
against viruses and transformed tumor cells.  The data includes expression
levels of 32 surface markers on each of thousands of cells from multiple
samples. NK cell repertoires may affect both NK cell function and immune
surveillance.  A key conceptual shift compared with existing approaches is to
explicitly characterize latent cell phenotypes through a FAM.  The models
simultaneously (1) characterize NK cell phenotypes based on expression /
non-expression of surface markers, (2) estimate compositions of the samples
based on the identified phenotypes, and (3) infer associations of subjects'
covariates, with the composition of the identified phenotypes in the samples.
The conventional Indian buffet process (IBP), one of the most popular feature
allocation models, is first utilized to model cell phenotypes. Non-ignorable
missing data that is present due to technical artifacts in mass cytometry are
accounted for using an informed prior missing mechanism. The repulsive FAM
(rep-FAM) is next proposed.  In contrast with the IBP, the rep-FAM produces a
parsimonious representation of phenotypes by discouraging the creation of
redundant phenotypes, and can thus improve inference on phenotypes.  Further
extensions to incorporate subject-based covariates are discussed to provide
inferences on phenotypes potentially associated with positive clinical
outcomes.  

%\noindent
%{\em Keywords:} ~  Count data, Laplace prior, Metagenomics, Microbiome, Regularizing prior, Process convolution,  Negative binomial model, Next-generation sequencing
\end{abstract}




\section{Introduction}
Clinical application of natural killer (NK) cells %as immunotherapeutic agents against leukemia
recently has emerged as a powerful treatment modality for advanced cancers refractory to conventional therapies \citep{rezvani2015application}. % and intensely investigated.
%NK cells, the third lymphocyte lineage, % preceded by T cells and B cells,
%play critical roles in the immune response to certain %virus infected cells and
%transformed tumor cells.
NK cells play a critical role in cancer immune surveillance and are the first line of defense against viruses and transformed tumor cells.
They have the intrinsic ability to infiltrate cancer tissue and their presence in the tumor is reported to be associated with better clinical outcomes \citep{suck2016natural}.  %NK cells develop in the bone marrow and are ``educated'' during their development to ensure not to attack normal self cells whiling rapidly killing tumor or virally infected cells.
% (called self-tolerance).
%During the development,
Drs. Thall and Rezvani, collaborators at UT MD Anderson Cancer Center, have conducted clinical trials to study potential clinical efficacy of umbilical cord blood (UCB) transplantation as a therapy for leukemia.  UCB has become an established source of hematopoietic stem cells for transplantation. UCB NK cell therapy has the advantages of low risk of viral transmission from donor to recipient \citep{sarvaria2017umbilical}. In the trials, leukemia patients received UCB cell transplants. % after irradiation therapy.  %Some characteristics of the patients such as their survival after the treatment are recorded.
During follow up, samples were taken at multiple time points from each patient.  Samples from %In addition, peripheral blood samples from
healthy subjects and cord blood samples also were collected for comparison to leukemia patient samples.
The samples were processed and expressions of 32 NK cell-associated cell surface protein markers, %in individual cells of the samples
measured for individual cells in the samples using mass cytometry.
Their primary research goal is to understand phenotypes and functions structured across heterogeneous NK cells. % based on the markers' expression levels.
Better understanding of the characteristics of NK cells is crucial to estimate the true potential of NK cell therapies against cancer.  %for cancer immunotherapy. %against leukemia and possibly also against solid tumors.



%NK cells play critical roles in defending against tumors. Furthermore, their diversity and function are
%known to be linked. Researchers have thus studied NK cell diversity from
%various perspectives. For instance, it is known that NK cell diversity
%is lower at birth \citep{strauss2015human} than in adults. Some
%researchers have studied the effect of introducing diverse NK cells into
%tumor patients. Yet again, some researchers have found that patients
%with higher NK diversity are associated with higher exposure risk of
%HIV-1, suggesting that existing diversity may decrease flexibility of
%the antiviral response. Many questions about NK cells remain to be
%answered. Understanding NK diversity through spectrometry has therefore
%been an important research area in the bio-sciences.   

%The main
%inferential goal of this project is to identify the NK cell phenotypes
%(or cell-types) in various samples as a set of subpopulations of the set
%of some provided surface markers. The NK cell-types are latent, and for
%\(J\) markers \(2^J\) different cell-types can be considered. This
%provides a computational challenge when the number of markers is even
%moderately large. Thirty-two markers are included in this analysis, and
%naively enumerating all possible markers is not feasible. We therefore,
%use a latent feature allocation model to learn the latent structure of
%predominant cell-types. Latent feature models have been successfully
%applied to various problems and will be reviewed in the following
%section.
%Data for this project is rendered through CyTOF analyses of
%NK-cell-targeting markers. Having some understanding of CyTOF and NK
%cells their importance is therefore necessary.


Advances in cytometry has led to more research and greater understanding  of
natural killer (NK) cells and how their diversity impacts immunity against the
development of tumors and other viral diseases. Flow cytometry (developed by
Wallace Coulter in the 1950's) is a laser-based biophysical technology which is
sometimes used for biomarker detection. It is regularly used to diagnose health
disorders like cancer. Cytometry has advanced over the years.
Fluorescence-based flow cytometry, which makes use of fluorescent dyes and
lasers that emit light at specific wavelengths, is one such advancement that
has been mainstream for several decades \citep{herzenberg2002history}.  In recent
years, a new technique called Cytometry at time-of-flight (CyTOF) has surfaced.
It makes use of time-of-flight mass spectrometry, where sophisticated devices
are used to accelerate, separate, and identify ions by mass. This new method
enables analyses of multiple parameters to be done in less time
\citep{cheung2011screening}. 
%Through CyTOF, scientists have been able to better understand natural killer
%(NK) cells \citep{horowitz2013genetic}. 
A major challenge in understanding mass cytometry data is developing efficient
inferential frameworks that can handle their complexity. These data are often
high-dimensional and noisy. Hence, many existing computational methods for
analyzing cytometry data use dimension reduction techniques and/or
clustering-based approaches.
%{\tt introduce some existing approaches.  Explain a bit the methods
%that we will use for comparison.  State limitations of existing methods.  Are
%they Bayesian?  Are they modeling phenotypes like our $\Z$.  Emphasize that our
%methods produce direct inferences on phenotypes or quantification of
%uncertainty associated with their inference.}
%
%{\tt include several examples of existing methods. explain what they do and
%produce for inference. explain what they miss.}
%
To better understand the properties of these different methods, 
\cite{weber2016comparison} performed a study which compares freely available
clustering methods for high-dimensional cytometry data. These methods are based
on a variety of techniques including hierarchical clustering methods , k-means,
density-based clustering methods, model-based clustering methods,
nearest-neighbor methods, and self-organizing maps. Each method exhibits their
own strengths.
% kmeans
In K-mean, one of the most well known clustering methods, observations are
iteratively grouped into clusters based on their proximity to the $K$
centroids, and then the centroids are updated based on the newly formed
clusters. This method, while fast, memory-efficient, and intuitive, produces
only point estimates of clusters. In addition, the number of clusters needs to
be predetermined.
%This is a common trait among the clustering methods used in analyzing cytometry
%data.
% hclust
In hierarchical clustering, no assumptions about the number of clusters are
made, and the final output is a dendrogram, which can then be cut at different
levels, forming clusters. However, these dendrograms are typically cut based on
some visual criteria, which can be cumbersome.  Hierarchical clustering
implementations also typically suffer from memory-related bottlenecks for large
datasets.  Memory-efficient implementations suited for such datasets have been
implemented in the statistical programming language R by
\citep{linderman2013package}, making it a viable candidate method for cytometry
data.
% nearest neighbors. phenograph @levine2015data
Nearest-neighbor clustering methods like PhenoGraph \citep{levine2015data}
similarly attempt to reduce the computational costs of hierarchical clustering
methods by merging clusters by following paths in the nearest neighborhood
graph of clusters.
%
These mentioned hierarchical clustering methods do not provide uncertainty
estimates.
%
% FLOWSOM
Some methods, like FlowSOM \citep{van2015flowsom}, are able to learn the number
of clusters in the data, while also allowing the number of clusters to be
explicitly specified.  FlowSOM uses a self-organizing map (SOM) to reduce the
dimensionality of cytometry data, and cluster them accordingly. Self-organizing
maps are trained using unsupervised neural networks to obtain a low-dimensional
representation of the input space. Since neural networks are capable of
learning non-linear functions, SOMs may be considered a nonlinear
generalization of principal components analysis \citep{yin2008learning}. In the
FlowSOM implementation, the input space is projected onto a two-dimensional
space and can be conveniently visualized in a graph called a map.
Among existing methods FlowSOM is considered the fastest and most flexible at
finding subpopulations in cytometry data. However, it does not quantify
uncertainty about the learned clusters.
%% TODO: Write about density-based, model-based, and nearest neighbors.
% density-based. DBSCAN \citep{ester1996density}
Another class of methods where the number of clusters does not need to be
specified beforehand is density-based clustering methods. The earliest of these
methods include DBSCAN \citep{ester1996density}. In density-based
clustering, clusters are formed when regions are densely occupied by
observations. Points from dense clusters that are near enough (according to
some predetermined proximity metric and threshold) are grouped into the
cluster. Points that are considered far from dense regions are classified as
outliers. In general, density-based methods can produce clusters that take on
very flexible shapes. But again, no uncertainty estimates are produced for
these clusters. In addition, older implementations like DBSCAN can be
computationally inefficient, having a worst-case complexity of
$\mathcal{O}(n^2)$ (due to the computation and storage of the distance matrix). 
% density-based. ClusterX @chen2016cytofkit 
ClusterX \citep{chen2016cytofkit} is a density-based clustering method used for
cytometry data. It alleviates some computational and memory burdens by using a
split-apply-combine strategy where the data are first split into smaller 
manageable chunks so as to compute a smaller distance matrix. Parameters
are computed for each small chunk and the results are combined. 
%
% model-based. flowClust @lo2009flowclust. 
%%%% - Uses mixture of t-dist on box-cox-transformed data
%%%% - model is fit using EM.
%%%% - Model is fit K times, each time with different number of clusters
%%%%      - using BIC, the number of clusters is chose
%%%% - slower. Where FlowSOM takes seconds, this could take hours to get about the same performance
Another clustering approach is model-based clustering, the most popular of
which is the finite gaussian mixture model. To account for irregular shapes and
the presence of outliers in cytometry data, \cite{lo2009flowclust} developed
flowClust, which first applies a Box-Cox transformation to the data, and then
uses a mixture of t-distributions to cluster the transformed data.  The
expectation-maximization (EM) algorithm \citep{dempster1997maximum} can be used
to learn the parameters in this model.
%Model-based clustering algorithms are
%among the best performing algorithms used in cytometry data analysis, but they
%can be much slower to train. For instance, flowClust may take hours to train in
%order to get the same performance as FlowSOM when it is only trained for
%seconds.
% 

In addition to the lack of uncertainty measurements, the methods used above do
not directly address missing values. Instead, missing values need to be
pre-imputed to produce a complete dataset which is then used for analyses.
%
To overcome the limitations of existing methods, we propose developing novel
Bayesian feature allocation models (FAMs) embedded with clustering
capabilities. We also propose mechanisms for imputing missing values within the
models we develop.


\subsection{Literature Review: Feature Allocation Models}\label{literature-review}
In the motivating data, one of the main inferential goals is  to learn a latent
structure of predominant cell phenotypes, where cell phenotype are composed of
distinct expression combinations of the markers. Specifically, phenotype $k$ is
represented by a $J$-dim binary vector, $\bm z_k=(z_{1k}, \ldots, z_{Jk})$,
with $J$ denoting the number of markers, where $z_{jk}$ equals 1 if marker $j$
is expressed in phenotype $k$, and 0 otherwise.  Let $\bZ$ denote a $J \times
K$ binary matrix by letting columns represent phenotypes.  \(2^J\) different
phenotypes can be constructed for \(J\) markers.  One may consider a \(J \times
2^J\) binary matrix that includes all possible phenotypes generated by the
\(J\) markers. It is computationally infeasible even when \(J\) is moderately
large. Taking a Bayesian approach, we consider a prior probability model over
binary matrices
%with infinite number of columns 
and learn the predominant ones in posterior that  generate observed data.
%For computational efficiency, we require a flexible prior which will also
%learn the number of cell-types \((K)\). 
These models are called latent feature allocation models. In feature allocation
models, rows and columns correspond to objects and features, respectively (in
our applications, markers and phenotypes, respectively). Similar to our
construction of phenotypes, if object $j$ takes feature $k$, the corresponding
$z_{jk}$ takes the value 1. Otherwise, it takes 0.  One of popular models for
binary feature matrices is the Indian buffet process (IBP), a Bayesian
nonparametric distribution over $\bZ$ with an unbounded number of latent
features, proposed by \citet{griffiths2011indian}. 
%The IBP have been used in a variety of applications where modelling latent binary features is of interest. 
They construct the IBP by considering the finite feature allocation
model and taking the limit with respect to the number of features; for a given
$K$,
\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1),~ k=1, \ldots, K \\
z_{jk} \mid \pi_k &\sim \text{Bernoulli}(v_k),~ k=1, \ldots, K~\mbox{ and } j=1, \ldots, J. \\
\end{split}
\label{eq:ibp}
\end{align}
The marginal limiting distribution of $\Z$ defines an IBP as $K \rightarrow
\infty$ and dropping all columns with all 0's, that is,  \(Z \sim
\text{IBP}(\alpha)\).
%Under the IBP, an object can take a set of latent features and is expected to
%have  \(\alpha\) active features.
Under the IBP, each row has an expected row sum of $\alpha$.
%
%{\tt active means?}
%
It can be shown that the number of non-zero columns in $Z$ has a Poisson
distribution with mean $\alpha \sum_{j=1}^J j^{-1}$. Hence, a prior
distribution can be placed on \(\alpha\) to reflect uncertainty of the number
of latent features. A gamma prior is popular for $\alpha$ due to conjugacy.
\cite{teh2007stick} represented the IBP using the stick-breaking construction
similar to the stick-breaking representation of the Dirichlet process (DP).
\cite{williamson2010dependent} developed a dependent IBP (dIBP) to induce
correlations between objects and to model multiple $\Z$s that can be dependent.

{\tt need to include Tamara's papers on feature allocation model, my papers on
  tumor heterogeneity. 

\begin{itemize}
\item ``Feature allocations, probability functions, and paintboxes.'',
  ``Combinatorial clustering and the beta negative binomial process.'' etc by
  Tamara \url{http://www.tamarabroderick.com/papers.html}

\item ``Bay- clone: Bayesian nonparametric inference of tumor subclones using
  ngs data'' by Subhajit et al.
 
\item YX's works: ``Bayesian Inference for Latent Biologic Structure with
  Determinantal Point Processes'', "MAD Bayes for Tumor Heterogeneity Feature
  Allocation with Exponential Family Sampling"
  \url{http://www.ams.jhu.edu/~yxu70/pub.html}.

\item There could be more.  Please search for those.
\end{itemize}
}

{\tt please include more. for example, XY's JASA paper develops a fast
computational method for feature allocation models.  It is something that we
need to know.  }

The IBP has been extended so that the feature allocation matrix may contain
integer values beyond 0 and 1 by replacing the Bernoulli distribution with an
integer-supported distribution \citep{broderick2013feature}. Such models have
been applied in topic modeling and computer vision
\cite{broderick2015combinatorial} as well as in the study of tumor sub-clones
\citep{sengupta2014bayclone}.
%The IBP has been successfully applied in feature
%allocation models to study tumor heterogeneity \citep{lee2016bayesian}, 

\subsection{Proposed Projects}
{\tt include a short summary of all three projects and explain how your report
will be organized in the later part.}

We propose Bayesian feature allocation models to study NK cell populations in our motivating data;
\begin{itemize}
\item \underline{Project 1:}

\item \underline{Project 2:}

\item \underline{Project 3:}

\end{itemize} 
The remainder of the document is organized as follows. Sections~\ref{sec:proj1}-\ref{sec:proj3} describe the proposed projects. Section~\ref{sec:time} discusses a plan to progress the projects.  





\section{Project 1: Bayesian Feature Allocation Model for Heterogeneous Cell Populations Using Mass Cytometry}\label{sec:proj1}
\subsection{Introduction}
% {\tt short intro for project 1}

{\tt expand} 

In this project, we analyze marker expression data from a CyTOF analysis of
blood samples from patients. Each sample consists of tens of thousands of cells
and records expression levels for 32 NK-cell markers. High marker expression
levels correspond to the expression of that marker, while low marker expression
levels and missing values correspond to non-expression. For a given cell, the
expression of certain markers corresponds to a phenotype. We are interested in
identifying phenotypes that occur frequently within each sample. This is
important to practitioners because NK-cell diversity is known to be affect
immunity against infectious diseases. We therefore propose modelling marker
expression levels with a flexible mixture model, and the latent cell-type
structure with a latent feature allocation model.



{\tt include some references on missing value modeling. Alex's paper and
Rubin's old papers.}  



\subsection{Model}\label{prob-model}
\subsubsection{Sampling Model} 
Samples are taken from \(I\) subjects, \(i = 1,2,...,I\). Sample \(i\)
consists of \(N_i\) cells, \(n=1, \ldots, N_i\) and for each cell,
expression levels of \(J\) markers are measured. Let
\(\tilde{y}_{inj} \in \mathbb{R}^+\) represent the raw measurement of an
expression level of marker \(j\) of cell \(n\) in sample \(i\). Let
\(c_{ij}\) denote the ``cutoff'' for
 marker \(j\) in sample \(i\). A marker of a cell is likely to be expressed if its observed expression level is greater than the cutoff. A value of $\tilde{y}_{inj}$ below the cutoff may imply that marker $j$ is not expressed in cell $n$ of sample $i$.    We consider the logarithm transformation
after scaling \(\tilde{y}_{inj}\) by \(c_{ij}\), \[
y_{inj}=\log\p{\frac{\tilde{y}_{inj}}{c_{ij}}} \in \mathbb{R}.
\]
Due to the transformation, a value above (below) 0 is likely to represent (no) expression. For some \((i, n, j)\), \(\tilde{y}_{inj}\) is missing due to experimental artifacts and we
introduce a binary indicator, \[
m_{inj} = \begin{cases}
  0, & \text{if $\tilde{y}_{inj}$ is observed,} \\
  1, & \text{if $\tilde{y}_{inj}$ is missing.}
\end{cases}
\] %That is, \(m_{inj}=1\) indicates that the expression level of marker \(j\) of cell \(n\) in sample \(i\) is missing.

%\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
%\tightlist
%\item
%  The data have infinite support.
%\item
%  \(y_{inj} = 0\) has a special meaning, which is that the data take on
%  the same value as the cutoff. Consequently, \(y_{inj} > 0\) means that
%  the data take on values greater than the cutoff, etc.
%\item
%  \(y_{inj}\) for which \(\tilde y_{inj} = 0\) are regarded as missing,
%  and is to be imputed.
%\end{enumerate}

%\newpage


We assume that a sample has heterogeneous cell populations having $K$ different phenotypes.  The phenotypes are not directly observable and we introduce latent phenotype indicators \(\lambda_{in} \in \{1, \ldots, K\}\), for cell $n$ in sample $i$, \(i=1, \ldots, I\) and \(n=1, \ldots, N_i\).
%\(\lambda_{in} \in \{1, \ldots, K\}\) denotes the cell phenotype of cell \(n\) in sample \(i\) defined by 
The event $\lambda_{in}=k$, $k=1, \ldots, K$ represents that cell $n$ in sample $i$ possesses phenotype $k$.  The cell phenotypes are defined by columns of \(J \times K\) binary matrix \(\Z\). The element \(z_{j, k} \in \{0, 1\}\) indicates if marker \(j\) is expressed in cell phenotype \(k\). The event \(z_{jk}=0\) represents that marker \(j\) is not expressed for phenotype $k$, and \(z_{jk}=1\) for
expression. We let $\Z$ and $\lambda_{in}$ random. Details will be discussed later.  Given \(z_{j, \lambda_{in}} \in \{0, 1\}\), we assume a mixture of
normals for \(y_{inj}\),
\begin{align}
y_{inj} \mid \eta_{ij}, \bmu^\star, \bsig^{2 \star}_{i} \ind
\begin{cases}
\sum_{\ell=1}^{L^0} \eta^0_{ij\ell}~ \N(\mu^\star_{0\ell}, \sigma^{2 \star}_{0i\ell}), &\mbox{if $z_{j,\lambda_{in}}=0$},\\
\sum_{\ell=1}^{L^1} \eta^1_{ij\ell}~ \N(\mu^\star_{1\ell}, \sigma^{2 \star}_{1i\ell}), &\mbox{if $z_{j,\lambda_{in}}=1$},\\
\end{cases} \label{eq:y-mix}
\end{align}
where the number of mixture components \(L^0\) and \(L^1\) are fixed.  The vectors $\bet^0_{ij}$ and $\bet^1_{ij}$ are mixture weights with \(\sum_{\ell=1}^{L^0} \eta^0_{ij\ell}=\sum_{\ell=1}^{L^1}\eta^1_{ij\ell}=1\), where \(0 < \eta^1_{ij\ell} < 1\) and \(0 < \eta^0_{ij\ell} < 1\).  In \eqref{eq:y-mix}, $\bmu^\star_1$ and $\bmu^\star_1$ are common for all samples and markers but $\bsig^{2\star}$ are indexed by sample $i$ to account for sample specific variability.  Sample and marker specific mixture weight vectors $\bet^0_{ij}$ and $\bet^1_{ij}$ allow markers in samples to have different distributions.   The mixture model can thus flexibly capture various features in data.  For easy computation, we introduce mixture component indicators $\gamma_{inj}$ for $y_{inj}$.  Given \(\lambda_{in}=k\) we define
\(\gamma_{inj}\); for \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and
\(j=1, \ldots, J\),
\begin{eqnarray}
p(\gamma_{inj} = \ell)=\eta^{z_{jk}}_{ij\ell}, \mbox{ where }~ \ell \in \{1,\ldots, L^{z_{jk}}\}. \label{eq:gam}
\end{eqnarray}
Given \(\lambda_{in}=k\) and \(\gamma_{inj}=\ell\), we assume a normal
distribution for \(y_{inj}\); for \(i=1, \ldots, I\),
\(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  y_{inj} \mid \mu_{inj}, \sigma^2_{inj}  &\ind \N(\mu_{inj}, \sigma^2_{inj}), \label{eq:y-gam}
\end{align}
where \(\mu_{inj} = \mu^\star_{z_{j,k},\ell}\) and \(\sigma^2_{inj} =
{\sigma^{2}}^\star_{iz_{j,k}\ell}\). After marginalizing over $\gamma_{inj}$,
the model in \eqref{eq:y-gam} and \eqref{eq:gam} is equivalent to the model in
\eqref{eq:y-mix}.  

We next build a model for the missingness mechanism.
%{\tt explain our approach for missing in few sentences.}
To build the mechanism, we incorporate information provided by a subject scientist that a marker expression level is
recorded as ``missing'' when a maker in a cell has a very weak signal, strongly implying that the marker is not expressed. % from cells for a marker are extremely weak.  
We take an empirical approach by assuming that the distribution of the values with $m_{inj}=1$ is similar to the distribution of observed $y$ below 0. We then let the probability of $y$ being missing depend on its unobserved value of $y$.  % and assume   % use observed values of $y$ to elicit the probability of $y$ being missing, $\Prob(m_{inj}=1 \mid y_{inj})$ as a function of $y$.  %In addition, we will impose the restriction that extremely low
%expression values have low prior probability. This ensures that parameters in
%the sampling density will not be unnecessarily inflated.
%
Given
\(y_{inj}\), we consider a selection function for \(m_{inj}\); for
\(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1, \ldots, J\),
\begin{align}
  m_{inj} \mid p_{inj} &\ind \Bern(p_{inj}) \nonumber \\
  \logit(p_{inj}) &= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber, \\
  \beta_{0i} - \beta_{1i}c_1\p{y_{inj}-c_0}^{1/2}, & \text{otherwise}, \nonumber \\
  \end{cases} \label{eq:missing}
\end{align}
where \(c_0\) and \(c_1\) are real constants, $\beta_{0i} \in \mathbb{R}$ , and
$\beta_{1i} > 0$.
%{\tt say something like it is ``untestable'' but take this approach by
%incorporating our input from biologists}

{\tt include a figure showing what the function looks like. }

Note that the assumptions for the distribution of the unobserved data are
untestable. However, we can incorporate input from biologists through informed
prior specifications. Prior uncertainty can then be directly propagated to the
posterior distribution for the missing mechanism.


\subsubsection{Priors}\label{priors}
\paragraph*{Latent cell phenotypes}  Recall that we characterize cell phenotypes with a $J\times K$ binary matrix \(\Z =\{z_{jk}\}\).  Following \citet{williamson2010dependent}, we assume
\begin{eqnarray*}
v_k \mid \alpha &\iid& \Be(\alpha/K, 1),~ k=1, \ldots, K, \\
\h_k &\iid& \N_J(\bm{0}, \Gamma), \\ 
z_{jk} \mid h_{jk}, v_k &=& \mathbb{I}\left\{ \Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k \right\},
\end{eqnarray*}
where $\Phi(h \mid m, s)$ is the cumulative distribution function of the normal
distribution with mean $m$ and variance $s$ and $\mathbb{I}(\cdot)$ is an
indicator function having 1 if $\Phi(h_{jk} \mid 0, \Gamma_{jj}) < v_k$ or 0
otherwise.  As $K \rightarrow \infty$, the limiting distribution of $Z$ is the
IBP \citep{griffiths2011indian}.  Interactions between $J$ markers in
phenotypes can be modeled through $\bm G$.  Due to the multivariate probit
construction for $\Z$, $\Gamma$ is not identifiable and it is common to
restrict $\Gamma$ to be a correlation matrix. Prior distributions for
correlation matrices have been proposed to handle such cases. Jointly uniform
and marginally uniform prior distributions have been identified by
\cite{barnard2000modeling} for correlation matrices. \cite{box2011bayesian}
have noted that the Jeffreys' prior for correlation matrices is $p(\Gamma)
\propto \abs{\Gamma}^{-(J+1)/2}$.  \cite{zhang2006sampling} presents more
generalized and flexible priors which can be reduced to the two previous priors
as special cases.
%
%{\tt discuss a bit models for correlation matrix.  I will send you some
%references on models for correlation matrix.}
%
We let $\alpha \sim \G(a_\alpha, b_\alpha)$ with mean $a_\alpha/b_\alpha$.  

The $K$ cell phenotypes are common in all samples but the relative weights vary
across samples. Let $w_{ik}$ denote an abundance level of phenotype $k$ in
sample $i$.  We assume independent Dirichlet priors for $\bw_i=(w_{i1},
\ldots, w_{iK})$ given $K$, $\bw_{i} \mid K \iid \Dir_K(d/K)$. For latent
cell phenotype indicators, we let $p(\lin=k \mid \bm \bw_i) = w_{ik}$.

\paragraph*{Parameters in the Mixture for $y$}
In \eqref{eq:y-mix}, normal mixture models are assumed for $y_{inj}$. The mean
expression level of maker $j$ in cell $n$ is determined by its phenotype
$\lambda_{in}$.  In particular, if the marker is not expressed in the cell
type, $z_{j \lambda_{in}}=0$, its mean expression level is below the cutoff,
that is, a negative value.  If the marker is expressed $z_{j \lambda_{in}}=1$,
the expression of marker $j$ take a positive value.   Recall that
$\mus_{0\ell}$, $\ell=1, \ldots, L^0$ are mixture locations for $z_{j
\lambda_{in}}=0$ and $\mus_{1\ell}$, $\ell=1, \ldots, L^1$ for $z_{j
\lambda_{in}}=1$.  We assume 
\begin{align*}
\mus_{0\ell} \mid \psi_0, \tau^2_0 &\iid \N_-(\psi_0, \tau^2_0), ~~~ \ell \in \bc{1,...,L^0}, \\
\mus_{1\ell} \mid \psi_1, \tau^2_1 &\iid \N_+(\psi_1, \tau^2_1), ~~~ \ell \in \bc{1,...,L^1}, 
\end{align*}
where \(\N_-(m,s^2)\) and \(\N_+(m,s^2)\) denote the normal distribution with
mean \(m\) and variance \(s^2\), truncated to take only negative values and
positive values, respectively.  The variances $\bsig^{2 \star}_0$ and $\bsig^{2 \star}_1$ in the mixture components differ
by the value of $z_{j \lambda_{in}}$ and also vary across samples. We let, for
\(i=1, \ldots, I\),
\begin{align*}
\sigma^2_{0i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^0}, \\
\sigma^2_{1i\ell} \mid s_i &\ind \IG(a_\sigma, s_i), ~~~ \ell \in \bc{1,...,L^1}.  
\end{align*}
We also assume $s_i \iid \G(a_s, b_s)$, $i \in \bc{1,...,I}$, with mean
\(a_s/b_s\). Lastly, we consider a model for mixture weights $\bm\eta^0_{ij}$ and $\bm\eta^1_{ij}$. To flexibly model
the distribution of $y$, we assume for a marker in a sample have two sets of it
own weights, one for each value of $z$, $\bm\eta^0_{ij}$ and $\bm\eta^1_{ij}$,
for each $(i, j)$. So for \(i=1, \ldots, I\), \(n=1, \ldots, N_i\) and \(j=1,
\ldots, J\),
\begin{align*}
\bm\eta^0_{ij} &\iid \Dir_{L^0}(a_{\eta^0}/L^0), \\
\bm\eta^1_{ij} &\iid \Dir_{L^1}(a_{\eta^1}/L^1). 
\end{align*}


\paragraph*{Parameters for Missingness Mechanism}
%{\tt explain what you do for missingness mechanism.}
%
A prior distribution over the missing mechanism can be specified through
placing priors on the parameters $\beta_{0i}$ and $\beta_{1i}$. 
%
We assume that $\beta_{0i} \iid \N(m_{\beta_0}, s^2_{\beta_0})$ and $\beta_{1i}
\iid \N_+(m_{\beta_1}, s^2_{\beta_1})$, $i=1, \ldots, I$.  We use data to
specify the values of the fixed hyperparameters, $m_{\beta_0}$ and
$m_{\beta_1}$. We let $s^2_{\beta_0}$ and $s^2_{\beta_1}$ be small to induce a
informative prior for $\beta_{0i}$ and $\beta_{1i}$. One way of determining
priors for the parameters in the missing mechanism is described in detail in 
the derivation of the full conditionals for $\beta$ in the appendix.



\subsubsection{Posterior Computation}\label{sampling-via-mcmc}
Let \(\btheta=\{\bZ, \bw, \bm \mu^\star_0, \bm \mu^\star_1, \bm \sigma^2_{0i}, \bm \sigma^2_{1i}, \bm \eta^0, \bm \eta^1, \bm \lambda, \bm \gamma, \bm v, \bm h, \bm \beta_0, \bm \beta_1\}\) represent all random parameters.  Let \(\y\) and \(\m\) denote all \(y_{inj}\) and \(m_{inj}$ for all \((i,n,j)\), respectively. The joint posterior distribution is 
\begin{align*}
p(\btheta \mid \y, \m) &\propto 
p(\btheta) \prod_{i,n,j} p(m_{inj} \mid y_{inj}, \btheta) p(y_{inj} \mid \btheta) \nonumber\\
&=  
p(\btheta)
\prod_{i,n,j} \left[
  p_{inj}^{m_{inj}} (1-p_{inj})^{1-m_{inj}} \times 
   \frac{1}{\sqrt{2\pi\sigma^2_{inj}}} \exp\bc{-\frac{(y_{inj}-\mu_{inj})^2}{2\sigma^2_{inj}}}
\right].
\end{align*}
%The marginal density for \(y_{i,n,j}\) after integrating out
%\(\lambda\) and \(\gamma\) is
%\begin{align}
%p(y_{inj} \mid \btheta) = \sum_{k=1}^K W_{ik} \sum_{\ell=1}^{L^{Z_{jk}}}
%\eta^{Z_{jk}}_{ijl} \cdot \N(y_{inj} \mid \mu^\star_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{jk},\ell}).
%\end{align}
Posterior simulation can be done via Gibbs sampling by repeatedly updating each
parameter one at a time until convergence. Parameter updates are made by
sampling from it full conditional distribution. Where this cannot be done
conveniently, a metropolis step can be used.
%
%{\tt do you use the marginal density of $y$ in posterior simulation?  If so,
%explain why we use it.}
%
Details of the posterior simulation are described in Appendix A. 


Summarizing the joint posterior distribution $p(\btheta \mid \y, \m)$ is
challenging, especially for $\Z$, $\bw$ and $\lambda$, possibly due to a
label switching problem.
%
%{\tt Include David's approach here and explain his
%approach.  Also explain why we include $w$ in the adjacency matrix, which is
%different from David's approach. }
We use a method based on sequentially-allocated latent structure optimization
(SALSO) \citep{salso} to find point estimates for $\Z$, $\bm W$, and $\lambda$.
SALSO first constructs $A(\bZ)=\{A_{j,j'}\}$, the $J \times J$ pairwise allocation matrix corresponding to a binary matrix
$\bZ$, where  $A_{j,j'} = \sum_{k=1}^K \Ind{Z_{j,k}=1}\Ind{Z_{j',k}=1}$, $1\leq j, j^\prime \leq J$ is the number of features that markers $j$ and $j'$ share. It then uses posterior samples of $\bZ$ and finds a point estimate $\hat{\bZ}$ that minimizes the sum of elementwise squared distances, 
%
\begin{eqnarray*}
\text{argmin}_Z\sum_{j=1}^J\sum_{j'=1}^J(A(\bZ)_{j,j'} - \bar{A}_{j,j'})^2
\label{eq:salso}
\end{eqnarray*}
%
where $\hat A$ is the pairwise allocation matrix averaged over all posterior
samples of $Z$. %Thus in the current application, $A_{j,j'} = \sum_{k=1}^K
%\Ind{Z_{j,k}=1}\Ind{Z_{j',k}=1}$ is the number of times that marker $j$ and
%marker $j'$ share a feature. 
%
We extend SALSO to point estimates for each sample $i$, $\hat{\bZ}_i$ by incorporating $\bw_i$. That is, we consider the sum of elementwise squared distances weighted by $\bw_i$.  We use posterior Monte Carlo samples to posterior point estimates $\hat{\Z}_i$, $\hat{\bm W}_i$ and $\hat{\lambda}_{in}$, $i=1, \ldots, I$ and $n=1, \ldots, N_i$ as follows; suppose we obtain \(B\) posterior samples simulated from the posterior
distribution of \(\theta\). For each posterior sample of \(\Z\) and \(\bw_i\), we compute a $J \times J$ adjacency matrix, \(\bm A_i^{(b)}
=\{A^{(b)}_{i,j,j'}\}\), where 
\[
A^{(b)}_{i,j,j'} = \sum_{k=1}^K w^{(b)}_{ik} 
\mathbb{I}\left( z^{(b)}_{jk} = 1\right)
\mathbb{I}\left(z^{(b)}_{j^\prime k} = 1\right), b \in \bc{1,...,B}.
\]
We then compute the mean adjacency matrix \(\bar A_i = \sum_{b=1}^B A_i^{(b)} /
B\).  We report a posterior point estimate of $\Z_i$ by choosing
\begin{eqnarray}
\hat{\bm Z}_i = \text{argmin}_{\bm Z} \sum_{j,j'} (A_{i,j,j'}^{(b)} - \bar A_{i,j,j'})^2).\label{eq:myZ}
\end{eqnarray}
Conditioning on $\hat{\Z}_i$, we report posterior point estimates $\hat{\bw}_i$ and $\hat{\lambda}_{in}$. %Note that in our adaptation, we weight
%the adjacency counts by the sample-specific weight for each latent feature.
Eq\eqref{eq:myZ} places greater weight on phenotypes that are more prevalent in samples, and downweights pheotnyes having small $w_{ik}$ for $\hat{\bZ}_i$.


%%% sim-study-proj1 %%%
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[th!]
  \begin{center}
\begin{tabular}{c}
\includegraphics{img/sim/Z_true_all.pdf}
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{The transpose of $\bZ^\true$ with markers in columns and latent phenotypes in rows. Black and white represents $z^\true_{jk}=0$ and 1, respectively.    The phenotypes and $\bw^\true_i$ are shown on the left and right sides of each panel.  The phenotypes are arranged in order of $w_{ik}^\true$ within each sample.}
\label{fig:sim-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Simulation Study} %%% Change name to Simulation Study?
We conducted a simulation study to  to evaluate the performance of the proposed model and compare to those of some existing methods.
%
%To understand the strengths and limitations of the proposed model, a
%simulation study was conducted. The study generated data that resemble the
%cord-blood (CB) CyTOF data. This section details the data-simulation strategy
%and the results of the analysis on the simulated data using the proposed model.
%
%\subsubsection{Data Simulation}
To simulate data, we assume $J=32$ markers and let the numbers of cells in samples $N_i=30000$, 20000 and 10000, $i=1, 2, 3$ for three samples ($I=3$).  We let the true number of latent cell types $K^\true=10$.  We specified $\bZ^\true$ and $\bw^\true_i$, $i=1,2,3$ as follows;
%
{\tt give specific distributions used for $\bw$ and explain the details.} 
%
Figure \ref{fig:sim-Z} shows the transpose of $\bZ^\true$ and $\bw^\true_i$ for the samples. In each panel, the cell types ($\bZ^\true$) are rearranged using $w_{ik}^\true$.  Three and four mixture components were used for non-expressed
and expressed marker expression levels, respectively, $L^{0, \true}=3$ and $L^{1, \true}=4$. We set values for $\sigma^{\star 2, \true}_{0i\ell}$,
$\sigma^{\star 2,\true}_{1i\ell}$, $\mu^{\star, \true}_{0\ell}$ and $\mu^{\star, \true}_{1\ell}$ using empirical values from CB data.
%
{\tt The specified values are .... give the details.}
%
 We simulated $\bet_{0ij}^\true$ and $\bet_{1ij}^\true$, $i=1,2,3$ and $j=1, \ldots, J$ similar to $\bw^\true_i$;
%
{\tt give specific distributions used for $\bet$ and explain the details.} 
%
 We then simulated latent phenotype indicators $\lambda_{in}^\true$ using $\bw_i^\true$ and conditional on $z^\true_{j,\lambda_{in}^\true}$ generated $y_{inj}$ from a mixture model, $\sum_{\ell=1}^{L^{0,\true}} \eta^\true_{0ij}\cdot\N(\mu^{\star, \true}_{0\ell}, \sigma^{\star 2, \true}_{0i\ell}$ or $\sum_{\ell=1}^{L^{1,\true}} \eta^\true_{1ij}\cdot\N(\mu^{\star, \true}_{1\ell}, \sigma^{\star 2, \true}_{1i\ell})$.   Finally, probabilistically let some of the $y_{inj}$ missing as follows; we simulated a proportion $p_{ij}$ of missing for marker $j$ in sample $i$, from a uniform distribution between 0 and  $\sum_k w^\true_{ik}(1-z^\true_{jk})$, sample $p_{ij}\times N_i$ cells  without replacement with probability proportional to
%
{\tt give the function used}.
%
Under the true missingness mechanism, $y$ taking a negative value has a larger chance to be missing, while $y$ with a positive value has only slight chance of being missing.  Note that the true mechanism is different from that assumed in the proposed model.   
Heatmaps of the simulated $\y$ are shown in the heatmaps on the top of each panel in Figure \ref{fig:sim-post-Z}. $y_{inj}$'s are sorted within a sample according to their posterior phenotype estimates.  Red, blue and white colors represent high expression levels, low expression levels, and missing values, respectively.



%\beginmyfig
%\includegraphics[scale=.3]{img/sim/Y001.png}
%\includegraphics[scale=.3]{img/sim/Y002.png}
%\includegraphics[scale=.3]{img/sim/Y003.png} \\
%\includegraphics[scale=.3]{img/sim/Ysorted001.png}
%\includegraphics[scale=.3]{img/sim/Ysorted002.png}
%\includegraphics[scale=.3]{img/sim/Ysorted003.png} 
%\caption{Heatmaps of simulated $y_{inj}$ with cells in rows and markers in columns.  The values are sorted according to their $\lambda_{in}^\true$.} %  The simulated marker expression data $y$. The upper panel contains
%data for sample 1 (left), sample 2 (middle), and sample 3 (right). The lower
%panel is the same data but with the rows (cells) grouped according to the true
%cell-types ($\lambda$).}
%\label{fig:sim-Y}
%\endmyfig
%reference this figure like so: \ref{fig:sim-Y}




%The steps for simulating data is as follows.
%First specify dimensions of the data through $I$ (number of samples), $J$
%(number of markers), and $N_i$ (number of cells in sample $i$). Specify the
%dimensions of the parameters through $K$ (true number of latent cell-types),
%$L^0$ (number of mixture components in density for cells not expressing a
%marker), and $L^1$ (number of mixture components in density for cells
%expressing a marker). Then fix the latent cell-type matrix $\bZ^\true$
%according to $J$ and $K$. Set values for $\sigma^{2, \true}_{0i\ell}$,
%$\sigma^{2,\true}_{1i\ell}$, $\mu^{\star, \true}_{0\ell}$ and $\mu^{\star,
%\true}_{1\ell}$.  This can be done using empirical values from CB data.
%Simulate $\bw^\true_i$ from a Dirichlet distribution with parameters $a_1,
%\ldots, a_K$. This ensures that $\sum_{k=1}^K \bw^\true_i = 1$.
%Similarly, $\eta^\true_{0ij}$ and $\eta^\true_{1ij}$ can be simulated
%from Dirichlet distributions. Note that $\eta^\true_{0ij}$ is an
%$L^0$-dimensional vector, while $\eta^\true_{0ij}$ is an $L^1$-dimensional
%vector. Using these parameters, simulate $y_{inj}$ according to the sampling
%density. Finally, probabilistically set some of the $y_{inj}$ to be missing.
%This can be done by first predetermining a certain percentage ($p\%$) of the
%data to be missing. This percentage should should be less than $\sum_k
%w^\true_{ik}(1-z^\true_{jk})$ so that truly expressed cells are unlikely to be
%missing, and so as to be consistent with the simulated $W^\true$ and
%$\bZ^\true$. Sample ($N_i\times p\%$) observations of $y_{inj}$ without
%replacement with the probability of missing for $y$ being proportional to some
%predetermined missing mechanism (possibly different from the one used in this
%model). Care should be taken to ensure $y$ with positive values have almost
%zero probability of being missing.

%\textbf{Simulated $\bZ^\true$ and $W^\true$}

%Following this scheme, we generated data comparable to the CB data.  In our
%simulated data we assume 3 samples, each having $J=32$ markers, and each sample
%containing tens of thousands of cells. This size resembles that of the CB data.
%We then simulated $\bZ^\true$ and $W^\true$ with number of columns being $K=10$
%to include a variety of cell-types. Figure \ref{fig:sim-Z} shows the simulated
%$\bZ^\true$ with the latent features sorted by $W_i$ for each sample. 


%The data $\y$ are shown in Figure \ref{fig:sim-Y}. The number of cells in each
%sample are $N=(30000, 20000, 10000)$. Note that in the upper panel contains
%marker expression data for sample 1 (left), sample 2 (middle), and sample 3
%(right). The lower panel contains the same data but with the rows grouped by
%the true cell-types ($\lambda$).  Red regions represent high expression levels,
%whereas blue regions represent low expression levels. Thus, through this
%visualization, we can observe the true latent cell-types that are generating
%the data. Note that in this simulated data set, the white regions represent
%missing data.  Three and four mixture components were used for non-expressed
%and expressed marker expression levels respectively. 


%\subsubsection{Results}
To fit the proposed model, we fix the hyperparameters as
%
{\tt include details. also explain how you specified missing mechanism empirically Value for $K$ used for fitting? values of $L^0$ and $L^1$ used for fitting??}.
%
We fixed $\Gamma=\bI_J$. $J\times J$ identity matrix, and $\beta_{1i}$ for
simplification. To run the MCMC simulation, we initialized the parameters
%
{\tt explain how}.
%
We then implemented posterior inference using MCMC simulation
over 3,000 iterations, discarding the first 1,000 iterations as burn-in.
%
{\tt no thinning?}  
%


Figure \ref{fig:sim-post-Z} summarizes the posterior inference for the simulated data.  The posterior point estimates $\hat{\bZ}_i$ and $\hat{\bw}_i$ are obtained using the method described in \S~\ref{sampling-via-mcmc}.  
%
%provides
%representation of the data $y$ sorted by a point-estimate of their cell-types,
%for each sample.  These figures are supplemented by a point-estimate of $\Z$
%and $W$. 
The bottom of each panel illustrates $\hat{\bZ}_i$ with $\hat{\bw}_i$ in percentages on the right side. Among $K=XX$ (give number) phenotypes, phenotypes having large $\hat{w}_{ik}$ that make up more than 90\% of cells are included in the plots of $\hat{\bZ}_i$.    Comparing their truth in Figure~\ref{fig:sim-Z}, $\hat{\bZ}_i$ and $\hat{\bw}_i$ are very close to $\bZ^\true$ and $\bw_i^\true$ for all sample. Note that the phenotype labels do not match in the figures due to the fact that the model for $\bZ$ is invariant under relabelling of the phenotyps. For example, phenotype 1 in $\bZ^\true$ has $w^\true_{ik}=23.6\%$ for sample 1.  That phenotype is shown as phenotype 8 in the very bottom of $\hat{\bZ}_i$ with $\hat{w}_{ik}=24\%$.   %Phenotypes with small $w^\true_{ik}$ are not well captured in $\hat{\bZ}_i$.  For example, cell type 7 ($k=7$) with $w^\true_{1k}=4.5\%$ is not well identified.  On the other hand, it is well captured for sample 2 since $w^\true_{27}=14.8\%$ in the truth. 
The heatmaps of observed $y$ arranged according to $\hat{\lambda}_{in}$ shows that the expression patterns in $y$ are well explained by $\hat{\bZ}_i$ and $\hat{\bw}_i$.  The top of each panel has a heatmap of $y$ rearranged by their $\hat{\lambda}_{in}$, with red, blue and white colors for large, small and missing values, respectively.  The horizontal dotted lines separate cells based on $\hat{\lambda}_{in}$.  It shows that the estimated phenotypes capture the expression patterns of $y$ well.    

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[th!]
  \begin{center}
\begin{tabular}{cc}
\includegraphics[scale=.48]{img/sim/YZ001.png}&
\includegraphics[scale=.48]{img/sim/YZ002.png}\\
(a) Sample 1 & (b) Sample 2\\
\includegraphics[scale=.48]{img/sim/YZ003.png}&\\
(c) Sample 3 & \\
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{[Simulation]  Heatmaps of $y$ for simulated data. Cells and markers are in rows and columns, respectively. Each column contains the expression levels of a
marker for all cells in the sample. High expression levels are red, low expression levels are blue, missing values are white.   Cells are rearranged by their posterior estimate of phenotype indicator, $\hat{\lambda}_{in}$.   Horizontal lines separate cells in different estimated phenotypes.
%
{\tt make it more visible}
%
At the bottom of each panel, the transpose of $\hat{\Z}_i$ and $\hat{\bw}_i$ are provided for each sample. We include phenotypes having large $\hat{w}_{ik}$ to explain at least 90\% of the cells in a sample.
%
{\tt make the figures more visible.  the resolution seems not high enough
especially for $\bw$}
%
}
\label{fig:sim-post-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%



%vary only rarely by
%small permutations in less common cell-types. The cell-types in $\Z$ are sorted
%by a point-estimate for $W_i$.  These also resemble $W_i^\true$ but vary
%occasionally from the truth for smaller values of $W_i$. Only the cell-types
%that make up the top 90\% of cells are included in Figure \ref{fig:sim-post-Z}.


%\textbf{Posterior Estimate for $\bZ$ and $W$}
%\beginmyfig
%\includegraphics[scale=.3]{img/sim/YZ001.png}
%\includegraphics[scale=.3]{img/sim/YZ002.png}
%\includegraphics[scale=.3]{img/sim/YZ003.png}
%\caption{$y$ sorted by a point-estimate of their cell-types ($\lambda$), for
%each sample.  Point-estimates of $\Z$ (sorted by $W_i$) and $W_i$ are provided
%for each sample below each $y_i$.}
%\label{fig:sim-post-Z}
%\endmyfig

{\tt 
\begin{itemize}
\item as we discussed, include posterior prob that a maker is not expressed if it is missing and posterior distributions of $y_{inj}$ for observed ones for each $(i,j)$.

\item comparison of posterior predictive distribution to density estimates of observed $y$.

\item comparison

\item sensitivity analysis ($K$ larger than $K^\true$ and smaller than $K^\true$.

\end{itemize}
}



%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
  \begin{center}
\begin{tabular}{cc}
\includegraphics[scale=.25]{img/cb/YZ001.png}&
\includegraphics[scale=.25]{img/cb/YZ002.png}\\
(a) Sample 1 & (b) Sample 2 \\
\includegraphics[scale=.25]{img/cb/YZ003.png} &\\
(c) Sample 3 & \\
  \end{tabular}
 \end{center}
 \vspace{-0.05in}
\caption{[CD Data]  Heatmaps of $y$ for the samples. Cells and markers are in
rows and columns, respectively. Each column contains the expression levels of
a marker for all cells in the sample. High expression levels are red, low
expression levels are blue, missing values are white.   Cells are rearranged
by their posterior estimate of phenotype indicator, $\hat{\lambda}_{in}$.
Horizontal lines separate cells in different estimated phenotypes.
%
{\tt make it more visible}
%
At the bottom of each panel, the transpose of $\hat{\Z}_i$
and $\hat{\bw}_i$ are provided for each sample. We include phenotypes having
large $\hat{w}_{ik}$ to explain at least 90\% of the cells in a sample.}
\label{fig:cb-post-Z}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%

%%% cb-proj1 %%%
\subsection{Cord Blood Data}
We fit the proposed model to a real data set comprising
cord-blood samples from three patients ($I=3$), with sample sizes $N=(41474,
10454, 5177)$, for $J=32$ markers. The heatmaps in Figure \ref{fig:cb-post-Z} illustrates heatmaps of observed expression levels $y_{inj}$.  Markers and cells are in columns and rows, respectively. The values are rescaled for illustration.  Red, blue and white colors represent high expression level, low expression level and missing values, respectively.  From the figure, expression levels of some markers are missing in most of cells, e.g, marker CD25 (column 9) in all samples.
%
{\tt give a short summary about \% of missing values for $(i,j)$}
%
From the provided information on the missingness mechanism, the marker is likely not to be expressed in most of NK cell phenotypes.   
%shows the data. 
%Each panel in the figure represents a different sample.  Each column in the
%panels contains the expression levels of a marker across all cells in the
%sample.  High expression levels are red, low expression levels are blue,
%and missing values are white.


We used hyperparameters similar to those in the simulation studies.
%
{\tt used similar hyperparameters?  check if this is right}
%
2000 samples from the posterior distribution were obtained after a burn-in period of 1000 iterations. The results are summarized in Figure
\ref{fig:cb-post-Z}.  Point estimates $\hat{\bZ}_i$ and $\hat{\bw}_i$ are obtained using the method in \S~\ref{sampling-via-mcmc}. % illustrated at the bottom of each panel.   
Plots of $\hat{\bZ}_i$ are given with corresponding $\hat{\bw}_i$ at the bottom
of the panels.  From the estimated weights $\hat{\bw}_i$ the samples have some
common phenotypes such as phenotypes 17, 15 and 18.  In particular, phenotype
17 is the most abundant pheontype in all samples, 28.4\%, 23.8\% and 30.2\% for
samples 1, 2 and 3, respectively.  $\hat{\bw}_i$ also shows heterogeneity
across samples.  For example, phenotype 7 has the second largest $\hat{w}_i$ in
samples 2 and 3. Specifically, 22.4\% and 13.5\% of the cells in samples 2 and
3, respectively possess phenotype 7, but only {\tt XXX  Give a number here} \%
of the cells in sample 1 has the phenotype.  Similarly, phenotype 18 is more
prevalent in sample 1.  $\hat{w}_{ik}$ for the phenotype is 17.4\%, 4.7\%,
3.0\% in samples 1, 2 and 3, respectively.  Phenotypes 7 and 18 are different
dominantly for expression of markers 17, 18 and 19 corresponding to EOMES, GrA
(granzyme A) and GrB (granzyme B).  Also, note that some phenotypes are very
similar, for example, phenotypes 2 and 18 in panel (a).  It may be because
independence across columns under the prior model for $\bZ$ allows identical
columns with positive probability.  For the heatmaps on the top of the panels,
$y_{inj}$ are sorted by a posterior estimate $\hat{\lambda}_{in}$ of their cell
phenotypes. The horizontal dotted lines separate cells using
$\hat{\lambda}_{in}$.  From the heatmaps, the cells having a phenotype have
similar expression patterns, implying that the model provides reasonable
estimates of underlying cell subpopluations.

%does k
%But it should be noted that
%the estimated $\bZ$ matrix for each sample contains different cell-types.
%Moreover, within each sample, the cell-types may be repeated or vary by only
%one marker. We would like the learned cell-types to be different rather than
%very similar (or repeated). We can impose this constraint using a repulsive
%mixture prior \citep{petralia2012repulsive} for the cell-types. This serves as
%the motivation for Project 2, where we impose more structure in the prior 
%specification of the latent feature matrix $\bZ$ so that similar cell-types
%appearing in the matrix is discouraged, while distinctness is encouraged.

%Finally, it can be seen that the different samples consist of different
%sub-populations of NK-cells though some cell-types are shared across samples.
%Hence, we see that information can be borrowed across multiple samples to
%discover common cell-types across samples.

%\textbf{Assessing Model Fit}

To assess model fit, we compare the posterior predictive distribution of
$y_{inj}$ with $m_{inj}=0$ (observed) to the distribution of observed data.
Figure XX illustrates comparison of posterior predictive distributions (grey
lines) to kernel density estimates (blue lines) of observed $y_{inj}$ for some
selected ($i,j)$. The fit is  reasonably good for (... ) in panels (a)-(c).
For (...), the fit is poor possibly because the number of observations is small
{\tt ??? is this what you meant? or something different??? } $y$ is missing for
most of cells.  The plot also shows a posterior probability that a marker is
expressed for phenotypes possessed by cells in a sample, $\hat{q}_1=\Prob(z_{j
\lambda_{in}}=1 \mid \by, \bf m)$ for each ($i,j)$. When observed $y_{inj}$ is
larger for many cells, $\hat{q}_1$ is expected to be large.  We also compute
$\hat{q}_2=\Prob(z_{j \lambda_{in}}=0 \mid \by, \bf m)$ for $(i,n,j)$ with
$m_{inj}=1$ (missing), posterior probability that a cell with missing value for
marker $j$ has a phenotype for which the marker is not expressed.  Figure~XXX
illustrates a histogram of $\hat{q}_2$ for all $(i, n, j)$.  Recall that the
assumption made for the missingness mechanism is non-testable.   The figure
shows that when a cell has a missing value for a marker, the marker is not
expressed in the phenotype that the cell possesses, which complies with the
subject knowledge provided by biologists.  



{\tt 
\begin{itemize}
\item make plots as described above.
\item comparison??
\end{itemize} }

%We check the posterior probabilities that a 

% if the distribution is tipped more to the positive side, and close to zero otherwise.


%At the top of each sub-figure, the sample ($i$) and marker ($j$) are listed.
%The statistic $Z_ij$ mean refers to the posterior mean proportion of cells
%that express marker $j$ in sample $i$. We expect this statistic to be close 
%to one if the distribution is tipped more to the positive side, and close to
%zero otherwise. 


%The fit is mostly good but can be poor when the number of
%observations is small. The figures are annotated with a statistic $\hat
%P(Z=0\mid m=1, \text{data})$, which is the posterior mean probability that an
%observation recorded as missing will not express a marker. We expect this
%quantity to be close to 1. This is generally the case, except for samples and
%markers where there are only a few missing values and most of the expression
%levels are high.


%The blue and grey lines represent the posterior predictive distribution and kernel density estiamtes  the observed data density
%for $y>0$.



% Since the data contains missing values, we compare
%the posterior predictive density to the observed data augmented with the
%posterior mean of the imputed data for each sample and marker.  We have
%included these figures in Appendix \ref{sec:cb-pp}. 
%At the top of each sub-figure, the sample ($i$) and marker ($j$) are listed.
%The statistic $Z_ij$ mean refers to the posterior mean proportion of cells
%that express marker $j$ in sample $i$. We expect this statistic to be close 
%to one if the distribution is tipped more to the positive side, and close to
%zero otherwise.  Within the figures, the thick grey line is the posterior
%predictive density, and the red line is the data augmented with the posterior
%mean of the imputed values.  The thin grey line is the observed data augmented
%with one sample from the posterior distribution of the imputed values.

%Another way to assess model fit is through comparing the observed data
%distribution to the posterior predictive distribution of non-missing data.
%This requires storing $\gamma_{inj}$ at each iteration of the MCMC and is quite
%expensive storage-wise. A compromise is to compare the observed data having
%positive values to the positive values in the posterior predictive
%distribution.  This comparison is provided in Appendix
%\ref{sec:cb-pp-positive}. The blue line represents the posterior predictive 
%distribution for $y>0$, and the grey line represents the observed data density
%for $y>0$. The fit is mostly good but can be poor when the number of
%observations is small. The figures are annotated with a statistic $\hat
%P(Z=0\mid m=1, \text{data})$, which is the posterior mean probability that an
%observation recorded as missing will not express a marker. We expect this
%quantity to be close to 1. This is generally the case, except for samples and
%markers where there are only a few missing values and most of the expression
%levels are high.


\subsection{Conclusions}

{\tt expand}

We have proposed a Bayesian feature allocation model for studying NK-cell
diversity from mass cytometry data. We used a simulation study to show that our
model is able to recover the true latent feature allocation matrix generating
the observed data. Through providing an informed prior distribution on the
missing mechanism, we are able to reasonably impute missing values to obtain a
full dataset
%
{\tt did not show specifically this point}.
%
When applied to real cord blood data, we find that the
cell-types learned tend to be similar, and sometimes repeated. This can be
remedied using a repulsive mixture prior and will be included in Project 2.


\section{Project 2: Repulsive Feature Allocation Model}\label{sec:proj2}

\section{Project 3: Covariate-Dependent Feature Allocation Model}\label{sec:proj3}
%Let $\bx_i$ a vector of covariates of subject $i$ and $t$ time after receiving
%a treatment.  We extend the model by letting $\bw_i$ a function of $\bx_i$ and
%$t$ and jointly analyze samples of subjects collected at different time
%points. While all samples share $\bZ$ (common cell phenotypes), phenotype
%abundances $\bw(\bx_i, t)$ are sample specific and depend on covariates and
%time points.  We also let $w_k(\bx_i, t)$ exactly equal to zero for some
%phenotypes. 

We extend the model by letting phenotype abundances $\bw=(w_1, \ldots, w_K)$ a
function of time $t$ after treatment and jointly analyze samples of a subject
collected at different time points. While all samples share $\bZ$ (common cell
phenotypes), phenotype abundances $\bw_{t_i, k}$, $i=1, \ldots, I$ are
dependent through time.  We also let $w_{t_i,k}$ exactly equal to zero for some
phenotypes. Let $\xi_{t_1,1}=1$ and assume $\xi_{t_1,k} =
\max(\xi^\prime_{t_1,k}, 0)$, $k=1, \ldots, K$, where $\xi^\prime_{t_1,k} \iid
\N(0, s^2_1)$. We then normalize $w^\prime_{t_1,k}$,  $w_{t_1,k} =
\xi_{t_1,k}/\sum_{k^\prime=1}^K \xi_{t_1, k^\prime}$, $k=1, \ldots, K$. For
time points $t_i$, $i=2, \ldots, t_I$, we assume $\xi_{t_i,k} =
\max(\xi^\prime_{t_i,k}, 0)$, $k, \ldots, K$ with $\xi^\prime_{t_i,k} =
f_k(t_i)$ and let $w_{t_i,k} = \xi_{t_i,k}/\sum_{k^\prime=1}^K
\xi_{t_i,k^\prime}$ similar to $w_{t_1,k}$.  For example, a quadratic function
in time $t$, $f_k(t) = \xi_{t_1,k} + \beta_{k1}t + \beta_{k2}t^2$ can be used.
$\xi_{t_1,k}$ is the baseline unnormalized baseline abundance for phenotype $k$
and ($\beta_{k1}, \beta_{k2}$) explain change in unnormalized abundance over
time. Alternatively, we can let $\xi^\prime_{t_i,k} = \xi^\prime_{t_{i-1},k} +
g_{t_i,k}$ where $g_{t_i, k} \iid \N(0, s^2)$.




\section{Timeline}\label{sec:time}



\bibliography{litreview.bib}


\appendix
\section{Posterior Computation for Project 1}
%{\tt Please write a short intro about the posterior sampling and reorganize the
%full conditionals }
This section presents detailed derivations of the full conditional
distributions for each model parameter. Using traditional Markov
chain Monte Carlo (MCMC), we can sample from the joint posterior 
distribution of the parameters.

To sample from a distribution which is otherwise difficult to sample
from, the Metropolis-Hastings algorithm can be used. This is
particularly useful when sampling from a full conditional distribution
of one of many parameters in an MCMC based sampling scheme (such as a
Gibbs sampler). Say \(B\) samples from a distribution with density
\(p(\theta)\) is desired, one can do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide an initial value for the sampler, e.g. \(\theta^{(0)}\).
\item
  Repeat the following steps for \(i = 1,...,B\).
\item
  Sample a new value \(\tilde\theta\) for \(\theta^{(i)}\) from a
  proposal distribution \(Q(\cdot \mid \theta^{(i-1)})\).

  \begin{itemize}
  \tightlist
  \item
    Let \(q(\tilde\theta \mid \theta)\) be the density of the proposal
    distribution.
  \end{itemize}
\item
  Compute the ``acceptance ratio'' to be \[
     \rho=
     \min\bc{1, \frac{p(\tilde\theta)}{p(\theta^{(i-1)})} \Big/ 
            \frac{q(\tilde\theta\mid\theta^{(i-1)})}
                 {q(\theta^{(i-1)}\mid\tilde\theta)}
        }
     \]
\item
  Set \[
     \theta^{(i)} := 
     \begin{cases}
     \tilde\theta &\text{with probability } \rho \\
     \theta^{(i-1)} &\text{with probability } 1-\rho.
     \end{cases}
     \]
\end{enumerate}

Note that in the case of a symmetric proposal distribution, the
acceptance ratio simplifies further to be
\(\frac{p(\tilde\theta)}{p(\theta^{(i-1)})}\).

The proposal distribution should be chosen to have the same support as
the parameter. Transforming parameters to have infinite support can,
therefore, be convenient as a Normal proposal distribution can be used.
Moreover, as previously mentioned, the use of symmetric proposal
distributions (such as the Normal distribution) can simplify the
computation of the acceptance ratio.

Some common parameter transformations are therefore presented here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For parameters bounded between \((0,1)\), a logit-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has support in the unit interval, then
  \(Y=\logit(X)=\log\p{\frac{p}{1-p}}\) will have density
  \(f_Y(y) = f_X\p{\frac{1}{1+\exp(-y)}}\frac{e^{-y}}{(1+e^{-y})^{2}}\).
\item
  For parameters with support \((0,\infty)\), a log-transformation may
  be used. Specifically, if a random variable \(X\) with density
  \(f_X(x)\) has positive support, then \(Y = \log(X)\) has pdf
  \(f_Y(y) = f_X(e^y) e^y\).
\end{enumerate}


%{\tt instead of subsections, we may do ``itemize'' for full conditionals.  I
%commented out the full conditionals temporarily since it gives some errors in
%compiling.  Please comment in as you revise. }

\vspace{5em}
\hrule
\vspace{5em}

% fc-v
\textbf{Full Conditional for $\bm v$}

The prior distribution for \(v_k\) are
\(v_k \mid \alpha \ind \Be(\alpha/K, 1)\), for \(k = 1,...,K\). So,
\(p(v_k \mid \alpha) = \alpha v_k^{\alpha/K-1}\).

Let \(S = \bc{(i,n)\colon \lin = k}\).

\begin{align*}
p(v_k \mid \y, \rest) &\propto p(v_k) \prod_{j=1}^J\prod_{(i,n)\in S} p(\y \mid v_k, \rest) \\
&\propto (v_k)^{\alpha/K-1} \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhLogitSpiel{v_k}{\xi}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(v_k\), and should be computed accordingly. Moreover, we
will only recompute the likelihood (in the metropolis acceptance ratio)
when \(Z_{jk}\) becomes different.
\vspace{2em}


% fc-alpha
\textbf{Full Conditional for $\alpha$}

\begin{align*}
p(\alpha \mid \y, \rest) &\propto p(\alpha) \times \prod_{k=1}^K p(v_k \mid \alpha) \\
&\propto \alpha^{a_\alpha - 1} \exp\bc{-b_\alpha \alpha} \times \prod_{k=1}^K 
\alpha~v_k^{\alpha/K} \\
&\propto \alpha^{a_\alpha + K -1} \exp\bc{-\alpha\p{b_\alpha - 
\frac{\sum_{k=1}^K \log v_k}{K}}}
\end{align*}

\[
\therefore \alpha \mid \y, \rest \sim 
\G\p{a_\alpha + K,~ b_\alpha - \frac{\sum_{k=1}^K \log v_k}{K}}
\]
\vspace{2em}


% fc-H
\textbf{Full Conditional for $\bm H$}

The prior for \(\h_k\) is \(\h_k \sim \N_J(0, \Gamma)\). We can
analytically compute the conditional distribution
\(h_{j,k} \mid \h_{-j,k}\), which is

\[
h_{jk}  \mid \h_{-j,k} \sim \N(m_j, S^2_j),
\]

where

\[
\begin{cases}
m_j &= \bm G_{j,-j} \bm G_{-j,-j}^{-1}(\h_{-j,k})\\
S_j^2 &= \bm G_{j,j} - \bm G_{j,-j}\bm G_{-j,-j}^{-1}\bm G_{-j,j}\\
\end{cases}
\]

and the notation \(\h_{-j,k}\) refers to the vector \(h_k\) excluding
the \(j^{th}\) element. Likewise, \(\bm G_{-j,k}\) refers to the
\(k^{th}\) column of the matrix \(\bm G\) excluding the \(j^{th}\) row.

Note that if \(\bm G = \I_J\), then \(m_j=0\) and \(S_j^2 = 1\). Let
\(S = \bc{(i,n)\colon \lin=k}\).

\begin{align*}
p(h_{jk} \mid \y, \rest)  &\propto p(h_{jk}) \prod_{(i,n) \in S} p(y_{inj} \mid h_{jk}, \rest) \\
%
&\propto
\exp\bc{\frac{-(h_{jk} - m_j)^2}{2S_j^2}}
 \prod_{j=1}^J \prod_{(i,n)\in S}
\sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
\N(y_{inj} \mid \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
\end{align*}

\mhSpiel{h_{jk}}

Note also that \(\mus_{Z_{jk}\ell}\) and \(\sss_{Z_{jk}i\ell}\) are
functions of \(h_{jk}\), and should be computed accordingly. Moreover,
we will only recompute the likelihood (in the metropolis acceptance
ratio) when \(Z_{jk}\) becomes different.  
\vspace{2em}


% fc-lam
\textbf{Full Conditional for $\bm \lambda$}

The prior for \(\lin\) is \(p(\lin = k \mid \bm W_i) = W_{ik}\).

\begin{align*}
p(\lin=k\mid \y,\rest) &\propto p(\lin=k) ~ p(\y \mid \lin=k, \rest) \\
&\propto W_{ik}
\prod_{j=1}^J 
\p{
  \sum_{\ell=1}^{L^{Z_{jk}}} \eta^{Z_{jk}}_{ij\ell} \cdot
  \N(y_{inj} \mid 
  \mus_{Z_{jk}\ell}, \sss_{Z_{jk}i\ell})
}\\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(k = 1,...,K\). Moreover, since \(k\) is discrete, a Gibbs update can
be done on \(\lin\).

\textbf{Full Conditional for $\bm W$}

The prior for \(\bm{W}_i\) is \(\bm W_i \sim \Dir(d, \cdots, d)\). So
the full conditional for \(\bm{W}_i\) is:

\begin{align*}
p(\bm W_i \mid \rest) \propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i} p(\lin \mid \bm{W}_i)\\
\propto&~~ p(\bm{W}_i) \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\Ind{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{d/K-1} \times \prod_{n=1}^{N_i}\prod_{k=1}^K W_{ik}^{\Ind{\lin=k}}\\
\propto&~~ \prod_{k=1}^K W_{ik}^{\p{d/K + \sum_{n=1}^{N_i}\Ind{\lin=k}}-1}\\
%
\end{align*}

Therefore, \[
\bm{W}_i \mid \y,\rest ~\sim~ \Dir\p{d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=1},...,d/K+\sum_{n=1}^{N_i}\Ind{\lambda_{i,n}=K}} 
\]

Consequently, the full conditional for \(\bm{W}_i\) can be sampled from
directly from a Dirichlet distribution of the form above.
\vspace{2em}


% fc-mu
\textbf{Full Conditional for $\bm\mu^\star$}

For \(\mus_{0\ell}\)g, let
\(S_{0i\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 0 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{0i\ell}|\) the cardinality of \(S_{0i\ell}\).

\newcommand\musZeroPostvarDenom{
  \frac{1}{\tau^2_0} + \sum_{i=1}^I\frac{|S_{0i\ell}|}{{\sigma^2}^\star_{0i\ell}}
}
\newcommand\musZeroPostMeanNum{
  \frac{\psi_0}{\tau^2_0} + 
  \sum_{i=1}^I \sum_{S_{0i\ell}}  
  \frac{y_{inj}}{{\sigma^2}^\star_{0i\ell}}
}

\begin{align*}
p(\mus_{0\ell} \mid \y, \rest) &\propto 
p(\mus_{0\ell} \mid \psi_0, \tau^2_0) \times p(\y \mid \mus_{0\ell},\rest) \\
%
&\propto
\Ind{\mus_{0\ell}<0} \exp\bc{\frac{-(\mus_{0\ell} - \psi_0)^2}{2\tau^2_{0}}}
\prod_{i=1}^I\prod_{(i,n,j)\in S_{0i\ell}} \exp\bc{\frac{-(y_{inj} - \mus_{0\ell})^2}{2{\sigma^2}^\star_{i0\ell}}} \\
%
&\propto
\exp\bc{
  -\frac{(\mus_{0\ell})^2}{2}\p{\musZeroPostvarDenom} + 
  \mus_{0\ell}\p{\musZeroPostMeanNum}
} \\ 
& ~~~ \times \Ind{\mus_{0i\ell}<0} \\
\end{align*}

\[
\renewcommand\musZeroPostvarDenom{
  1 + \tau^2_0\sum_{i=1}^I(|S_{0i\ell}|/{\sigma^2}^\star_{0i\ell})
}
\renewcommand\musZeroPostMeanNum{
  \psi_0 + \tau^2_0 \sum_{i=1}^I\sum_{S_{0i\ell}} (y_{inj} / {\sigma^2}^\star_{0i\ell})
}
\therefore \mus_{0l} \mid \y, \rest \ind \N_-\p{
  \frac{\musZeroPostMeanNum}{\musZeroPostvarDenom},
  \frac{\tau^2_0}{\musZeroPostvarDenom}
}
\]

Similarly for \(\mus_{1\ell}\)g, let
\(S_{1\ell} = \bc{(i,n,j) : \p{Z_{j,\lin} = 1 ~\cap~ \gamma_{inj} = \ell}}\)g
and \(|S_{1i\ell}|\) the cardinality of \(S_{1i\ell}\).

\[
\newcommand\musOnePostvarDenom{
  1 + \tau^2_1 \sum_{i=1}^I (|S_{1i\ell}|/{\sigma^2}^\star_{1i\ell})
}
\newcommand\musOnePostMeanNum{
  \psi_1 + \tau^2_1 \sum_{i=1}^I \sum_{S_{1i\ell}} (y_{inj} / {\sigma^2}^\star_{1i\ell})
}
\therefore \mus_{1l} \mid \y, \rest \ind \N_+\p{
  \frac{\musOnePostMeanNum}{\musOnePostvarDenom},
  \frac{\tau^2_1}{\musOnePostvarDenom}
}
\]
\vspace{2em}


% fc-sig2
\textbf{Full Conditional for $\bm{{\sigma^2}}^*$}

Let
\(S_{0i\ell} = \bc{(i, n,j): Z_{j,\lin} = 0 ~\cap~ \gamma_{inj}=\ell}\),
\(i=1, \ldots, I\).

\begin{align*}
p(\sss_{0i\ell} \mid \y, \rest) &\propto p(\sss_{0i\ell} \mid s_i) \times p(\y \mid \sss_{0i\ell}, \rest) \\
&\propto (\sss_{0i\ell})^{-a_\sigma-1} \exp\bc{-\frac{s_i}{\sss_{0i\ell}}} 
\prod_{(i,n,j)\in S_{0i\ell}} \bc{
  \frac{1}{\sqrt{2\sss_{0i\ell}}}
  \exp\bc{\frac{-(y_{inj}-\mus_{0\ell})^2}{2\sss_{0i\ell}}}
} \\
&\propto (\sss_{0i\ell})^{-(a_\sigma + \frac{\abs{S_{0i\ell}}}{2})-1}
\exp\bc{\p{\frac{1}{\sss_{0i\ell}}}\p{s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}}.
\end{align*}

\[
\therefore \sss_{0i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{0i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{0i\ell}}
\frac{(y_{inj}-\mus_{0\ell})^2}{2}
}.
\]

Similarly, let
\(S_{1i\ell} = \bc{(i, n,j): Z_{j,\lin} = 1 ~\cap~ \gamma_{inj}=\ell}\).
Then, the full conditional for \(\sss_{1i\ell}\) is \[
\therefore \sss_{1i\ell} \mid \y, \rest \ind
\IG\p{a_\sigma + \frac{\abs{S_{1i\ell}}}{2}, ~~ s_i + \sum_{(i,n,j)\in S_{1i\ell}}
\frac{(y_{inj}-\mus_{1\ell})^2}{2}
}.
\]
\vspace{2em}


% fc-s
\textbf{Full Conditional for $s_i$}

\begin{align*}
p(s_i \mid \y, \rest) &\propto p(s_i) \times \prod_{z=0}^1 \prod_{\ell=1}^{L^z} p(\sss_{zi\ell} \mid s_i)\\
&\propto s_i^{a_s-1} \exp\bc{-b_s s_i} \times \prod_{z=0}^1  \prod_{\ell=1}^{L^z} s_i^{a_\sigma} \exp\bc{-s_i / \sss_{zi\ell}} \\
&\propto s_i^{a_s + (L^0 + L^1)a_\sigma - 1} \exp\bc{-s_i \p{b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} 1 / \sss_{zi\ell}}}.
\end{align*}

\[
\therefore s_i \mid \y, \rest \sim 
\G\p{a_s + (L^0 + L^1)a_\sigma, ~~ b_s + \sum_{z=0}^1 \sum_{\ell=1}^{L^z} \frac{1}{\sss_{zi\ell}} }.
\]
\vspace{2em}


% fc-gam
\textbf{Full Conditional for $\bm\gamma$}

The prior for \(\gamma_{inj}\) is
\(p(\gamma_{inj} = \ell \mid Z_{j\lin}=z, \eta^z_{ij\ell}) = \eta^z_{ij\ell}\),
where \(\ell \in \bc{1,...,L^z}\).

\begin{align*}
p(\gamma_{inj}=\ell \mid \y, Z_{j\lin}=z, \rest) &\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \gamma_{inj}=\ell, \rest) \\
&\propto p(\gamma_{inj}=\ell) \times p(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}, \rest) \\
%
&\propto \eta^z_{ij\ell} \times \N(y_{inj} \mid \mus_{z\ell}, \sss_{zi\ell}) \\
&\propto \eta^z_{ij\ell} \times (\sss_{zi\ell})^{-1/2}
\exp\bc{-\frac{(y_{inj} - \mus_{z\ell})^2}{2\sss_{zi\ell}}} \\
\end{align*}

The normalizing constant is obtained by summing the last expression over
\(\ell = 1,...,L^z\). Moreover, since \(\ell\) is discrete, a Gibbs
update can be done on \(\gamma_{inj}\).
\vspace{2em}


% fc-eta
\textbf{Full Conditional for $\bm\eta$}

The prior for \(\bm\eta^z_{ij}\) is
\(\bm \eta^z_{ij} \sim \Dir_{L^z}(a_{\eta^z})\), for \(z\in\bc{0,1}\).
So the full conditional for \(\bm\eta^z_{ij}\) is:

\begin{align*}
p(\bm \eta^z_{ij} \mid \rest) \propto&~~ p(\bm{\eta}^z_{ij}) \times \prod_{n=1}^{N_i} p(\gamma_{inj} \mid \bm \eta^z_{ij})\\
\propto&~~ p(\bm \eta^z_{ij}) \times \prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
%
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{a_{\eta^z}/L^z-1} \times 
\prod_{n=1}^{N_i}\prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}}\\
\propto&~~ \prod_{\ell=1}^{L^z} \p{\eta^z_{ij\ell}}^{\p{a_{\eta^z} / L^z + \sum_{n=1}^{N_i} \Ind{ \gamma_{inj}=\ell ~\cap~ Z_{j\lin=z}}} - 1}\\
%
\end{align*}

Therefore, \[
\bm{\eta}^z_{ij} \mid \y,\rest ~\sim~ \Dir_{L^z}\p{a^*_1,...,a^*_{L^z}}
\] where
\(a^*_\ell = a_{\eta^z}/L^z+\sum_{n=1}^{N_i}\Ind{\gamma_{inj}=\ell ~\cap~ Z_{j\lin}=z}\).
Consequently, the full conditional for \(\bm{\eta}^z_{ij}\) can be
sampled from directly from a Dirichlet distribution of the form above.
\vspace{2em}


% fc-beta
\textbf{Full Conditional for $\bm\beta$}

Define \(f_{inj}\) to be

\begin{align*}
f_{inj} &:= P(m_{inj} \mid p_{inj}, y_{inj}) \\
&= p_{inj}^{m_{inj}} (1-p_{inj})^{1 - m_{inj}} \\
&= \left(\frac{1}{1+e^{-x_{inj}}} \right)^{m_{inj}}\left(\frac{1}{1+e^{x_{inj}}} \right)^{1-m_{inj}},
\end{align*}

where

\begin{align*}
  x_{inj} &:= \begin{cases}
  \beta_{0i} - \beta_{1i}(y_{inj}-c_0)^2, & \text{if } y_{inj} < c_0\nonumber \\
  \beta_{0i} - \beta_{1i}c_1\sqrt{y_{inj}-c_0}, & \text{otherwise}, \nonumber \\
  \end{cases}\\
\end{align*}

where \(c_0\) and \(c_1\) are real constant, $\beta_{0i} \in \mathbb{R}$, and
$\beta_{1i} > 0$.
One way to determine a prior for the missing mechanism is to first select three
points to constrain the missing mechanism $(c_\text{low}, p_\text{low})$,
$(c_0, p_0)$, and $(c_\text{high}, p_\text{high})$, and then 
solve for $\beta_0$, $\beta_1$, and $c_1$. Figure \ref{fig:prob-miss-eg} shows
an example missing mechanism where $(c_\text{low}, p_\text{low}) = (-6,0.1)$,
$(c_0, p_0)=(-2,.99)$, and $(c_\text{high}, p_\text{high}) = (-1,0.01)$.
\beginmyfig
\includegraphics[scale=.5]{img/prob_miss_example.pdf}
\caption{Example missing mechanism. The blue points serve as guides in
determining a missing mechanism. Values for $\beta$ and $c$ can be solved for
through a system of equations.}
\label{fig:prob-miss-eg}
\endmyfig

Through simple algebra, we can solve for $\beta$ and $c_0$ as follows:
\begin{align*}
  \beta_0 &:= \logit(p_0) \\
  \beta_1 &:= \frac{\beta_0 - \logit(p_\text{low})}{(y_\text{low} - c_0)^2} \\
  c_1 &:= \frac{\beta_0 - \logit(p_\text{high})}{\beta_1 ~ \sqrt{y_\text{high} - c_0} }. \\
\end{align*}
Hence, specifying three critical points in the missing mechanism can guide
the construction of its prior distribution.
\vspace{2em}


% fc-beta_0
\textbf{Full Conditional for $\beta_{0i}$}

Recall that \(\beta_{0i} \iid \N(m_{\beta_0},s^2_{\beta_0})\).

\begin{align*}
p(\beta_{0i} \mid \y, \rest) &\propto
p(\beta_{0i}) \times \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{\frac{-(\beta_{0i}-m_{\beta_0})^2}{2s^2_{\beta_0}}} \prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhSpiel{\beta_{0i}}
\vspace{2em}


%fc-beta_1
\textbf{Full Conditional for $\beta_{1i}$}

Recall that $\beta_{1i}\ind \N^+(m_{\beta_1}, s^2_{\beta_1})$.
%
\begin{align*}
p(\beta_{1i} \mid \y, \rest) &\propto
p(\beta_{1i}) \times 
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
%
&\propto \exp\bc{-\frac{(\beta_{1i} - m_{\beta_1})^2}{2s^2_{\beta_1}}}
\Ind{\beta_{1i} > 0}
\prod_{n=1}^{N_i} \prod_{j=1}^J f_{inj} \\
\end{align*}

\mhLogSpiel{\beta_{1i}}{\xi}
\vspace{2em}


% fc-y
\textbf{Full Conditional for Missing $\bm \y$}

\begin{align*}
p(y_{inj} \mid m_{inj}=1, \rest) &\propto
p(m_{inj} =1\mid y_{inj}, \rest) ~
p(y_{inj} \mid \rest) \\
%
%&\propto
%\exp\bc{\frac{-(y_{inj} - \mu_{inj})^2}{2\sigma^2_{inj}}}
%f_{inj} \\
&\propto
p_{inj} 
\sum_{\ell=1}^{L^{Z_{j\lin}}} \eta^{Z_{j\lin}}_{ij\ell} \cdot \N(y_{inj} \mid \mu^*_{Z_{jk}, \ell}, {\sigma^2}^\star_{i,Z_{j\lin},\ell}).
\end{align*}

\mhSpiel{y_{inj}}

Note that \(f_{inj}\) is a function of \(y_{inj}\) and should be
computed accordingly.
\end{document}



%%% TODO %%%
% Remove these graphs.
% Put only a couple of these subgraphs in the main report.

\newpage

%%% Many Graphs 
\section{Posterior Predictive for CB data \label{sec:cb-pp}}
\foreach \ppp in {1,...,12}{
  \beginmyfig \includegraphics[page=\ppp]{img/cb/y_hist.pdf} \endmyfig
}

\section{Posterior Predictive for Positive values of CB data \label{sec:cb-pp-positive}}
\foreach \ppp in {1,...,12}{
  \beginmyfig \includegraphics[page=\ppp]{img/cb/pp_obs.pdf} \endmyfig
}

\end{document}
