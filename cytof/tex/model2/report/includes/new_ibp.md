We desire to learn the latent structure of predominant cell-types, where each
cell-type is composed of various combinations of some known markers.
Specifically, each cell-type can be represented by a binary vector where a "1"
in at a location $j$ of the binary vector indicates that marker $j$ is
expressed in the cell-type, and a "0" at the same location indicates that the
marker is not expressed. For $J$ distinct markers, $2^J$ possible cell-types
can be constructed. One could create a $J \times 2^J$ matrix which contains all
possible cell-types generated by the $J$ markers, with each column containing
each cell-type. But this becomes infeasible when $J$ is large. Using Bayesian
modelling methods, we can explore the sample space of possible cell-types and
learn the predominant ones. For computational efficiency, we require a flexible
prior which will also learn the number of cell-types $(K)$ which generate the
observed data. The Indian buffet process (IBP) proposed by @griffiths2011indian
serves as a suitable prior for modelling the matrix of cell-types.  The IBP
have been used in a variety of applications where modelling latent features is
of interest. These models are also called latent feature allocation models.  We
will review some of the common representations of the IBP and their
computational advantages and properties. We will first discuss the original
representation by @griffiths2011indian. Then we will review the stick-breaking
construction for the IBP developed by @teh2007stick. We will then review the
dependent IBP (dIBP) developed by @williamson2010dependent for feature
allocation models where prior information on the correlation between the features
is available.

### The Indian Buffet Process

The Indian buffet process (IBP) proposed by @griffiths2011indian can be
constructed by first considering the finite feature allocation model, and then
taking the limit with respect to the number of features.

For the remainder of this section, let $Z$ be an $n \times K$ binary matrix.
The prior probability that object $i$ possessing feature $k$ is $\pi_k$.
Let $\pi_k$ have prior distribution $\text{Beta}(\alpha/K, 1)$. For the time being,
assume that $\alpha$ is known and fixed.

\begin{align}
\begin{split}
\pi_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1) \\
Z_{ik} \mid \pi_k &\sim \text{Bernoulli}(\pi_k) \\
\end{split}
\label{eq:ibp}
\end{align}

The matrix $Z$ has an IBP distribution with mass parameter $\alpha$ when the
$\pi_k$ are integrated out and in the limit $K \rightarrow \infty$ .  That is,
marginally, $Z \sim \text{IBP}(\alpha)$. While the binary matrix $Z$ is
unbounded in the columns, it can be shown that the number non-zero of columns
$K^+$ is distributed $\text{Poisson}(\alpha \suml \frac{1}{i})$.  Moreover, the
each row in $Z$ has is expected to have $\alpha$ active features.  In other
words, the mass parameter $\alpha$ influences the final number of columns in
the sampled matrices.  This can be shown using law of total expectation:
$$
\E\bk{\sum_{k=1}^K Z_{ik}} = \sum_{k=1}^K \E\bk{Z_{ik}} 
=\sum_{k=1}^K \E\bk{\E\bk{Z_{ik}\mid\pi_k}}
=\sum_{k=1}^K \E\bk{\pi_k}
=\sum_{k=1}^K \alpha/K
= \alpha.
$$

Since sampled matrix from model in (\ref{eq:ibp}) is extremely sparse,
arranging the columns in a way such that columns with more active features are
at the left most columns can yield computational advantages.
@griffiths2011indian suggest modeling, instead, the equivalence class
left-ordered of the binary matrices drawn from the model in (\ref{eq:ibp}).
That is, columns that represent a higher binary number are at the left of the
matrix. The probability mass function (pmf) of the left-ordered matrices can be
shown to have the following form:

\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prod_{i=1}^N {K_1}^{(i)}!} 
              \exp\bc{-\alpha H_N}\prod_{k=1}^{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}

where $H_N=\sum_{i=1}^N i^{-1}$ is the harmonic number, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(i)}$ is the number of features activated in row $i$ of 
$Z$ that are not activated in previous rows.

The name of this process, like the Chinese restaurant process, suggests a
culinary metaphor. The metaphor is as follows. Let $Z$ be an $N \times \infty$
binary matrix. Each row in $Z$ represents a customer who enters an Indian
buffet restaurant and each column represents one dish (out of an infinite
number of dishes) in the buffet. Customers enter the restaurant one after
another. The first customer samples an $r=\text{Poisson}(\alpha)$ number of
dishes, where $\alpha > 0$.  This is indicated by setting the first $r$ columns
of the first row in $Z$ to be $1$.  The other values in the row are set to $0$.
Each subsequent customer samples each previously sampled dish with probability
proportional to its popularity.  That is, the next customer samples dish $k$
with probability $m_k/i$, where $m_k=\sum_{j=1}^{i-1} Z_{jk}$ is the number of
customers that have sampled dish $k$, and $i$ is the current customer number
(or row number in $Z$). Each customer also samples an additional
Poisson$(\alpha/i)$ number of new dishes.  Once all the $N$ customers have gone
through this process, the resulting $Z$ matrix will be a draw from the Indian
buffet process with mass parameter $\alpha$.

Other representations and extensions of the IBP have been proposed. I will
discuss a few that are relevant to this project.

### Stick-breaking Construction for the IBP

The stick-breaking construction for the IBP was proposed by @teh2007stick.

\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \nonumber \\
\pi_k &:= \prod_{l=1}^k v_k \nonumber \nonumber \\
Z_{ik} \mid \pi_k &\sim \text{Bernoulli}(\pi_k)
\end{split}
\end{align}

### Dependent IBP

$$
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \\
\pi_k &:= \prod_{l=1}^k v_k \\
\bm h_{k} &\sim \N(\bm\mu, \bm\Sigma) \\
Z_{ik} \mid \pi_k &:= \Ind{\Phi\p{\frac{h_{ik} - \mu_i}{\sqrt\Sigma_{kk}}} < \pi_k} \\
\end{split}
$$

### Attraction Indian Buffet Distribution

### Prior for $\alpha$ in Stick-breaking Construction for IBP

$$
\begin{split}
v_k |\alpha &\sim \text{Beta}(\alpha, 1) \\
\alpha &\sim \text{Gamma}(a,b), ~~~ \text{with mean } (a/b) \\
\end{split}
$$

Under a Gamma prior, $\alpha$ is conjugate and its full conditional has the form

$$\alpha \mid \bm v \sim \text{Gamma}(a + K, b - \sum_{k=1}^K \log v_k)$$

