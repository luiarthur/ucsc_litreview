We desire to learn the latent structure of predominant cell-types, where each
cell-type is composed of various combinations of some known markers.
Specifically, each cell-type can be represented by a binary vector where a "1"
in at a location $j$ of the binary vector indicates that marker $j$ is
expressed in the cell-type, and a "0" at the same location indicates that the
marker is not expressed. For $J$ distinct markers, $2^J$ possible cell-types
can be constructed. One could create a $J \times 2^J$ matrix which contains all
possible cell-types generated by the $J$ markers, with each column containing
each cell-type. But this becomes infeasible when $J$ is large. Using Bayesian
modelling methods, we can explore the sample space of possible cell-types and
learn the predominant ones. For computational efficiency, we require a flexible
prior which will also learn the number of cell-types $(K)$ which generate the
observed data. The Indian buffet process (IBP) proposed by @griffiths2011indian
serves as a suitable prior for modelling the matrix of cell-types.  The IBP
have been used in a variety of applications where modelling latent features is
of interest. These models are also called latent feature allocation models.  We
will review some of the common representations of the IBP and their
computational advantages and properties. We will first discuss the original
representation by @griffiths2011indian. Then we will review the stick-breaking
construction for the IBP developed by @teh2007stick. We will then review the
dependent IBP (dIBP) developed by @williamson2010dependent for feature
allocation models where prior information on the correlation between the objects
(rows) is available.

### The Indian Buffet Process

The Indian buffet process (IBP) proposed by @griffiths2011indian can be
constructed by first considering the finite feature allocation model, and then
taking the limit with respect to the number of features.

For the remainder of this section, let $Z$ be an $J \times K$ binary matrix.
The prior probability that object $j$ possessing feature $k$ is $\pi_k$.
Let $\pi_k$ have prior distribution $\text{Beta}(\alpha/K, 1)$. For the time being,
assume that $\alpha$ is known and fixed.

\begin{align}
\begin{split}
\pi_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1) \\
Z_{jk} \mid \pi_k &\sim \text{Bernoulli}(\pi_k) \\
\end{split}
\label{eq:ibp}
\end{align}

The matrix $Z$ has an IBP distribution with mass parameter $\alpha$ when the
$\pi_k$ are integrated out and in the limit $K \rightarrow \infty$ .  That is,
marginally, $Z \sim \text{IBP}(\alpha)$. While the binary matrix $Z$ is
unbounded in the columns, it can be shown that the number non-zero of columns
$K^+$ is distributed $\text{Poisson}(\alpha \sum_{j=1}^J j^{-1})$.
Moreover, the each row in $Z$ has is expected to have $\alpha$ active features.
In other words, the mass parameter $\alpha$ influences the final number of
columns in the sampled matrices.  This can be shown using law of total
expectation:
$$
\E\bk{\sum_{k=1}^K Z_{jk}} = \sum_{k=1}^K \E\bk{Z_{jk}} 
=\sum_{k=1}^K \E\bk{\E\bk{Z_{ik}\mid\pi_k}}
=\sum_{k=1}^K \E\bk{\pi_k}
=\sum_{k=1}^K \alpha/K
= \alpha.
$$

Since sampled matrix from model in (\ref{eq:ibp}) is extremely sparse,
arranging the columns in a way such that columns with more active features are
at the left most columns can yield computational advantages.
@griffiths2011indian suggest modeling, instead, the equivalence class
left-ordered of the binary matrices drawn from the model in (\ref{eq:ibp}).
That is, columns that represent a higher binary number are at the left of the
matrix. The probability mass function (pmf) of the left-ordered matrices can be
shown to have the following form:

\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prod_{j=1}^J {K_1}^{(j)}!} 
              \exp\bc{-\alpha H_N}\prod_{k=1}^{K_+}
              \frac{(J-m_k)!(m_k-1)!}{J!},
  \label{eq:ibplo}
\end{equation}

where $H_J=\sum_{j=1}^J j^{-1}$ is the harmonic number, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(j)}$ is the number of features activated in row $j$ of 
$Z$ that are not activated in previous rows.

The name of this process, like the Chinese restaurant process, suggests a
culinary metaphor. The metaphor is as follows. Let $Z$ be an $J \times \infty$
binary matrix. Each row in $Z$ represents a customer who enters an Indian
buffet restaurant and each column represents one dish (out of an infinite
number of dishes) in the buffet. Customers enter the restaurant one after
another. The first customer samples an $r=\text{Poisson}(\alpha)$ number of
dishes, where $\alpha > 0$.  This is indicated by setting the first $r$ columns
of the first row in $Z$ to be $1$.  The other values in the row are set to $0$.
Each subsequent customer samples each previously sampled dish with probability
proportional to its popularity.  That is, the next customer samples dish $k$
with probability $m_k/j$, where $m_k=\sum_{l=1}^{j-1} Z_{lk}$ is the number of
customers that have sampled dish $k$, and $l$ is the current customer number
(or row number in $Z$). Each customer also samples an additional
Poisson$(\alpha/j)$ number of new dishes.  Once all the $J$ customers have gone
through this process, the resulting $Z$ matrix will be a draw from the Indian
buffet process with mass parameter $\alpha$.

Other representations and extensions of the IBP have been proposed. I will
discuss a few that are relevant to this project.

### Stick-breaking Construction for the IBP

The stick-breaking construction for the IBP was proposed by @teh2007stick,
and can be sampled from by the following scheme:

\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \\
\pi_k &:= \prod_{l=1}^k v_k \\
Z_{jk} \mid \pi_k &\sim \text{Bernoulli}(\pi_k). \\
\end{split}
\label{eq:sbibp}
\end{align}

This "stick-breaking" construction is can be derived by first starting with
model (\ref{eq:ibp}), then ordering the $\pi_k$ so that $Z_{jk} \mid
{\pi_{(k)}} \sim \text{Bernoulli}(\pi_{(k)})$, where $\pi_{(a)} > \pi_{(b)}$
for all $a < b$, and $a,b \in \mathbb{N}$. The features in this model are
ordered in the sense that activated features tend to appear in the left-most
columns. This differs from the traditional definition of the IBP in
(\ref{eq:ibp}) as the features are not in any way ordered. Moreover, this
construction does not yield binary matrices that are left-ordered. So this
process does not yield the probability mass function in (\ref{eq:ibplo}). This
stick-breaking representation resembles the stick-breaking representation of
the Dirichlet process (DP), and so can be extended in similar ways that the DP
has been extended. In a Gibbs sampler, the elements in $Z$ can be easily
updated using Gibbs steps and metropolis steps.

The number of columns in $Z$ can be fixed in advanced at some large value. But
as noted by @teh2007stick, the truncation value is somewhat arbitrary.  A
slice-sampler can be used to avoid this unnecessary approximation.
Alternatively, a prior can be placed on $K^+$, which is what we do in this
project.


### Dependent IBP

The dependent IBP (dIBP) is one extension of the IBP under the stick-breaking
representation which allows prior information on the correlation between 
items (rows) to be included in feature allocation models. This correlation 
(or dependency) is introduced into the model through a covariance matrix
$\bm S$. Hence, the model is as follows:

\begin{align}
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \\
\pi_k &:= \prod_{l=1}^k v_k \\
\bm h_{k} &\sim \N(\bm 0, \bm S) \\
Z_{jk} \mid \pi_k &:= \Ind{\pi_k > \Phi\p{\frac{h_{jk} - 0}{\sqrt{S_{kk}}}}} \\
\end{split}
\label{eq:dibp}
\end{align}

Here, $\Ind{\cdot}$ is the indicator function, $\Phi(\cdot)$ is the cumulative
standard Normal distribution function, and $\bm S$ is known and contains the
covariance of the objects $j=1,...,J$.  Note that when $\bm S = \I$ then the
dIBP reduces to the stick-breaking construction of the IBP. Hence, the
"dependency" between objects is introduced to the model through $\bm S$.
Recall also that since the stick-breaking representation of the IBP implicitly
imposes an ordering of the columns so that features that are activated are more
likely to occur in the left-most columns, the traditional IBP is not recovered
in the sense that the pmf is recovered. But since the ordering of the features
is not of major interest in application, the stick-breaking and dependent IBP
behave like the IBP.

### Prior for $\alpha$ in Stick-breaking Construction for IBP

In the previous sections, $\alpha$ is treated as fixed and known. A prior
distribution can be placed on $\alpha$ to reflect uncertainty. In general, for
variants of the IBP that make use of the stick-breaking representation, a
(conjugate) Gamma prior can be placed on $\alpha$, and its full conditional is
as follows:

$$
\begin{split}
v_k |\alpha &\sim \text{Beta}(\alpha, 1) \\
\alpha &\sim \text{Gamma}(a,b), ~~~ \text{with mean } (a/b) \\
\alpha \mid \bm v &\sim \text{Gamma}(a + K, b - \sum_{k=1}^K \log v_k). \\
\end{split}
$$



