We desire to learn the latent structure of predominant cell-types, where each
cell-type is composed of various combinations of some known markers.
Specifically, each cell-type can be represented by a binary vector where a "1"
in at a location $j$ of the binary vector indicates that marker $j$ is
expressed in the cell-type, and a "0" at the same location indicates that the
marker is not expressed. For $J$ distinct markers, $2^J$ possible cell-types
can be constructed. One could create a $J \times 2^J$ matrix which contains all
possible cell-types generated by the $J$ markers, with each column containing
each cell-type. But this becomes infeasible when $J$ is large. Using Bayesian
modelling methods, we can explore the sample space of possible cell-types and
learn the predominant ones. For computational efficiency, we require a flexible
prior which will also learn the number of cell-types $(K)$ which generate the
observed data. The Indian buffet process (IBP) proposed by @griffiths2011indian
serves as a suitable prior for modelling the matrix of cell-types.  The IBP
have been used in a variety of applications where modelling latent features is
of interest. These models are also called latent feature allocation models.  We
will review some of the common representations of the IBP and their
computational advantages and properties. We will first discuss the original
representation by @griffiths2011indian. Then we will review the stick-breaking
construction for the IBP developed by @teh2007stick. We will then review the
dependent IBP (dIBP) developed by @williamson2010dependent for feature
allocation models where prior information on the correlation between the features
is available.

### The Indian Buffet Process

The Indian buffet process (IBP) proposed by @griffiths2011indian can be
constructed by first considering the finite feature allocation model, and then
taking the limit with respect to the number of features.

For the remainder of this section, let $Z$ be an $n \times K$ binary matrix.
The prior probability that object $i$ possessing feature $k$ is $\pi_k$.
Let $\pi_k$ have prior distribution $\text{Beta}(\alpha/K, 1)$. For the time being,
assume that $\alpha$ is known and fixed.

$$
\begin{split}
\pi_k \mid \alpha &\sim \text{Beta}(\alpha/K, 1) \\
Z_{ik} \mid \pi_k &\sim \text{Bernoulli}(\pi_k)
\end{split}
$$

The matrix $Z$ has an IBP distribution with concentration parameter $\alpha$
when the $\pi_k$ are integrated out and in the limit $K \rightarrow \infty$ .
That is, marginally, $Z \sim \text{IBP}(\alpha)$. While the binary matrix 
$Z$ is unbounded in the columns, it can be shown that the number non-zero
of columns $K^+$ is distributed $\text{Poisson}(\alpha \suml \frac{1}{i})$.
Moreover, the each row in $Z$ has is expected to have $\alpha$ active features.
This can be shown using law of total expectation:
$$
\E\bk{\sum_{k=1}^K Z_{ik}} = \sum_{k=1}^K \E\bk{Z_{ik}} 
=\sum_{k=1}^K \E\bk{\E\bk{Z_{ik}\mid\pi_k}}
=\sum_{k=1}^K \E\bk{\pi_k}
=\sum_{k=1}^K \alpha/K
= \alpha.
$$


### Stick-breaking Construction for the IBP

The stick-breaking construction for the IBP was proposed by @teh2007stick.

$$
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \\
\pi_k &:= \prod_{l=1}^k v_k \\
Z_{ik} \mid \pi_k &\sim \text{Bernoulli}(\pi_k)
\end{split}
$$


### Dependent IBP

$$
\begin{split}
v_k \mid \alpha &\sim \text{Beta}(\alpha,1) \\
\pi_k &:= \prod_{l=1}^k v_k \\
\bm h_{k} &\sim \N(\bm\mu, \bm\Sigma) \\
Z_{ik} \mid \pi_k &:= \Ind{\Phi\p{\frac{h_{ik} - \mu_i}{\sqrt\Sigma_{kk}}} < \pi_k} \\
\end{split}
$$

### Attraction Indian Buffet Distribution

### Prior for $\alpha$ in Stick-breaking Construction for IBP

$$
\begin{split}
v_k |\alpha &\sim \text{Beta}(\alpha, 1) \\
\alpha &\sim \text{Gamma}(a,b), ~~~ \text{with mean } (a/b) \\
\end{split}
$$

Under a Gamma prior, $\alpha$ is conjugate and its full conditional has the form

$$\alpha \mid \bm v \sim \text{Gamma}(a + K, b - \sum_{k=1}^K \log v_k)$$

